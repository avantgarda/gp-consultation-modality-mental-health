{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8166b9a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import sys\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "from IPython.display import display, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bb6df4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "INPUT_DATA_PATH = '../data/'\n",
        "CLEANED_DATA_PATH = '../data/cleaned/'\n",
        "if not os.path.exists(CLEANED_DATA_PATH): os.makedirs(CLEANED_DATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb0d5ab5",
      "metadata": {},
      "source": [
        "### IMPORTANT\n",
        "Function for reading CSV with non-escaped delimiters in *Terms* column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1f5fa63",
      "metadata": {},
      "outputs": [],
      "source": [
        "def bad_csv_line_consultations(line):\n",
        "    assert len(line[0]) == 64 # confirm pseudo ID in first column\n",
        "    assert line[1].isnumeric # confirm patient ID in second column\n",
        "    assert line[2].isnumeric # confirm consultation ID in third column\n",
        "    try: pd.to_datetime(line[3]) # confirm for date in fourth column\n",
        "    except: raise ValueError('This should be a date', line)\n",
        "    assert line[4].isalpha # confirm consultation type in fifth column\n",
        "    code_column = 6\n",
        "    while not re.search(r'^(?=.*[0-9])(?=.*[A-Z])([A-Z0-9]+)$', line[code_column]): code_column += 1 # check for GP code\n",
        "    return line[:5] + [','.join(line[5:code_column])] + line[code_column:] # merge Term columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca8e353e",
      "metadata": {},
      "source": [
        "Extraction end dates:\n",
        "- **LDN** - September 2021 (most recent LDN update in CRIS)\n",
        "- **SLaM** - June 2022\n",
        "\n",
        "**NOTE** CSV Python engine seems to put read details in last two rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a65e346a",
      "metadata": {},
      "outputs": [],
      "source": [
        "ldn_consultations_load = pd.read_csv(INPUT_DATA_PATH + 'output2.csv', engine='python', on_bad_lines=bad_csv_line_consultations, dtype=str)\n",
        "assert ldn_consultations_load.iloc[-2, :].str.contains(r'^\\([0-9]* rows affected\\)$', regex=True).iloc[0]\n",
        "assert ldn_consultations_load.iloc[-1, :].str.contains(r'^Completion time: [0-9T:\\-\\+\\.]*$', regex=True).iloc[0]\n",
        "ldn_consultations_load = ldn_consultations_load[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d4dcfaa",
      "metadata": {},
      "outputs": [],
      "source": [
        "slam_contacts_load = pd.read_excel(INPUT_DATA_PATH + 'slam_outputs_1_2.xlsx', sheet_name='output1_events', dtype=str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80dac2f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "slam_admissions_load = pd.read_excel(INPUT_DATA_PATH + 'slam_outputs_1_2.xlsx', sheet_name='output2_admissions', dtype=str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a7d17fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "patient_level_load = pd.read_excel(INPUT_DATA_PATH + 'ldn_outputs1_3.xlsx', sheet_name='output1', dtype=str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f6b0565",
      "metadata": {},
      "outputs": [],
      "source": [
        "ldn_medications_load = pd.read_excel(INPUT_DATA_PATH + 'ldn_outputs1_3.xlsx', sheet_name='output3', dtype=str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "561e819e-d5e3-41de-be76-49c3ee8898dd",
      "metadata": {},
      "source": [
        "## Cleaning\n",
        "- Parsing and initial cleaning of datasets\n",
        "- Removing those duplicate registration rows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca20df72-eb80-4aa6-97e0-e24352647475",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dab8f051-bfb2-4ac3-b8ac-547afba7df6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_id_format_and_standardise(df, patient_id_column):\n",
        "    ### confirm that the patient identifiers are parsed as strings\n",
        "    ### and confirm that pseudo-NHS numbers are exactly 64 characters\n",
        "    ### and patient IDs are less than or equal to 7 characters\n",
        "    df = df.copy()\n",
        "    df = df.rename(columns={'PseudonymisedNHSNumber':'nhs', patient_id_column:'patient_id'})\n",
        "    ### assertions\n",
        "    assert df.nhs.notna().all()\n",
        "    assert df.nhs.apply(lambda x: isinstance(x, str)).all()\n",
        "    assert df.nhs.apply(len).eq(64).all()\n",
        "    assert df.patient_id.notna().all()\n",
        "    assert df.patient_id.apply(lambda x: isinstance(x, str)).all()\n",
        "    assert df.patient_id.apply(len).le(7).all()\n",
        "    ### assert that ID-to-NHS is 1-to-1\n",
        "    assert df.groupby('patient_id').nhs.nunique().eq(1).all()\n",
        "    ### then zero-pad patient IDs to 7 characters\n",
        "    df.patient_id = df.patient_id.str.zfill(7)\n",
        "    ### return checked and padded dataframe\n",
        "    return df\n",
        "\n",
        "def filter_to_inclusion_cohort(df, patient_level_nhs):\n",
        "    ### all LDN  outputs are for active LDN registration from 2019 AND inclusion of LTC\n",
        "    ### all SLAM outputs are for active LDN registration from 2019 ONLY\n",
        "    ### i.e. SLAM outputs could also include LDN patients with any conditions and without our LTCs!\n",
        "    df = df.copy()\n",
        "    patient_level_nhs = patient_level_nhs.copy()\n",
        "    ### remove irrelevant patients from SLAM outputs who do not meet our LTC inclusion criteria (theoretically)\n",
        "    df = df.loc[df.nhs.isin(patient_level_nhs)]\n",
        "    ### return filtered dataframe\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_id_counts_and_sets(df, patient_level_id_pairs):\n",
        "    ### but some patient IDs from SLAM outputs will not appear in LDN patient level EVEN if that patient NHS number is in our patient level\n",
        "    ### this is because the patient ID pertains to a GP registration, and our LTCs maybe not appear during that specific registration\n",
        "    ### combine dataframe IDs with patient level, and then get count lookup table\n",
        "    df_and_patient_level_ids = pd.concat([df, patient_level_id_pairs])[['nhs', 'patient_id']].copy()\n",
        "    patient_id_counts = df_and_patient_level_ids.groupby('nhs').patient_id.nunique().to_dict()\n",
        "    ### all pseudo-NHS numbers, and all patient IDs in cohort\n",
        "    pseudo_nhs_all = set(df_and_patient_level_ids.nhs)\n",
        "    patient_ids_all = set(df_and_patient_level_ids.patient_id)\n",
        "    ### return lookup table, and sets of pseudo-NHS and IDs\n",
        "    return patient_id_counts, pseudo_nhs_all, patient_ids_all\n",
        "\n",
        "    \n",
        "def check_patient_ids(patient_ids):\n",
        "    ### checks the patient IDs for SEEMINGLY duplicate patient events\n",
        "    ### checks everything is the same, including pseudo-NHS number, EXCEPT the patient IDs\n",
        "    ### so pseudo-NHS is the same in each group (grouped by this too)\n",
        "    ### only patient ID is being checked, and could possibly be different values\n",
        "\n",
        "    ### in theory, should only be one patient ID as grouping by pseudo-NHS number!\n",
        "    ### but some patients have multiple patient IDs, one per GP registration,\n",
        "    ### and this seems to have resulted in duplicate events, POTENTIALLY one per unique patient ID\n",
        "\n",
        "    ### issue is it's possible for a patient to have two events i.e. separate events on the same day, but look the same for each data-point!\n",
        "    ### so we need to de-duplicate the actual duplicates, and leave behind the unique event and any similar but separate events!\n",
        "    \n",
        "    ### patient_ids is just a list of the all the patient IDs\n",
        "    ### patient IDs are not null\n",
        "\n",
        "    num_events = len(patient_ids)\n",
        "    unique_ids = patient_ids.unique()\n",
        "    num_unique_ids = patient_ids.nunique()\n",
        "    ids_value_counts = patient_ids.value_counts()\n",
        "    \n",
        "    nhs_in_patient_level = patient_ids.name[0] in pseudo_nhs_all\n",
        "    id_in_patient_level = all([p_id in patient_ids_all for p_id in unique_ids])\n",
        "    nhs_in_patient_level_tick = '✔' if nhs_in_patient_level else '✘'\n",
        "    id_in_patient_level_tick = '✔' if id_in_patient_level else '✘'\n",
        "\n",
        "    # print('---')\n",
        "    # print(f\"Events: {num_events}\", ids_value_counts.to_dict())\n",
        "\n",
        "    mismatch_msg = ''\n",
        "    \n",
        "    if nhs_in_patient_level and id_in_patient_level:\n",
        "        # print('NHS number matched ☑ | Patient ID (registration) matched ☑')\n",
        "        num_recorded_duplicate_ids = patient_id_counts[patient_ids.name[0]]\n",
        "        # make sure the number of IDs per NHS (on record) matches the number of IDs for this event\n",
        "        # assert num_recorded_duplicate_ids == num_unique_ids, patient_ids\n",
        "        if num_recorded_duplicate_ids != num_unique_ids:\n",
        "            msg = f'Number of IDs on record ({num_recorded_duplicate_ids}) do not match number of IDs for this event ({num_unique_ids})'\n",
        "            # display(HTML(f'<p style=\"background-color: red;\">{msg}</p>'))\n",
        "            mismatch_msg = ' MM'\n",
        "    else:\n",
        "        msg = f'NHS number matched {nhs_in_patient_level_tick} | Patient ID (registration) matched {id_in_patient_level_tick}'\n",
        "        # display(HTML(f'<p style=\"background-color: red;\">{msg}</p>'))\n",
        "        return 'Missing ID'  + mismatch_msg\n",
        "    \n",
        "    if num_events == 1:\n",
        "        msg = f'Single event for single patient and matching NHS number associated patient IDs ({num_unique_ids}) ☑'\n",
        "        # display(HTML(f'<p style=\"background-color: lightgreen;\">{msg}</p>'))\n",
        "        return f'Single ({num_unique_ids})' + mismatch_msg\n",
        "    elif num_unique_ids == 1:\n",
        "        msg = f'Similar events only ({num_events} events) ☑'\n",
        "        # display(HTML(f'<p style=\"background-color: lightblue;\">{msg}</p>'))\n",
        "        return f'Similar ({num_events})' + mismatch_msg\n",
        "    elif ids_value_counts.eq(1).all():\n",
        "        msg = f'Possible duplicates patient IDs ({num_unique_ids}) and matching NHS number associated patient IDs ({num_recorded_duplicate_ids}) ☑'\n",
        "        # display(HTML(f'<p style=\"background-color: yellow;\">{msg}</p>'))\n",
        "        return f'Duplicate ({num_unique_ids})' + mismatch_msg\n",
        "    elif ids_value_counts.nunique() == 1:\n",
        "        assert num_events % num_unique_ids == 0\n",
        "        msg = f'Possible duplicates ({unique_ids} patient IDs) AND similar events ({num_events // num_unique_ids} events) ☑'\n",
        "        # display(HTML(f'<p style=\"background-color: plum;\">{msg}</p>'))\n",
        "        return f'Similar and Duplicate ({num_events // num_unique_ids}, {num_unique_ids})' + mismatch_msg\n",
        "    else:\n",
        "        raise('Something is wrong...')\n",
        "\n",
        "\n",
        "def assign_duplicate_type(df, patient_level_id_pairs):\n",
        "    # make a copy of the input dataframe\n",
        "    df = df.copy()\n",
        "    patient_level_id_pairs = patient_level_id_pairs.copy()\n",
        "    # check all NHS match patient level\n",
        "    assert df.nhs.isin(patient_level_id_pairs.nhs).eq(True).all()\n",
        "    print(f\"All NHS numbers matched to patient level. Patient IDs unmatched: {(~df.patient_id.isin(patient_level_id_pairs.patient_id)).sum()}/{df.shape[0]}\\n\")\n",
        "    # apply group checking function\n",
        "    df['Status'] = df.groupby(list(df.columns.drop('patient_id')), dropna=False).patient_id.progress_transform(check_patient_ids)\n",
        "    sys.stdout.flush()  # <-- Add this to flush the buffer\n",
        "    sys.stderr.flush()  # <-- And this for stderr\n",
        "    print('\\nGroup assignment complete!\\n')\n",
        "    display(df.Status.value_counts().sort_index())\n",
        "    # return dataframe\n",
        "    return df\n",
        "    \n",
        "\n",
        "def de_duplicate_group(patient_ids):\n",
        "    # parse status\n",
        "    status_string = patient_ids.name[-1]\n",
        "    status, other = status_string.strip(')').split(' (')\n",
        "    assert status in ['Single', 'Similar', 'Duplicate', 'Similar and Duplicate']\n",
        "\n",
        "    num_events = len(patient_ids)\n",
        "    num_unique_ids = patient_ids.nunique()\n",
        "\n",
        "    if status == 'Single':\n",
        "        assert num_events == 1\n",
        "        num_keep = 1\n",
        "    elif status == 'Similar':\n",
        "        num_events_parsed = int(other)\n",
        "        assert num_unique_ids == 1\n",
        "        assert num_events == num_events_parsed\n",
        "        num_keep = num_events\n",
        "    elif status == 'Duplicate':\n",
        "        num_unique_ids_parsed = int(other)\n",
        "        assert num_unique_ids == num_unique_ids_parsed\n",
        "        assert num_events == num_unique_ids_parsed\n",
        "        num_keep = 1\n",
        "    elif status == 'Similar and Duplicate':\n",
        "        num_events_parsed, num_unique_ids_parsed = (int(x) for x in other.split(','))\n",
        "        assert num_unique_ids == num_unique_ids_parsed\n",
        "        assert num_events % num_unique_ids == 0\n",
        "        assert (num_events // num_unique_ids) == num_events_parsed\n",
        "        num_keep = num_events_parsed\n",
        "    else:\n",
        "        raise('Something is wrong...')\n",
        "\n",
        "    # Return 0, 1, 2, ... num_keep-1, then NaN for rest\n",
        "    assert isinstance(num_keep, int)\n",
        "    return pd.Series(\n",
        "        list(range(num_keep)) + [np.nan] * (len(patient_ids) - num_keep),\n",
        "        index=patient_ids.index\n",
        "    )\n",
        "\n",
        "def de_duplicate_dataframe(df):\n",
        "    # make a copy of the input dataframe\n",
        "    df = df.copy()\n",
        "    # apply de-duplication function, and filter out duplicate rows\n",
        "    print('\\nDe-duplicating dataframe...\\n')\n",
        "    df['EVENT_ID'] = df.groupby(list(df.columns.drop(['patient_id'])), dropna=False).patient_id.progress_transform(de_duplicate_group)\n",
        "    df_de_duplicated = df.loc[df.EVENT_ID.notna()].copy()\n",
        "    # format dataframe\n",
        "    df_de_duplicated.EVENT_ID = df_de_duplicated.EVENT_ID.astype(int)\n",
        "    df_de_duplicated = df_de_duplicated.drop(['patient_id', 'Status'], axis=1)\n",
        "    sys.stdout.flush()  # <-- Add this to flush the buffer\n",
        "    sys.stderr.flush()  # <-- And this for stderr\n",
        "    print(f\"\\nChecks complete and de-duplication finished!\")\n",
        "    print(f\"Previous size: {df.shape[0]} | Resulting size: {df_de_duplicated.shape[0]} | Removed {df.shape[0] - df_de_duplicated.shape[0]} duplicated events\")\n",
        "    # return de-depulicated dataframe\n",
        "    return df_de_duplicated\n",
        "    # return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "953ab157-afe7-4c4c-b347-861320fe77c7",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Patient Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f05d7f-9f1d-4dad-bcfe-3d42d2d11a79",
      "metadata": {},
      "outputs": [],
      "source": [
        "# copy from loading dataframe\n",
        "patient_level = patient_level_load.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b59eddb-0abb-4d51-b772-48a9e18f8a23",
      "metadata": {},
      "outputs": [],
      "source": [
        "# format date columns\n",
        "patient_level.yearofbirth = pd.to_datetime(patient_level.yearofbirth, format='mixed').dt.normalize()\n",
        "patient_level.dateofdeath = pd.to_datetime(patient_level.dateofdeath, format='mixed').dt.normalize()\n",
        "patient_level.registrationstartdate = pd.to_datetime(patient_level.registrationstartdate, format='mixed').dt.normalize()\n",
        "patient_level.RegistrationEndDate = pd.to_datetime(patient_level.RegistrationEndDate, format='mixed').dt.normalize()\n",
        "\n",
        "# format boolean columns\n",
        "patient_level.SLAM_SMI_ever = patient_level.SLAM_SMI_ever.map({'0': False, '1': True})\n",
        "patient_level.SLAM_depression_ever = patient_level.SLAM_depression_ever.map({'0': False, '1': True})\n",
        "patient_level.SLAM_anxiety_ever = patient_level.SLAM_anxiety_ever.map({'0': False, '1': True})\n",
        "\n",
        "# format integer columns\n",
        "patient_level.LDN_N_anxiety_ever = patient_level.LDN_N_anxiety_ever.astype(int)\n",
        "patient_level.LDN_N_depression_ever = patient_level.LDN_N_depression_ever.astype(int)\n",
        "patient_level.LDN_N_SMI_ever = patient_level.LDN_N_SMI_ever.astype(int)\n",
        "patient_level.LDN_N_AF_ever = patient_level.LDN_N_AF_ever.astype(int)\n",
        "patient_level.LDN_N_heart_failure_ever = patient_level.LDN_N_heart_failure_ever.astype(int)\n",
        "patient_level.LDN_N_IHD_ever = patient_level.LDN_N_IHD_ever.astype(int)\n",
        "patient_level['Index of Multiple Deprivation (IMD) Rank (where 1 is most depriv'] = patient_level['Index of Multiple Deprivation (IMD) Rank (where 1 is most depriv'].astype('Int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd431417-92c0-44be-b76c-9e3d8b024706",
      "metadata": {},
      "outputs": [],
      "source": [
        "# rename and standardise ID columns\n",
        "patient_level = check_id_format_and_standardise(patient_level, 'patientid')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "670ecf6d-d7c9-4a82-b1db-d15e62a462b7",
      "metadata": {},
      "source": [
        "Add IMD deciles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f32a5d90-5cfd-42f4-ae46-78d0682db2dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "patient_level = patient_level.rename(columns={'Index of Multiple Deprivation (IMD) Rank (where 1 is most depriv':'IMD_Rank', 'sex_LDN':'Sex'})\n",
        "patient_level['IMD_Decile'] = (1 + np.floor(10 * (patient_level['IMD_Rank'] - 1) / 32_844)).astype('Int32')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84431c02-0081-436f-99f2-a22785417c3e",
      "metadata": {},
      "source": [
        "Add ethnicity categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3060da9f-54e1-4b31-9c80-2c320c0c2b30",
      "metadata": {},
      "outputs": [],
      "source": [
        "### QUESTIONABLE (MANUAL) ASSIGNMENT OF CENSUS 2021 CATEGORIES\n",
        "### SHOULD USE OFFICAL READ CODE TO SNOMED, AND SNOMED TO CENSUS 2021 LOOKUP/MAPPING\n",
        "ethnicities = pd.read_excel('./Resources/Ethnicities.xlsx', usecols=['description', 'A/AB', 'B/BB/C/A', 'M/ME', 'W', 'O', 'UNK'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c81b872e-939f-46e1-9576-3451266261b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "ethnicity_categories = {\n",
        "    'A/AB':'Asian or Asian British',\n",
        "    'B/BB/C/A':'Black, Black British, Caribbean or African',\n",
        "    'M/ME':'Mixed or multiple ethnic groups',\n",
        "    'W':'White',\n",
        "    'O':'Other ethnic group',\n",
        "    'UNK':'Missing'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea3ac846-d3fd-4843-9a86-8a2c5becc6da",
      "metadata": {},
      "outputs": [],
      "source": [
        "ethnicities['Ethnicity'] = ethnicities.iloc[:, 1:].idxmax(axis=1)\n",
        "ethnicities['Ethnicity'] = ethnicities['Ethnicity'].replace(ethnicity_categories)\n",
        "ethnicities = ethnicities.rename(columns={'description':'LDN_ethnicity'})\n",
        "ethnicities = ethnicities.drop_duplicates()\n",
        "\n",
        "# merge in ethnicities\n",
        "patient_level = patient_level.merge(ethnicities[['LDN_ethnicity', 'Ethnicity']], on='LDN_ethnicity', how='left')\n",
        "\n",
        "# change Missing to NaN\n",
        "patient_level.Ethnicity = patient_level.Ethnicity.replace({'Missing':np.nan})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d63953d9-02be-4620-afd8-ce913cd9ec3a",
      "metadata": {},
      "source": [
        "Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc7fac77-f649-4c6b-a1d2-e307e8e86179",
      "metadata": {},
      "outputs": [],
      "source": [
        "# First_Language_ID - this is mainly just ensure a valid value is propogated to the top, if it exists\n",
        "patient_level['First_Language_ID'] = patient_level['First_Language_ID'].replace({'Not Known':np.nan, 'Other':np.nan})\n",
        "\n",
        "# LDN_preferred_language\n",
        "### HAVEN'T HANDLED THIS YET - NOT USING CURRENTLY..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f8dd9f4-3ebb-472e-89f0-0fd7b8c00b3a",
      "metadata": {},
      "source": [
        "Create lists for ID mappings and dynamic values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "558316d1-91bf-4073-b0d0-90aab9ff608b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# save NHS-ID pairs for de-duplication of other outputs\n",
        "patient_level_id_pairs = patient_level[['patient_id', 'nhs']].drop_duplicates().copy()\n",
        "\n",
        "# save NHS to GP details\n",
        "patient_level_gp_lookup = patient_level[['nhs', 'IMD_Decile', 'NationalPracticeCode', 'registrationstartdate', 'RegistrationEndDate']].drop_duplicates().copy()\n",
        "\n",
        "# save dynamic values to file - IMD and GP\n",
        "patient_level_gp_lookup.to_csv(CLEANED_DATA_PATH + 'Patient_Level_Dynamic.csv', index=False, quoting=csv.QUOTE_ALL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40822d33-f64e-4e9b-85a8-283c54c2fdd1",
      "metadata": {},
      "source": [
        "Reconcile demographic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb9a780-09db-4266-987c-fe2af899d09b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Sort by most recent registration (Crucial for 'first' logic)\n",
        "patient_level = patient_level.sort_values('registrationstartdate', ascending=False)\n",
        "\n",
        "# 2. PREPARATION: Handle NaNs for sum columns only\n",
        "# We fill NaNs with 0 here so the sum works correctly. \n",
        "# We do NOT backfill these.\n",
        "sum_columns = ['LDN_N_anxiety_ever', 'LDN_N_depression_ever', 'LDN_N_SMI_ever',  'LDN_N_AF_ever', 'LDN_N_heart_failure_ever', 'LDN_N_IHD_ever']\n",
        "\n",
        "patient_level[sum_columns] = patient_level[sum_columns].fillna(0)\n",
        "\n",
        "# 3. BUILD DICTIONARY\n",
        "# We define the logic: 'sum' for counts, 'first' for everything else.\n",
        "# Optimization Note: In pandas groupby, 'first' skips NaNs automatically.\n",
        "agg_dict = {\n",
        "    col: 'sum' if col in sum_columns else 'first' \n",
        "    for col in patient_level.columns \n",
        "    if col != 'nhs' and col != 'patient_id'\n",
        "}\n",
        "\n",
        "# 4. AGGREGATE (The Speed Optimization)\n",
        "# Instead of backfilling first (slow), we rely on .agg()\n",
        "patient_level = patient_level.groupby('nhs', as_index=False).agg(agg_dict)\n",
        "\n",
        "# 5. CLEANUP\n",
        "# patient_id is no longer needed (nhs is the grouper)\n",
        "if 'patient_id' in patient_level.columns: patient_level = patient_level.drop('patient_id', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a55c5390-def7-4c63-a966-7604c855ff70",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Number of unique patients:', patient_level.shape[0])\n",
        "patient_level.sample(2).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "961df4d3-d475-4eab-9c9f-acfdebde8530",
      "metadata": {},
      "outputs": [],
      "source": [
        "# save to file\n",
        "assert not patient_level.duplicated().any()\n",
        "assert not patient_level.nhs.duplicated().any()\n",
        "patient_level.to_csv(CLEANED_DATA_PATH + 'Patient_Level.csv', index=False) # save newly cleaned version to CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cbfe742-4a76-45a1-bd81-3e332070dce4",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### SLAM Admissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6beabae4-b8a8-478d-bbfc-1dc0cf8f1526",
      "metadata": {},
      "outputs": [],
      "source": [
        "# copy from loading dataframe\n",
        "slam_admissions = slam_admissions_load.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "247a264f-c88f-4e4a-a536-23c4f7c91d19",
      "metadata": {},
      "outputs": [],
      "source": [
        "# format date columns\n",
        "slam_admissions.Admission_Date = pd.to_datetime(slam_admissions.Admission_Date, format='mixed').dt.normalize()\n",
        "slam_admissions.Discharge_Date = pd.to_datetime(slam_admissions.Discharge_Date, format='mixed').dt.normalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "163aa50a-12fe-432b-83d7-ab43b725d90e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# rename and standardise ID columns\n",
        "slam_admissions = check_id_format_and_standardise(slam_admissions, 'PatientId')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8b4c584-d2dc-4fa3-90e4-e493015b4ef4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# get counts and sets of all NHS and IDs\n",
        "patient_id_counts, pseudo_nhs_all, patient_ids_all = get_id_counts_and_sets(slam_admissions, patient_level_id_pairs)\n",
        "\n",
        "# filter to cohort and de-duplicate patient IDs\n",
        "slam_admissions = filter_to_inclusion_cohort(slam_admissions, patient_level_id_pairs.nhs)\n",
        "slam_admissions = assign_duplicate_type(slam_admissions, patient_level_id_pairs)\n",
        "slam_admissions = de_duplicate_dataframe(slam_admissions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fd9692b-b688-494b-8942-a6794fa3c479",
      "metadata": {},
      "outputs": [],
      "source": [
        "# only groups with SIMILAR statuses (multiple valid events) will result in multiple events, and therefore with some EVENT_ID > 0\n",
        "slam_admissions.EVENT_ID.value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06f2fa9a-e8a0-4d5a-b3be-6b165f4b5109",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Number of unique SLAM admissions:', slam_admissions.shape[0])\n",
        "slam_admissions.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69c2c33e-c881-4c5a-85c5-9bef0469b795",
      "metadata": {},
      "outputs": [],
      "source": [
        "# save to file\n",
        "assert slam_admissions.Admission_Date.notna().all()\n",
        "slam_admissions.to_csv(CLEANED_DATA_PATH + 'SLaM_Admissions.csv', index=False) # save newly cleaned version to CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a532d0b6-519b-4f0b-b22f-33a5eb872fb9",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### SLAM Contacts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "223049c9-2b8b-4624-8fff-5e894f79beee",
      "metadata": {},
      "source": [
        "Replace modalities with standard categories (as per LDN):\n",
        "- F2F\n",
        "- Telephone\n",
        "- Home Visit\n",
        "- Video/Email/Text\n",
        "- Other\n",
        "- Missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57f4b2d7-cde3-4f45-a23f-bb51b9d33039",
      "metadata": {},
      "outputs": [],
      "source": [
        "# copy from loading dataframe\n",
        "slam_contacts = slam_contacts_load.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dc21885-a881-4fc5-ba71-09a5e5a8aa74",
      "metadata": {},
      "outputs": [],
      "source": [
        "# format date columns\n",
        "slam_contacts.SLAM_event_date = pd.to_datetime(slam_contacts.SLAM_event_date, format='mixed').dt.normalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c524f20-5310-40cb-a78b-7b27afd4d85a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# define modality lookup table\n",
        "modality = {\n",
        " 'ECT Treatment': 'F2F',\n",
        " 'Face To Face': 'F2F',\n",
        " 'Face to Face': 'F2F',\n",
        " 'Group Contact': 'F2F',\n",
        " 'Group Contact (Contract Option)': 'F2F',\n",
        " 'Individual Contact (Contract Option)': 'F2F',\n",
        " 'Observation': 'F2F',\n",
        " 'Phone - Clinical': 'Telephone',\n",
        " 'Phone - Triage': 'Telephone',\n",
        " 'RC discussion after SOAD': 'F2F',\n",
        " 'Statutory Consultee after SOAD': 'F2F',\n",
        " 'Video (virtual) appointment': 'Video',\n",
        " 'Video (virtual) appointment (Contract Option)': 'Video',\n",
        " 'Video link': 'Video',\n",
        " 'f2f': 'F2F',\n",
        " 'na': 'Missing'\n",
        "}\n",
        "\n",
        "# parse and format modality\n",
        "slam_contacts['Modality'] = slam_contacts['event_type_of_contact'].replace(modality)\n",
        "slam_contacts['Modality'] = slam_contacts['Modality'].where((slam_contacts['Modality'] != 'Missing') | (slam_contacts['dimension_1_medium'] != 'f2f'), other='F2F')\n",
        "slam_contacts['Modality'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e5ff4ca-dc2f-4f61-afc3-af176f8c4096",
      "metadata": {},
      "outputs": [],
      "source": [
        "# rename and standardise ID columns\n",
        "slam_contacts = check_id_format_and_standardise(slam_contacts, 'PatientId')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8aa07bc-2252-40f1-8f55-8afa25376615",
      "metadata": {},
      "outputs": [],
      "source": [
        "# get counts and sets of all NHS and IDs\n",
        "patient_id_counts, pseudo_nhs_all, patient_ids_all = get_id_counts_and_sets(slam_contacts, patient_level_id_pairs)\n",
        "\n",
        "# filter to cohort and de-duplicate patient IDs\n",
        "slam_contacts = filter_to_inclusion_cohort(slam_contacts, patient_level_id_pairs.nhs)\n",
        "slam_contacts = assign_duplicate_type(slam_contacts, patient_level_id_pairs)\n",
        "slam_contacts = de_duplicate_dataframe(slam_contacts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73b5447d-7531-41e5-8230-c7edaf4df693",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Number of unique SLAM contacts:', slam_contacts.shape[0])\n",
        "slam_contacts.sample(2).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1659dba0-cacd-4bfe-adc5-6e5f6643ce71",
      "metadata": {},
      "outputs": [],
      "source": [
        "# save to file\n",
        "assert slam_contacts.SLAM_event_date.notna().all()\n",
        "slam_contacts.to_csv(CLEANED_DATA_PATH + 'SLaM_Contacts.csv', index=False) # save newly cleaned version to CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42cb0e25-481e-4774-b20a-6bb059b72ff5",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### LDN Consultations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ad56a3d-2469-4c87-bf21-431bf2dcb3e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# copy from loading dataframe\n",
        "ldn_consultations = ldn_consultations_load.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ac51361-37dc-488d-b21b-83919874beba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# format date columns (and filter to good dates)\n",
        "ldn_consultations.EffectiveDateTime = pd.to_datetime(ldn_consultations.EffectiveDateTime, format='mixed', errors='coerce')\n",
        "assert ldn_consultations.EffectiveDateTime.isna().sum() == 1 # should only be only bad date\n",
        "ldn_consultations = ldn_consultations.loc[ldn_consultations.EffectiveDateTime.notna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29852279-0c0f-4f44-a128-83d8b7e80bdc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# rename and standardise ID columns\n",
        "ldn_consultations = check_id_format_and_standardise(ldn_consultations, 'PatientId')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b229260-ef36-4423-987e-c5014b42103e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# replace HCP with high-level categories\n",
        "ldn_hcp_lookup = pd.read_excel('./Resources/LDN HCP Lookup.xlsx')\n",
        "ldn_hcp_lookup['TypeHCP'] = ldn_hcp_lookup.iloc[:, -1].replace({1:'GP', 2:'Nurse or Midwife', 3:'Other HCP or Unspecified', 4:'Admin'})\n",
        "ldn_hcp_lookup_dict = {hcp_name:hcp_type for hcp_name, hcp_type in zip(ldn_hcp_lookup['UserRoleName'], ldn_hcp_lookup['TypeHCP'])}\n",
        "\n",
        "# rename users\n",
        "ldn_consultations['userrolename'] = ldn_consultations['userrolename'].replace(ldn_hcp_lookup_dict)\n",
        "ldn_consultations['userrolename'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfa7e979-e818-46dd-ab85-577c3098d35a",
      "metadata": {},
      "source": [
        "Consultation type: 1 = GP; 2 = nurse or midwife; 3 = other HCP or unspecified clinical staff; 4 = admin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34feb6ac-b395-4faf-a95e-e2ebc054425d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# replace modality with high-level categories\n",
        "ldn_modality_lookup = pd.read_excel('./Resources/LDN Modality Lookup.xlsx')\n",
        "ldn_modality_lookup['Modality'] = ldn_modality_lookup.iloc[:, 2].replace({1:'F2F', 2:'Telephone', 3:'Home Visit', 4:'Video/Email/Text', 5:'Other', np.nan:'Other'})\n",
        "ldn_modality_lookup_dict = {mod_name:mod_type for mod_name, mod_type in zip(ldn_modality_lookup['top_ConsultationTypeDescription'], ldn_modality_lookup['Modality'])}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84fd741d-5882-4a70-83f8-02436da67c70",
      "metadata": {},
      "source": [
        "Add in other modalities present in LDN extraction, but not in lookup\n",
        "\n",
        "**NOTE** Hand-picked lookup values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0059a0b4-098d-44f1-a617-cd09f96ca309",
      "metadata": {},
      "outputs": [],
      "source": [
        "other_modality = {\n",
        " 'Consultation via SMS text message': 'Video/Email/Text',\n",
        " 'Home visit note': 'Other',\n",
        " 'Third party consultation': 'Other',\n",
        " 'Face to face consultation': 'F2F',\n",
        " 'Patient encounter data NOS': 'Other',\n",
        " 'Repeat prescription': 'Other',\n",
        " 'Consultation via telemedicine web camera': 'Video/Email/Text',\n",
        " 'Night visit note': 'Other',\n",
        " 'Emergency appointment': 'Other',\n",
        " 'Consultation via multimedia': 'Video/Email/Text',\n",
        " 'EMIS patient record transfer': 'Other',\n",
        " 'Extended hours consultation': 'Other',\n",
        " 'Discussion with other professional': 'Other',\n",
        " 'OOH report': 'Other',\n",
        " 'Diagnostic testing': 'Other',\n",
        " 'Same day appointment': 'Other',\n",
        " 'Follow up attendance face to face': 'F2F',\n",
        " 'First attendance face to face': 'F2F',\n",
        " 'Seen in other clinic': 'Other',\n",
        " 'Community care activity type': 'Other',\n",
        " 'Hospital inpatient note': 'Other',\n",
        " 'Appt cancelled by patient': 'Other',\n",
        " 'Walk-in centre': 'Other',\n",
        " 'Genito-urinary medicine': 'Other',\n",
        " 'Multidisciplinary team meeting without patient': 'Other'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f563c8b-8942-411f-8042-2ca25d0136f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# update modality lookup with manual values\n",
        "ldn_modality_lookup_dict.update(other_modality)\n",
        "\n",
        "### KEEP GRANULAR MODALITIES\n",
        "ldn_consultations['GranularModality'] = ldn_consultations['ConsultationTypeTerm']\n",
        "\n",
        "# replace modality names\n",
        "ldn_consultations['ConsultationTypeTerm'] = ldn_consultations['ConsultationTypeTerm'].replace(ldn_modality_lookup_dict)\n",
        "assert all(ldn_consultations['ConsultationTypeTerm'].dropna().isin(ldn_modality_lookup['Modality'].unique())) # all assigned category\n",
        "\n",
        "# replace nulls with Missing category\n",
        "ldn_consultations['ConsultationTypeTerm'] = ldn_consultations['ConsultationTypeTerm'].replace({np.nan:'Missing'})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84b8a58b-6ccf-4fad-bb89-b5599376e259",
      "metadata": {},
      "source": [
        "**NOTE** Choosing to merge home visits into F2F, and *Other* into *Missing*\n",
        "\n",
        "**UPDATE** *Other* into **np.nan**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a0ea814-7586-4f0c-9a1e-0cb55721761b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ldn_consultations['ConsultationTypeTerm'] = ldn_consultations['ConsultationTypeTerm'].replace({'Home Visit':'F2F', 'Other':'Missing'})\n",
        "\n",
        "ldn_consultations['ConsultationTypeTerm'] = ldn_consultations['ConsultationTypeTerm'].replace({'Home Visit':'F2F'})\n",
        "ldn_consultations = ldn_consultations.loc[ldn_consultations.ConsultationTypeTerm != 'Other'] # drop admin (other) consults\n",
        "\n",
        "ldn_consultations['ConsultationTypeTerm'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f88413be-fea4-4452-a5b7-af44ae5ff68d",
      "metadata": {},
      "outputs": [],
      "source": [
        "valid_consultation_fields = ['nhs', 'EffectiveDateTime', 'ConsultationTypeTerm', 'userrolename', 'GranularModality']\n",
        "\n",
        "# ignore patient ID, consultation ID, GP name, GP code, and just get unique consultations (based mainly on time and modality)\n",
        "ldn_consultations = ldn_consultations[valid_consultation_fields].drop_duplicates()\n",
        "\n",
        "### then for that patient (NHS) and that date, find the most recently registered GP practice for them which overlaps this date\n",
        "\n",
        "# Merge on NHS number\n",
        "ldn_consultations = ldn_consultations.merge(patient_level_gp_lookup, on='nhs', how='left')\n",
        "\n",
        "# Create mask of registrations which overlap with consultation\n",
        "mask = (\n",
        "    (ldn_consultations['registrationstartdate'] <= ldn_consultations['EffectiveDateTime']) &\n",
        "    ((ldn_consultations['RegistrationEndDate'] >= ldn_consultations['EffectiveDateTime']) | (ldn_consultations['RegistrationEndDate'].isna()))\n",
        ")\n",
        "\n",
        "# Filter to registrations that overlap with consultation date\n",
        "ldn_consultations = ldn_consultations[mask].copy()\n",
        "\n",
        "# For each consultation, keep only the most recent registration (by registrationstartdate)\n",
        "ldn_consultations = ldn_consultations.sort_values('registrationstartdate', ascending=False)\n",
        "ldn_consultations = ldn_consultations.groupby(valid_consultation_fields, as_index=False, dropna=False).first()\n",
        "\n",
        "# Drop registration dates (just keep GP code)\n",
        "ldn_consultations = ldn_consultations.drop(['registrationstartdate', 'RegistrationEndDate'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0c3510f-e5cb-49fd-ac25-8ab63547b33e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# save to file\n",
        "assert not ldn_consultations.duplicated().any()\n",
        "ldn_consultations.to_csv(CLEANED_DATA_PATH + 'LDN_Consultations.csv', index=False, quoting=csv.QUOTE_ALL) # save newly cleaned version to CSV"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
