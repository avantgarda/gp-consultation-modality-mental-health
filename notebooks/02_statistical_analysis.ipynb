{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af7db995-5b3e-4f8a-8c43-3d09ded23067",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import warnings\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import miceforest as mf\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "from scipy import stats\n",
        "from patsy import dmatrices\n",
        "from tableone import TableOne\n",
        "from IPython.display import display\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from statsmodels.discrete.count_model import ZeroInflatedNegativeBinomialP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0351417f-eb45-47d7-b525-634a44c3f7c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "### important dates\n",
        "START_DATE      = pd.to_datetime('01-01-2019', format='%d-%m-%Y') # start of all datasets\n",
        "EXTRACTION_DATE = pd.to_datetime('02-12-2022', format='%d-%m-%Y') # date of second (final) extraction from MB\n",
        "END_DATE_SLAM   = pd.to_datetime('30-06-2022', format='%d-%m-%Y') # extended SLAM extraction\n",
        "END_DATE_LDN    = pd.to_datetime('07-10-2021', format='%d-%m-%Y') # last consultation date, but spec says up to November 2021...\n",
        "\n",
        "### modifiable period parameters (in months)\n",
        "EXPOSURE_LENGTH = 6\n",
        "OUTCOME_LENGTH  = 6\n",
        "WINDOW_STEP     = 6\n",
        "PERIODS_START   = 1 # 1 is first available month\n",
        "\n",
        "### results directory\n",
        "RESULTS_DIR = '../results/'\n",
        "CLEANED_DATA_PATH = '../data/cleaned/'\n",
        "if not os.path.exists(RESULTS_DIR): os.makedirs(RESULTS_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fab7659-23d0-4b33-932c-36955495ee29",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Loading and preparing datasets\n",
        "No saved output. Run this manually first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d60e3be9-0c41-4d2b-9fbf-af651df370af",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_demos = pd.read_csv(CLEANED_DATA_PATH + 'Patient_Level.csv', parse_dates=['yearofbirth', 'dateofdeath'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b05f07a1-c3a1-476c-8a0b-de0c3610c261",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_demos_dynamic = pd.read_csv(CLEANED_DATA_PATH + 'Patient_Level_Dynamic.csv', parse_dates=['registrationstartdate', 'RegistrationEndDate'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69a26a30-86c0-4a3b-abf0-e513b06ae706",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cons_ldn = pd.read_csv(CLEANED_DATA_PATH + 'LDN_Consultations.csv', parse_dates=['EffectiveDateTime'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aa19745-f9ee-4c27-9aef-b1a3a136640f",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cons_slam = pd.read_csv(CLEANED_DATA_PATH + 'SLaM_Contacts.csv', parse_dates=['SLAM_event_date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d552460-c503-4e1e-b695-042b19f4c32f",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_adms_slam = pd.read_csv(CLEANED_DATA_PATH + 'SLaM_Admissions.csv', parse_dates=['Admission_Date', 'Discharge_Date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc266551-1184-4246-aa7b-59ddd1016673",
      "metadata": {},
      "outputs": [],
      "source": [
        "# filter to patients with at least one mental health diagnosis\n",
        "# rare sexes are set to unknown\n",
        "\n",
        "df_demos_copy = df_demos.copy()\n",
        "\n",
        "df_demos_copy = df_demos_copy.rename(columns={\n",
        "    'PseudonymisedNHSNumber':'nhs',\n",
        "    # 'SLAM_anxiety_ever':'anx',\n",
        "    # 'SLAM_depression_ever':'dep',\n",
        "    # 'SLAM_SMI_ever':'smi',\n",
        "    'LDN_N_anxiety_ever':'anx',\n",
        "    'LDN_N_depression_ever':'dep',\n",
        "    'LDN_N_SMI_ever':'smi',\n",
        "    'LDN_N_AF_ever':'af',\n",
        "    'LDN_N_heart_failure_ever':'hf',\n",
        "    'LDN_N_IHD_ever':'ihd',\n",
        "    'yearofbirth':'birth_year',\n",
        "    'dateofdeath':'dod',\n",
        "    'Sex':'sex',\n",
        "    'NationalPracticeCode':'gp',\n",
        "    'IMD_Decile':'imd',\n",
        "    'Ethnicity':'ethnicity'})\n",
        "\n",
        "df_demos_copy.sex = df_demos_copy.sex.replace({'I':pd.NA, 'U':pd.NA})\n",
        "df_demos_copy.imd = df_demos_copy.imd.astype('Int32')\n",
        "df_demos_copy.ethnicity = df_demos_copy.ethnicity.replace({'Missing':pd.NA})\n",
        "df_demos_copy[['anx', 'dep', 'smi', 'af', 'hf', 'ihd']] = df_demos_copy[['anx', 'dep', 'smi', 'af', 'hf', 'ihd']].gt(0)\n",
        "\n",
        "#### IMPORTANT\n",
        "df_demos_copy = df_demos_copy.loc[df_demos_copy[['anx', 'dep', 'smi']].any(axis=1)] # filter to MH\n",
        "# df_demos_copy = df_demos_copy.loc[df_demos_copy[['smi']].any(axis=1)] # filter to SMI\n",
        "\n",
        "df_demos_copy = df_demos_copy[['nhs', 'anx', 'dep', 'smi', 'af', 'hf', 'ihd', 'birth_year', 'dod', 'sex', 'gp', 'imd', 'ethnicity']].reset_index(drop=True)\n",
        "df_demos_copy = df_demos_copy.rename(columns={d:f\"{d}_ever\" for d in ['anx', 'dep', 'smi', 'af', 'hf', 'ihd']})\n",
        "display(df_demos_copy.sample(2).T)\n",
        "demos = df_demos_copy.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa454d55-fd32-4c5b-a0ae-57053f0398f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_demos_dynamic_copy = df_demos_dynamic.copy()\n",
        "# rename columns\n",
        "df_demos_dynamic_copy = df_demos_dynamic_copy.rename(columns={'RegistrationEndDate':'registration_end', 'registrationstartdate':'registration_start', 'NationalPracticeCode':'gp', 'IMD_Decile':'imd'})\n",
        "### match patients in demographic dataset\n",
        "df_demos_dynamic_copy = df_demos_dynamic_copy.loc[df_demos_dynamic_copy.nhs.isin(df_demos_copy.nhs)]\n",
        "# format IMD\n",
        "df_demos_dynamic_copy.imd = df_demos_dynamic_copy.imd.astype('Int32')\n",
        "# show dataframe\n",
        "display(df_demos_dynamic_copy.sample(3))\n",
        "demos_dynamic = df_demos_dynamic_copy.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c7b07a-ffaa-43e5-b02c-06bb2586d219",
      "metadata": {},
      "outputs": [],
      "source": [
        "# use only GP consultations, and de-duplicate same consultation IDs (always same modality)\n",
        "# leaving in mulitple consultations per day (can have up to 24)\n",
        "\n",
        "df_cons_ldn_copy = df_cons_ldn.loc[df_cons_ldn.userrolename.eq('GP')].copy()\n",
        "df_cons_ldn_copy = df_cons_ldn_copy.rename(columns={'PseudonymisedNHSNumber':'nhs', 'EffectiveDateTime':'date', 'ConsultationTypeTerm':'modality', 'NationalPracticeCode':'gp', 'IMD_Decile':'imd', 'GranularModality':'modality_granular'})\n",
        "\n",
        "### match patients in demographic dataset\n",
        "df_cons_ldn_copy = df_cons_ldn_copy.loc[df_cons_ldn_copy.nhs.isin(df_demos_copy.nhs)]\n",
        "\n",
        "df_cons_ldn_copy = df_cons_ldn_copy[['nhs', 'date', 'modality', 'gp', 'imd', 'modality_granular']].reset_index(drop=True)\n",
        "df_cons_ldn_copy.imd = df_cons_ldn_copy.imd.astype('Int32')\n",
        "df_cons_ldn_copy.date = df_cons_ldn_copy.date.dt.normalize()\n",
        "df_cons_ldn_copy.modality = df_cons_ldn_copy.modality.replace({'Missing':pd.NA}) \n",
        "df_cons_ldn_copy = df_cons_ldn_copy.drop_duplicates()\n",
        "granular_modalities = df_cons_ldn_copy[['nhs', 'date', 'modality', 'modality_granular']].copy() # save for descriptive stats\n",
        "df_cons_ldn_copy = df_cons_ldn_copy.drop('modality_granular', axis=1)\n",
        "display(df_cons_ldn_copy.sample(3))\n",
        "consults = df_cons_ldn_copy.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f63fedd-da8e-419d-a319-9c9f41eddb28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# attended emergency contacts only (liaison)\n",
        "\n",
        "df_cons_slam_copy = df_cons_slam.loc[df_cons_slam.team_type.eq('emergency') & df_cons_slam.dimension_2_outcome.eq('attended')].copy()\n",
        "df_cons_slam_copy.loc[df_cons_slam_copy.dimension_1_medium.eq('f2f') & df_cons_slam_copy.event_type_of_contact.eq('na'), 'Modality'] = 'F2F'\n",
        "df_cons_slam_copy.loc[df_cons_slam_copy.dimension_1_medium.eq('Indirect_clinical') & df_cons_slam_copy.event_type_of_contact.eq('na'), 'Modality'] = 'Remote (Unknown)'\n",
        "df_cons_slam_copy = df_cons_slam_copy.rename(columns={'PseudonymisedNHSNumber':'nhs', 'SLAM_event_date':'date', 'Modality':'modality', 'location_name':'location'})\n",
        "\n",
        "### inclusion criteria of cardiac/mental condition not applied - so remove extra patients not in demographic data\n",
        "df_cons_slam_copy = df_cons_slam_copy.loc[df_cons_slam_copy.nhs.isin(df_demos_copy.nhs)]\n",
        "\n",
        "df_cons_slam_copy = df_cons_slam_copy[['nhs', 'date', 'modality', 'location']].reset_index(drop=True)\n",
        "display(df_cons_slam_copy.sample(3))\n",
        "emergencies = df_cons_slam_copy.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebe496fc-c658-4632-b224-b4f80f2202c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# length-of-stay is calculated to extraction date if still admitted\n",
        "\n",
        "df_adms_slam_copy = df_adms_slam.copy()\n",
        "df_adms_slam_copy['mha'] = df_adms_slam_copy.MHA_Admission_Status_ID.isin(['Admitted On Section', 'Pre-existing Section', 'Formally detained under MHA Sec 3'])\n",
        "df_adms_slam_copy = df_adms_slam_copy.rename(columns={'PseudonymisedNHSNumber':'nhs', 'Admission_Date':'admission_date', 'Discharge_Date':'discharge_date'})\n",
        "\n",
        "### inclusion criteria of cardiac/mental condition not applied - so remove extra patients not in demographic data\n",
        "df_adms_slam_copy = df_adms_slam_copy.loc[df_adms_slam_copy.nhs.isin(df_demos_copy.nhs)]\n",
        "\n",
        "df_adms_slam_copy = df_adms_slam_copy[['nhs', 'mha', 'admission_date', 'discharge_date']].reset_index(drop=True)\n",
        "# # df_adms_slam_copy['los'] = df_adms_slam_copy.discharge_date.fillna(EXTRACTION_DATE).sub(df_adms_slam_copy.admission_date).dt.days\n",
        "display(df_adms_slam_copy.sample(3))\n",
        "admissions = df_adms_slam_copy.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12999df5-54b1-4f7d-bfb3-6e24f1dc9da9",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Summary of initial cohort\n",
        "File sub-directory: ```descriptive/```\n",
        "- Saves text to: ```summary_statistics.txt```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa27e1d-8785-4fff-a3d4-39f30ed2fa4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_summary_statistics(verbose=True, results_dir=None, ldn_dates=None, slam_dates=None):\n",
        "    \"\"\"Write summary statistics to file. If verbose=True, also print to console.\n",
        "    \n",
        "    Args:\n",
        "        verbose: If True, print to console as well as file\n",
        "        results_dir: Directory to save output file\n",
        "        ldn_dates: Tuple of (start_date, end_date) to filter LDN data. If None, uses full data.\n",
        "        slam_dates: Tuple of (start_date, end_date) to filter SLAM data. If None, uses full data.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Set date ranges\n",
        "    ldn_start, ldn_end = ldn_dates if ldn_dates is not None else (START_DATE, END_DATE_LDN)\n",
        "    slam_start, slam_end = slam_dates if slam_dates is not None else (START_DATE, END_DATE_SLAM)\n",
        "    \n",
        "    # Find patients with active GP registration during LDN period\n",
        "    # A registration overlaps if: registration_start <= ldn_end AND (registration_end >= ldn_start OR registration_end is NaT)\n",
        "    active_registrations = demos_dynamic[\n",
        "        (demos_dynamic.registration_start <= ldn_end) &\n",
        "        ((demos_dynamic.registration_end >= ldn_start) | (demos_dynamic.registration_end.isna()))\n",
        "    ]\n",
        "    active_patients = active_registrations.nhs.unique()\n",
        "    \n",
        "    # Filter LDN dataframes by date and patient subset\n",
        "    demos_filtered = demos[demos.nhs.isin(active_patients)].copy()\n",
        "    consults_filtered = consults[\n",
        "        (consults.date.between(ldn_start, ldn_end)) &\n",
        "        (consults.nhs.isin(active_patients))\n",
        "    ].copy()\n",
        "    granular_modalities_filtered = granular_modalities[\n",
        "        (granular_modalities.date.between(ldn_start, ldn_end)) &\n",
        "        (granular_modalities.nhs.isin(active_patients))\n",
        "    ].copy()\n",
        "    \n",
        "    # Filter SLAM dataframes by date and patient subset\n",
        "    emergencies_filtered = emergencies[\n",
        "        (emergencies.date.between(slam_start, slam_end)) &\n",
        "        (emergencies.nhs.isin(active_patients))\n",
        "    ].copy()\n",
        "    \n",
        "    # Filter admissions by overlap: active at any point during SLAM period\n",
        "    # admission_date <= slam_end AND (discharge_date >= slam_start OR discharge_date is NaT)\n",
        "    admissions_filtered = admissions[\n",
        "        (admissions.admission_date <= slam_end) &\n",
        "        ((admissions.discharge_date >= slam_start) | (admissions.discharge_date.isna())) &\n",
        "        (admissions.nhs.isin(active_patients))\n",
        "    ].copy()\n",
        "    \n",
        "    # Count admissions that started before the period (ongoing)\n",
        "    ongoing_admissions = (admissions_filtered.admission_date < slam_start).sum()\n",
        "    ongoing_mha = ((admissions_filtered.admission_date < slam_start) & admissions_filtered.mha).sum()\n",
        "    \n",
        "    # Open file for writing\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    output_file_path = os.path.join(results_dir, 'summary_statistics.txt')\n",
        "    \n",
        "    def tee_print(*args, file=None, **kwargs):\n",
        "        \"\"\"Print to both console and file\"\"\"\n",
        "        if verbose:\n",
        "            print(*args, **kwargs)  # Print to console\n",
        "        if file:\n",
        "            print(*args, file=file, **kwargs)  # Print to file\n",
        "    \n",
        "    with open(output_file_path, 'w') as f:\n",
        "        granular_lookup = {\n",
        "            # f2f\n",
        "            'Nursing home visit note':'Nursing home visit',\n",
        "            # remote\n",
        "            'Telephone consultation':'Telephone',\n",
        "            'Telephone triage encounter':'Telephone',\n",
        "            'Other consultation medium used':np.nan,\n",
        "            'Telephone call to a patient':'Telephone',\n",
        "            'Consultation via SMS text message':'Text',\n",
        "            'Telephone call from a patient':'Telephone',\n",
        "            'E-mail sent to patient':'Email',\n",
        "            'Telephone call to relative/carer':'Telephone',\n",
        "            'Telephone call from relative/carer':'Telephone',\n",
        "            'E-mail received from patient':'Email',\n",
        "            'Consultation via video conference':'Video',\n",
        "            'E-mail consultation':'Email',\n",
        "            'E-mail received from carer':'Email',\n",
        "            'Consultation via multimedia':np.nan,\n",
        "            'E-mail sent to carer':'Email',\n",
        "        }\n",
        "        \n",
        "        tee_print(f\"Overall statistics, pre-filtering and before period creation\\n{'-' * 60}\", end='\\n\\n', file=f)\n",
        "        \n",
        "        tee_print(f\"LDN DATA (From {pd.Timestamp(ldn_start).date()} to {pd.Timestamp(ldn_end).date()})\", file=f)\n",
        "        tee_print(f\"Total patients with active registration during period: {len(active_patients)} ({consults_filtered.nhs.nunique()} with consultations)\", file=f)\n",
        "        tee_print(f\"Total GP count: {consults_filtered.gp.nunique()}\", file=f)\n",
        "        tee_print(f\"Total GP consultation count: {consults_filtered.shape[0]}\", file=f)\n",
        "        \n",
        "        gm = granular_modalities_filtered.copy()\n",
        "        gm.modality_granular = gm.modality_granular.replace(granular_lookup)\n",
        "        remote_granular = gm.loc[gm.modality.isin(['Telephone', 'Video/Email/Text'])].modality_granular.rename('Top 3 Remote modalities').value_counts().head(4).to_string()\n",
        "        f2f_granular = gm.loc[gm.modality.eq('F2F')].modality_granular.rename('Top 3 F2F modalities').value_counts().head(3).to_string()\n",
        "        \n",
        "        tee_print(f\"Total GP consultation count, by modality:\", file=f)\n",
        "        tee_print(f\"F2F: {consults_filtered.modality.eq('F2F').sum()}\", file=f)\n",
        "        tee_print(f\"Remote: {consults_filtered.modality.isin(['Telephone', 'Video/Email/Text']).sum()}\", file=f)\n",
        "        tee_print(f\"Missing: {consults_filtered.modality.isna().sum()}\\n\", file=f)\n",
        "        \n",
        "        tee_print(f2f_granular, end='\\n\\n', file=f)\n",
        "        tee_print(remote_granular, file=f)\n",
        "        \n",
        "        bed_days = admissions_filtered[['admission_date', 'discharge_date']].fillna(slam_end).clip(lower=slam_start, upper=slam_end).diff(axis=1).iloc[:, -1].dt.days.sum()\n",
        "        \n",
        "        tee_print(f\"\\nSLAM DATA (From {pd.Timestamp(slam_start).date()} to {pd.Timestamp(slam_end).date()}) - for above patient cohort only\", file=f)\n",
        "        tee_print(f\"Total emergency liaison contact count: {emergencies_filtered.shape[0]}\", file=f)\n",
        "        tee_print(f\"Total psychiatric admission count: {admissions_filtered.shape[0]} ({ongoing_admissions} ongoing from before period)\", file=f)\n",
        "        tee_print(f\"Total psychiatric MHA-sectioned admission count: {admissions_filtered.mha.sum()} ({ongoing_mha} ongoing from before period)\", file=f)\n",
        "        tee_print(f\"Total psychiatric bed-days (in period) count: {bed_days}\", file=f)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\nSummary statistics saved to: {output_file_path}\")\n",
        "\n",
        "# # With date filtering\n",
        "# save_summary_statistics(\n",
        "#     verbose=True,\n",
        "#     results_dir=f\"{RESULTS_DIR}/descriptive\",\n",
        "#     ldn_dates=(START_DATE, END_DATE_LDN),\n",
        "#     slam_dates=(START_DATE, END_DATE_SLAM)\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54979c02-fdfc-440c-b809-f15cbdfae991",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Define periods\n",
        "File sub-directory: ```period_definition/```\n",
        "- Saves table to: ```periods.csv```\n",
        "- Saves text to: ```timeline_visualization.txt```\n",
        "- Saves figure to: ```timeline_visualization.png```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4ba2fe-930e-4f8e-a7c5-27eba1f025f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_and_visualize_periods(\n",
        "    start_date, \n",
        "    end_date_ldn, \n",
        "    end_date_slam, \n",
        "    exposure_length, \n",
        "    outcome_length, \n",
        "    window_step, \n",
        "    periods_start=1,\n",
        "    visualize=True,\n",
        "    silence=False,\n",
        "    save_png=True,  # New parameter\n",
        "    results_dir=RESULTS_DIR\n",
        "):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.font_manager as fm\n",
        "\n",
        "    assert exposure_length >= 1\n",
        "    assert outcome_length >= 1\n",
        "    assert window_step >= 1\n",
        "    assert periods_start >= 1\n",
        "    \n",
        "    # --- Core Logic ---\n",
        "    timeline_start_month = start_date.to_period('M')\n",
        "    total_months = (end_date_slam.to_period('M') - timeline_start_month).n + 1\n",
        "    char_per_month = 3\n",
        "    total_width = 6 + total_months * char_per_month\n",
        "\n",
        "    # Generate Periods\n",
        "    periods = []\n",
        "    \n",
        "    months_to_shift = periods_start - 1\n",
        "    current_start = start_date + relativedelta(months=months_to_shift)\n",
        "    \n",
        "    period_id = 1\n",
        "    \n",
        "    while True:\n",
        "        exp_end = current_start + relativedelta(months=exposure_length) - pd.Timedelta(days=1)\n",
        "        if exp_end > end_date_ldn: break\n",
        "        out_start = exp_end + pd.Timedelta(days=1)\n",
        "        out_end = out_start + relativedelta(months=outcome_length) - pd.Timedelta(days=1)\n",
        "        if out_end > end_date_slam: break\n",
        "        \n",
        "        periods.append({\n",
        "            'period_id': period_id, \n",
        "            'exposure_start': current_start, \n",
        "            'exposure_end': exp_end,\n",
        "            'outcome_start': out_start,\n",
        "            'outcome_end': out_end\n",
        "        })\n",
        "        current_start += relativedelta(months=window_step)\n",
        "        period_id += 1\n",
        "        \n",
        "    periods_df = pd.DataFrame(periods)\n",
        "\n",
        "    # --- Create output directory ---\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    \n",
        "    # --- Save DataFrame ---\n",
        "    csv_path = os.path.join(results_dir, 'periods.csv')\n",
        "    periods_df.to_csv(csv_path, index=False)\n",
        "    \n",
        "    # --- Generate detailed visualization (always for file) ---\n",
        "    viz_lines = []\n",
        "    \n",
        "    if periods_df.empty:\n",
        "        summary_line = f\"No periods generated. (Exp: {exposure_length}m, Out: {outcome_length}m, Start Month: {periods_start})\"\n",
        "        viz_lines.append(summary_line)\n",
        "    else:\n",
        "        # Generate detailed visualization\n",
        "        viz_data = []\n",
        "        for p in periods:\n",
        "            line_parts = ['   '] * total_months\n",
        "            p_id = p['period_id']\n",
        "            idx_s_exp = (p['exposure_start'].to_period('M') - timeline_start_month).n\n",
        "            idx_e_exp = ((p['exposure_start'] + relativedelta(months=exposure_length) - pd.Timedelta(days=1)).to_period('M') - timeline_start_month).n\n",
        "            idx_s_out = ((p['exposure_start'] + relativedelta(months=exposure_length)).to_period('M') - timeline_start_month).n\n",
        "            idx_e_out = (p['outcome_end'].to_period('M') - timeline_start_month).n\n",
        "            \n",
        "            if idx_e_exp >= total_months: idx_e_exp = total_months - 1\n",
        "            if idx_e_out >= total_months: idx_e_out = total_months - 1\n",
        "\n",
        "            for i in range(idx_s_exp, idx_e_exp + 1): line_parts[i] = 'e--'\n",
        "            for i in range(idx_s_out, idx_e_out + 1): line_parts[i] = 'o--'\n",
        "            viz_data.append(f\"P{p_id}|{''.join(line_parts)}|P{p_id}\")\n",
        "\n",
        "        end_ldn_month = end_date_ldn.to_period('M')\n",
        "        end_ldn_idx = (end_ldn_month - timeline_start_month).n\n",
        "        end_ldn_col = 3 + end_ldn_idx * char_per_month\n",
        "\n",
        "        def insert_vline(s, pos):\n",
        "            if 0 <= pos < len(s):\n",
        "                return s[:pos] + '|' + s[pos+1:]\n",
        "            return s\n",
        "\n",
        "        # Build detailed visualization (always for file)\n",
        "        viz_lines.append(\"=\" * total_width)\n",
        "        viz_lines.append(f\"Detailed Month-by-Month Timeline ({total_months} Months):\")\n",
        "        viz_lines.append(\"=\" * total_width)\n",
        "\n",
        "        header_line, month_line = \"   \", \"\"\n",
        "        current_year, year_start_idx = timeline_start_month.year, 0\n",
        "        for i in range(total_months):\n",
        "            m_p = timeline_start_month + i\n",
        "            if m_p.year > current_year:\n",
        "                header_line += f\"{current_year:<{(i - year_start_idx) * char_per_month}}\"\n",
        "                current_year = m_p.year\n",
        "                year_start_idx = i\n",
        "            month_line += m_p.strftime('%b')\n",
        "            \n",
        "        header_line += f\"{current_year:<{(total_months - year_start_idx) * char_per_month}}   \"\n",
        "\n",
        "        viz_lines.append(header_line)\n",
        "        viz_lines.append(\"-\" * total_width)\n",
        "        viz_lines.append(insert_vline(f\"   {month_line}   \", end_ldn_col))\n",
        "        viz_lines.append(insert_vline(\"-\" * total_width, end_ldn_col))\n",
        "        for line in viz_data: \n",
        "            viz_lines.append(insert_vline(line, end_ldn_col))\n",
        "        viz_lines.append(insert_vline(\"=\" * total_width, end_ldn_col))\n",
        "        \n",
        "        bottom_line = \" \" * total_width\n",
        "        viz_lines.append(insert_vline(bottom_line, end_ldn_col).replace(\" \" * end_ldn_col + \"|\", \"LDN END\".rjust(end_ldn_col) + \"|\"))\n",
        "\n",
        "        # Summary line for console output\n",
        "        start_str = periods_df.iloc[0]['exposure_start'].strftime('%Y-%m-%d')\n",
        "        end_str = periods_df.iloc[-1]['outcome_end'].strftime('%Y-%m-%d')\n",
        "        count = len(periods_df)\n",
        "        summary_line = f\"Generated {count} periods from {start_str} to {end_str} (Exp: {exposure_length}m, Out: {outcome_length}m, Step: {window_step}m, Start Month: {periods_start})\"\n",
        "    \n",
        "    # --- Console output based on visualize flag ---\n",
        "    if not silence:\n",
        "        if visualize:\n",
        "            # Print detailed visualization\n",
        "            for line in viz_lines:\n",
        "                print(line)\n",
        "        else:\n",
        "            # Print only summary\n",
        "            print(summary_line)\n",
        "    \n",
        "    # --- Save visualization to text file (always detailed) ---\n",
        "    viz_path = os.path.join(results_dir, 'timeline_visualization.txt')\n",
        "    with open(viz_path, 'w') as f:\n",
        "        f.write('\\n'.join(viz_lines))\n",
        "    \n",
        "    # --- Save as PNG ---\n",
        "    if save_png and not periods_df.empty:\n",
        "        png_path = os.path.join(results_dir, 'timeline_visualization.png')\n",
        "        \n",
        "        # Create figure\n",
        "        fig, ax = plt.subplots(figsize=(20, len(viz_lines) * 0.3 + 1))\n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Use monospace font\n",
        "        full_text = '\\n'.join(viz_lines)\n",
        "        ax.text(0.02, 0.98, full_text, \n",
        "                fontfamily='monospace',\n",
        "                fontsize=10,\n",
        "                verticalalignment='top',\n",
        "                horizontalalignment='left',\n",
        "                transform=ax.transAxes)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(png_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "        plt.close()\n",
        "        \n",
        "        if not silence:\n",
        "            print(f\"PNG saved to: {png_path}\")\n",
        "\n",
        "    return periods_df  \n",
        "    \n",
        "# # Example Call\n",
        "# periods_df = generate_and_visualize_periods(\n",
        "#     START_DATE, \n",
        "#     END_DATE_LDN, \n",
        "#     END_DATE_SLAM, \n",
        "#     EXPOSURE_LENGTH, \n",
        "#     OUTCOME_LENGTH, \n",
        "#     WINDOW_STEP,\n",
        "#     PERIODS_START, # 1 means start at the beginning\n",
        "#     visualize=False,\n",
        "#     save_png=True,  # New parameter\n",
        "#     results_dir=f\"{RESULTS_DIR}/period_definition/\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "722db2a0-f8bd-457f-83a4-a10372ed3455",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Populate periods with data\n",
        "No saved output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f88d3a34-2241-4fd7-b053-a354e46f775d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_cohort(\n",
        "    demos, \n",
        "    consults, \n",
        "    emergencies, \n",
        "    admissions, \n",
        "    demos_dynamic, \n",
        "    periods_df,\n",
        "    extraction_date,\n",
        "    use_single_random_period=True,\n",
        "    sample_after_eligibility=True,\n",
        "    random_seed=2025,\n",
        "    verbose=True,\n",
        "    long_stay_threshold_days=7\n",
        "):\n",
        "    \"\"\"\n",
        "    Executes the full cohort extraction pipeline.\n",
        "    \n",
        "    Args:\n",
        "        demos, consults, emergencies, admissions, demos_dynamic, periods_df: Input DataFrames.\n",
        "        extraction_date: The date used to cap open-ended records (e.g., current date).\n",
        "        use_single_random_period (bool): If True, targets 1 period per patient. If False, keeps all valid periods.\n",
        "        sample_after_eligibility (bool): If True, checks all periods first, then samples 1. \n",
        "                                         (Only applies if use_single_random_period=True).\n",
        "        random_seed (int): Seed for reproducibility.\n",
        "        verbose (bool): If True, prints detailed step logs. If False, prints 1 summary line.\n",
        "        long_stay_threshold_days (int): Minimum length of stay (in days) to count as a \"long stay\" admission.\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: The final analysis cohort.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Helper for conditional printing\n",
        "    def log(msg):\n",
        "        if verbose:\n",
        "            print(msg)\n",
        "\n",
        "    # ==============================================================================\n",
        "    # STEP 1: DATA INTEGRITY & MASTER LIST\n",
        "    # ==============================================================================\n",
        "    log(\"--- Step 1: Data Integrity Checks ---\")\n",
        "\n",
        "    # 1. Establish Master List from Demos\n",
        "    required_demo_cols = ['nhs', 'birth_year', 'dod', 'sex', 'ethnicity']\n",
        "    if not set(required_demo_cols).issubset(demos.columns):\n",
        "        raise ValueError(f\"Demos dataframe missing required columns: {set(required_demo_cols) - set(demos.columns)}\")\n",
        "\n",
        "    master_patients = demos[required_demo_cols].copy()\n",
        "    valid_nhs_set = set(master_patients['nhs'])\n",
        "    log(f\"Master Patient Index established. Total Patients: {len(master_patients):,}\")\n",
        "\n",
        "    # 2. Validate Foreign Keys\n",
        "    def validate_ids(df, name):\n",
        "        orphan_mask = ~df['nhs'].isin(valid_nhs_set)\n",
        "        orphan_count = orphan_mask.sum()\n",
        "        if orphan_count > 0:\n",
        "            log(f\"WARNING: Found {orphan_count:,} records in '{name}' with NHS numbers NOT in demos. These will be ignored.\")\n",
        "            return df[~orphan_mask].copy()\n",
        "        log(f\" - '{name}' integrity check passed.\")\n",
        "        return df\n",
        "\n",
        "    consults = validate_ids(consults, 'consults')\n",
        "    emergencies = validate_ids(emergencies, 'emergencies')\n",
        "    admissions = validate_ids(admissions, 'admissions')\n",
        "    demos_dynamic = validate_ids(demos_dynamic, 'demos_dynamic')\n",
        "\n",
        "\n",
        "    # ==============================================================================\n",
        "    # STEP 2: COHORT CANDIDATE GENERATION\n",
        "    # ==============================================================================\n",
        "    log(\"\\n--- Step 2: Period Assignment ---\")\n",
        "\n",
        "    if use_single_random_period and not sample_after_eligibility:\n",
        "        log(\"Mode: Single Random Period Assignment (Pre-Check / Random Assign First)\")\n",
        "        np.random.seed(random_seed)\n",
        "        \n",
        "        master_patients['period_id'] = np.random.choice(periods_df['period_id'].unique(), size=len(master_patients))\n",
        "        candidates_df = pd.merge(master_patients, periods_df, on='period_id', how='left')\n",
        "        \n",
        "    else:\n",
        "        if use_single_random_period and sample_after_eligibility:\n",
        "            log(\"Mode: Single Random Period Assignment (Post-Check / Maximize Retention)\")\n",
        "            log(\" -> Generating full patient-period panel first...\")\n",
        "        else:\n",
        "            log(\"Mode: Full Panel (Multi-Period)\")\n",
        "            \n",
        "        # Cartesian Product\n",
        "        candidates_df = pd.merge(\n",
        "            master_patients.assign(key=1), \n",
        "            periods_df.assign(key=1), \n",
        "            on='key'\n",
        "        ).drop('key', axis=1)\n",
        "\n",
        "    log(f\"Generated {len(candidates_df):,} candidate patient-period pairs.\")\n",
        "\n",
        "\n",
        "    # ==============================================================================\n",
        "    # STEP 3: ELIGIBILITY & ACTIVE DAYS CALCULATION\n",
        "    # ==============================================================================\n",
        "    log(\"\\n--- Step 3: Eligibility Filtering & Active Days ---\")\n",
        "\n",
        "    # 1. Death Check\n",
        "    dead_mask = (candidates_df['dod'] <= candidates_df['outcome_start'])\n",
        "    n_dead = dead_mask.sum()\n",
        "    candidates_alive = candidates_df[~dead_mask].copy()\n",
        "\n",
        "    # 2. Age Check (18+)\n",
        "    # Calculate age exactly as it was done in Step 6, but do it here now.\n",
        "    candidates_alive['age_at_exposure'] = (candidates_alive['exposure_start'] - candidates_alive['birth_year']).dt.days / 365.25\n",
        "    \n",
        "    under_18_mask = candidates_alive['age_at_exposure'] < 18\n",
        "    n_under_18 = under_18_mask.sum()\n",
        "    \n",
        "    # Filter to adults only\n",
        "    candidates_adults = candidates_alive[~under_18_mask].copy()\n",
        "\n",
        "    # 3. Registration Expansion\n",
        "    candidates_expanded = pd.merge(\n",
        "        candidates_adults,\n",
        "        demos_dynamic[['nhs', 'registration_start', 'registration_end']],\n",
        "        on='nhs',\n",
        "        how='left'\n",
        "    )\n",
        "    candidates_expanded['registration_end'] = candidates_expanded['registration_end'].fillna(extraction_date)\n",
        "\n",
        "    # A. Calculate Active Days\n",
        "    candidates_expanded['clip_start'] = candidates_expanded[['registration_start', 'exposure_start']].max(axis=1)\n",
        "    candidates_expanded['clip_end'] = candidates_expanded[['registration_end', 'exposure_end']].min(axis=1)\n",
        "    candidates_expanded['segment_days'] = (candidates_expanded['clip_end'] - candidates_expanded['clip_start']).dt.days\n",
        "    candidates_expanded['segment_days'] = candidates_expanded['segment_days'].clip(lower=0)\n",
        "\n",
        "    active_days_agg = candidates_expanded.groupby(['nhs', 'period_id'])['segment_days'].sum().reset_index(name='active_days_in_exposure')\n",
        "\n",
        "    # B. Determine Eligibility Flags\n",
        "    candidates_expanded['is_active'] = (\n",
        "        (candidates_expanded['registration_start'] < candidates_expanded['exposure_end']) & \n",
        "        (candidates_expanded['registration_end'] > candidates_expanded['exposure_start'])\n",
        "    )\n",
        "    candidates_expanded['is_partial'] = (\n",
        "        candidates_expanded['is_active'] & \n",
        "        (candidates_expanded['registration_start'] > candidates_expanded['exposure_start'])\n",
        "    )\n",
        "\n",
        "    validity_aggs = candidates_expanded.groupby(['nhs', 'period_id']).agg(\n",
        "        is_active=('is_active', 'any'),\n",
        "        is_partial=('is_partial', 'any')\n",
        "    ).reset_index()\n",
        "\n",
        "    candidates_checked = pd.merge(candidates_adults, validity_aggs, on=['nhs', 'period_id'], how='left')\n",
        "    candidates_checked = pd.merge(candidates_checked, active_days_agg, on=['nhs', 'period_id'], how='left')\n",
        "    \n",
        "    # 2. FILL NA TO ENSURE BOOLEANS (Safety net)\n",
        "    candidates_checked['is_active'] = candidates_checked['is_active'].fillna(False).astype(bool)\n",
        "    candidates_checked['is_partial'] = candidates_checked['is_partial'].fillna(False).astype(bool)\n",
        "    candidates_checked['active_days_in_exposure'] = candidates_checked['active_days_in_exposure'].fillna(0).astype(int)\n",
        "\n",
        "    # Stats\n",
        "    n_total = len(candidates_df)\n",
        "    n_unreg = len(candidates_checked[~candidates_checked['is_active']])\n",
        "    n_valid = len(candidates_checked[candidates_checked['is_active']])\n",
        "    n_partial = len(candidates_checked[candidates_checked['is_active'] & candidates_checked['is_partial']])\n",
        "\n",
        "    log(f\"Eligibility Report:\")\n",
        "    log(f\" - Total Candidates:                            {n_total:,}\")\n",
        "    log(f\" - Excluded (died on/before outcome start):     {n_dead:,}\")\n",
        "    log(f\" - Excluded (under 18 at exposure start):       {n_under_18:,}\")\n",
        "    log(f\" - Excluded (not actively registered):          {n_unreg:,}\")\n",
        "    log(f\" - Final Eligible Candidates:                   {n_valid:,}\")\n",
        "    log(f\" ---> Partially registered during exposure (noiser exposure signal):    {n_partial:,}\")\n",
        "\n",
        "    eligible_cohort = candidates_checked[candidates_checked['is_active']].copy()\n",
        "    eligible_cohort.drop(['is_active', 'is_partial'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "    # ==============================================================================\n",
        "    # STEP 4: EXPOSURE CALCULATION\n",
        "    # ==============================================================================\n",
        "    log(\"\\n--- Step 4: Calculating Exposures ---\")\n",
        "\n",
        "    cohort_consults = pd.merge(eligible_cohort, consults, on='nhs', how='left')\n",
        "    valid_consults_mask = (\n",
        "        (cohort_consults['date'] >= cohort_consults['exposure_start']) &\n",
        "        (cohort_consults['date'] < cohort_consults['exposure_end'])\n",
        "    )\n",
        "    cohort_consults = cohort_consults[valid_consults_mask].copy()\n",
        "\n",
        "    remote_mods = ['Telephone', 'Video/Email/Text']\n",
        "    cohort_consults['is_remote'] = cohort_consults['modality'].isin(remote_mods).astype(int)\n",
        "    cohort_consults['is_missing'] = (\n",
        "        cohort_consults['modality'].isna() | \n",
        "        (cohort_consults['modality'].astype(str).str.lower() == 'unknown')\n",
        "    ).astype(int)\n",
        "    cohort_consults['is_f2f'] = (\n",
        "        (cohort_consults['is_remote'] == 0) & \n",
        "        (cohort_consults['is_missing'] == 0)\n",
        "    ).astype(int)\n",
        "\n",
        "    exposure_metrics = cohort_consults.groupby(['nhs', 'period_id']).agg(\n",
        "        total_consults=('date', 'size'),\n",
        "        remote_consults=('is_remote', 'sum'),\n",
        "        f2f_consults=('is_f2f', 'sum'),\n",
        "        missing_modality_consults=('is_missing', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    cohort_with_exposure = pd.merge(eligible_cohort, exposure_metrics, on=['nhs', 'period_id'], how='left')\n",
        "    fill_cols = ['total_consults', 'remote_consults', 'f2f_consults', 'missing_modality_consults']\n",
        "    cohort_with_exposure[fill_cols] = cohort_with_exposure[fill_cols].fillna(0).astype(int)\n",
        "\n",
        "    n_before_drop = len(cohort_with_exposure)\n",
        "    cohort_with_exposure = cohort_with_exposure[cohort_with_exposure['total_consults'] > 0].copy()\n",
        "    log(f\"Dropped {n_before_drop - len(cohort_with_exposure):,} patient-periods pairs with 0 consults.\")\n",
        "\n",
        "    # --- NEW SAMPLING LOGIC ---\n",
        "    if use_single_random_period and sample_after_eligibility:\n",
        "        log(f\"\\n[Sampling] Reducing to 1 period per patient (Post-Check Mode)\")\n",
        "        log(f\" - Candidates before sampling: {len(cohort_with_exposure):,} rows\")\n",
        "        \n",
        "        cohort_with_exposure = cohort_with_exposure.groupby('nhs').sample(n=1, random_state=random_seed)\n",
        "        \n",
        "        log(f\" - Final Unique Patients:      {len(cohort_with_exposure):,} rows\")\n",
        "\n",
        "\n",
        "    # ==============================================================================\n",
        "    # STEP 5: OUTCOME CALCULATION\n",
        "    # ==============================================================================\n",
        "    log(\"\\n--- Step 5: Calculating Outcomes ---\")\n",
        "\n",
        "    # A. Emergencies\n",
        "    cohort_emerg = pd.merge(cohort_with_exposure, emergencies, on='nhs', how='left')\n",
        "    valid_emerg = cohort_emerg[\n",
        "        (cohort_emerg['date'] >= cohort_emerg['outcome_start']) &\n",
        "        (cohort_emerg['date'] < cohort_emerg['outcome_end'])\n",
        "    ]\n",
        "    emerg_counts = valid_emerg.groupby(['nhs', 'period_id']).size().reset_index(name='outcome_emergencies')\n",
        "\n",
        "    # B. Admissions\n",
        "    cohort_adm = pd.merge(cohort_with_exposure, admissions, on='nhs', how='left')\n",
        "    valid_adm = cohort_adm[\n",
        "        (cohort_adm['admission_date'] >= cohort_adm['outcome_start']) &\n",
        "        (cohort_adm['admission_date'] < cohort_adm['outcome_end'])\n",
        "    ]\n",
        "    adm_counts = valid_adm.groupby(['nhs', 'period_id']).size().reset_index(name='outcome_admissions')\n",
        "\n",
        "    # C. MHA\n",
        "    valid_mha = valid_adm[valid_adm['mha'] == True]\n",
        "    mha_counts = valid_mha.groupby(['nhs', 'period_id']).size().reset_index(name='outcome_mha_admissions')\n",
        "\n",
        "    # D. Bed Days\n",
        "    cohort_bed = pd.merge(cohort_with_exposure, admissions, on='nhs', how='left')\n",
        "    relevant_adm = cohort_bed[\n",
        "        (cohort_bed['admission_date'] < cohort_bed['outcome_end']) &\n",
        "        (cohort_bed['discharge_date'].fillna(extraction_date) >= cohort_bed['outcome_start'])\n",
        "    ].copy()\n",
        "    relevant_adm['clip_start'] = np.maximum(relevant_adm['admission_date'], relevant_adm['outcome_start'])\n",
        "    relevant_adm['clip_end'] = np.minimum(relevant_adm['discharge_date'].fillna(extraction_date), relevant_adm['outcome_end'])\n",
        "    relevant_adm['bed_days'] = (relevant_adm['clip_end'] - relevant_adm['clip_start']).dt.days\n",
        "    bed_day_sums = relevant_adm.groupby(['nhs', 'period_id'])['bed_days'].sum().reset_index(name='outcome_bed_days')\n",
        "\n",
        "    # E. Long Stay Admissions (admissions > N days)\n",
        "    valid_adm_los = valid_adm.copy()\n",
        "    valid_adm_los['length_of_stay'] = (\n",
        "        valid_adm_los['discharge_date'].fillna(extraction_date) - valid_adm_los['admission_date']\n",
        "    ).dt.days\n",
        "    long_stay_adm = valid_adm_los[valid_adm_los['length_of_stay'] > long_stay_threshold_days]\n",
        "    long_stay_counts = long_stay_adm.groupby(['nhs', 'period_id']).size().reset_index(name='outcome_long_stay_admissions')\n",
        "\n",
        "    cohort_with_outcomes = cohort_with_exposure.copy()\n",
        "    cohort_with_outcomes = pd.merge(cohort_with_outcomes, emerg_counts, on=['nhs', 'period_id'], how='left')\n",
        "    cohort_with_outcomes = pd.merge(cohort_with_outcomes, adm_counts, on=['nhs', 'period_id'], how='left')\n",
        "    cohort_with_outcomes = pd.merge(cohort_with_outcomes, mha_counts, on=['nhs', 'period_id'], how='left')\n",
        "    cohort_with_outcomes = pd.merge(cohort_with_outcomes, bed_day_sums, on=['nhs', 'period_id'], how='left')\n",
        "    cohort_with_outcomes = pd.merge(cohort_with_outcomes, long_stay_counts, on=['nhs', 'period_id'], how='left')\n",
        "\n",
        "    out_cols = ['outcome_emergencies', 'outcome_admissions', 'outcome_mha_admissions', 'outcome_bed_days', 'outcome_long_stay_admissions']\n",
        "    cohort_with_outcomes[out_cols] = cohort_with_outcomes[out_cols].fillna(0).astype(int)\n",
        "\n",
        "\n",
        "    # ==============================================================================\n",
        "    # STEP 6: FEATURE ENGINEERING & RATES\n",
        "    # ==============================================================================\n",
        "    log(\"\\n--- Step 6: Feature Engineering & Rate Calculation ---\")\n",
        "\n",
        "    cohort_sorted = cohort_with_outcomes.sort_values('exposure_start')\n",
        "    demos_dynamic_sorted = demos_dynamic.sort_values('registration_start')\n",
        "\n",
        "    # 1. Dynamic GP\n",
        "    valid_gp_hist = demos_dynamic_sorted[demos_dynamic_sorted['gp'].notna()][['nhs', 'registration_start', 'gp']]\n",
        "    cohort_w_gp = pd.merge_asof(\n",
        "        cohort_sorted,\n",
        "        valid_gp_hist,\n",
        "        left_on='exposure_start',\n",
        "        right_on='registration_start',\n",
        "        by='nhs',\n",
        "        direction='nearest'\n",
        "    )\n",
        "    cohort_w_gp.rename(columns={'gp': 'gp_at_exposure'}, inplace=True)\n",
        "    cohort_w_gp.drop('registration_start', axis=1, inplace=True)\n",
        "\n",
        "    # 2. Dynamic IMD\n",
        "    valid_imd_hist = demos_dynamic_sorted[demos_dynamic_sorted['imd'].notna()][['nhs', 'registration_start', 'imd']]\n",
        "    cohort_w_covars = pd.merge_asof(\n",
        "        cohort_w_gp,\n",
        "        valid_imd_hist,\n",
        "        left_on='exposure_start',\n",
        "        right_on='registration_start',\n",
        "        by='nhs',\n",
        "        direction='nearest'\n",
        "    )\n",
        "    cohort_w_covars.rename(columns={'imd': 'imd_at_exposure'}, inplace=True)\n",
        "    cohort_w_covars.drop('registration_start', axis=1, inplace=True)\n",
        "\n",
        "    # 3. Static Demographics\n",
        "    exclude_cols = ['nhs', 'birth_year', 'dod', 'sex', 'ethnicity', 'gp', 'imd']\n",
        "    extra_static_cols = [c for c in demos.columns if c not in exclude_cols]\n",
        "    final_cohort = pd.merge(cohort_w_covars, demos[['nhs'] + extra_static_cols], on='nhs', how='left')\n",
        "\n",
        "    # 4. Derived Features\n",
        "    final_cohort['died_in_outcome'] = (\n",
        "        (final_cohort['dod'] >= final_cohort['outcome_start']) &\n",
        "        (final_cohort['dod'] <= final_cohort['outcome_end'])\n",
        "    )\n",
        "\n",
        "    # 6. Outcome Days at Risk\n",
        "    risk_end_date = np.minimum(\n",
        "        final_cohort['dod'].fillna(final_cohort['outcome_end']),\n",
        "        final_cohort['outcome_end']\n",
        "    )\n",
        "    final_cohort['outcome_days_at_risk'] = (risk_end_date - final_cohort['outcome_start']).dt.days\n",
        "    final_cohort['outcome_days_at_risk'] = final_cohort['outcome_days_at_risk'].clip(lower=0)\n",
        "    final_cohort.drop('dod', axis=1, inplace=True)\n",
        "\n",
        "    # 7. Calculate Rates\n",
        "    exp_count_cols = ['total_consults', 'remote_consults', 'f2f_consults', 'missing_modality_consults']\n",
        "    for col in exp_count_cols:\n",
        "        final_cohort[f'{col}_rate'] = final_cohort[col] / final_cohort['active_days_in_exposure']\n",
        "\n",
        "    out_count_cols = ['outcome_emergencies', 'outcome_admissions', 'outcome_bed_days', 'outcome_mha_admissions', 'outcome_long_stay_admissions']\n",
        "    for col in out_count_cols:\n",
        "        final_cohort[f'{col}_rate'] = final_cohort[col] / final_cohort['outcome_days_at_risk']\n",
        "\n",
        "    # 8. Calculate Proportion Remote (UPDATED: Removed * 100)\n",
        "    known_consults = final_cohort['total_consults'] - final_cohort['missing_modality_consults']\n",
        "    final_cohort['pct_remote'] = np.where(\n",
        "        known_consults > 0,\n",
        "        (final_cohort['remote_consults'] / known_consults), \n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    # ==============================================================================\n",
        "    # STEP 7: FINAL ORDERING\n",
        "    # ==============================================================================\n",
        "    desired_order = [\n",
        "        'nhs', 'period_id', \n",
        "        'exposure_start', 'exposure_end', \n",
        "        'outcome_start', 'outcome_end',\n",
        "        'total_consults', 'total_consults_rate',\n",
        "        'remote_consults', 'remote_consults_rate',\n",
        "        'f2f_consults', 'f2f_consults_rate',\n",
        "        'missing_modality_consults', 'missing_modality_consults_rate',\n",
        "        'pct_remote',\n",
        "        'active_days_in_exposure',\n",
        "        'outcome_days_at_risk', \n",
        "        'died_in_outcome',\n",
        "        'outcome_emergencies', 'outcome_emergencies_rate',\n",
        "        'outcome_admissions', 'outcome_admissions_rate',\n",
        "        'outcome_bed_days', 'outcome_bed_days_rate',\n",
        "        'outcome_mha_admissions', 'outcome_mha_admissions_rate',\n",
        "        'outcome_long_stay_admissions', 'outcome_long_stay_admissions_rate',\n",
        "        'sex', 'ethnicity', \n",
        "        'age_at_exposure', 'imd_at_exposure', 'gp_at_exposure',\n",
        "        'anx_ever', 'dep_ever', 'smi_ever', 'af_ever', 'hf_ever', 'ihd_ever'\n",
        "    ]\n",
        "\n",
        "    final_analysis_cohort = final_cohort[desired_order]\n",
        "\n",
        "    log(\"\\n--- Processing Complete ---\")\n",
        "    log(f\"Final Analysis Cohort Shape: {final_analysis_cohort.shape}\")\n",
        "    \n",
        "    if not verbose:\n",
        "        print(f\"[Summary] Extraction Complete | SinglePeriod: {use_single_random_period}, SamplePostCheck: {sample_after_eligibility} | Input Patients: {len(master_patients):,} -> Final Cohort: {len(final_analysis_cohort):,}\")\n",
        "        \n",
        "    return final_analysis_cohort\n",
        "\n",
        "# # --- USAGE EXAMPLE ---\n",
        "# final_df = generate_cohort(\n",
        "#     demos, consults, emergencies, admissions, demos_dynamic, periods_df,\n",
        "#     extraction_date=EXTRACTION_DATE,\n",
        "#     use_single_random_period=True,\n",
        "#     sample_after_eligibility=True,\n",
        "#     random_seed=2025,\n",
        "#     verbose=True,\n",
        "#     long_stay_threshold_days=7  # Count admissions longer than 7 days\n",
        "# )\n",
        "\n",
        "# display(final_df.head(2).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24a76d8f-2ce2-4f3a-867c-bc6fea67f847",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Normality and parametric testing\n",
        "File sub-directory: ```distribution_checks/```\n",
        "- Saves table to: ```distribution_assumptions.csv```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00f1a593-6390-43c0-95c4-a3e21ce986ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_distribution_assumptions(df, outcome_cols, results_dir=None, verbose=True):\n",
        "    \"\"\"\n",
        "    Checks normality and overdispersion to justify Negative Binomial GEE.\n",
        "    Saves a formatted table to CSV if results_dir is provided.\n",
        "    \"\"\"\n",
        "    results_list = []\n",
        "    \n",
        "    # Setup Plotting Grid\n",
        "    n_cols = len(outcome_cols)\n",
        "    fig, axes = plt.subplots(1, n_cols, figsize=(5 * n_cols, 4))\n",
        "    if n_cols == 1: axes = [axes]\n",
        "    for i, col in enumerate(outcome_cols):\n",
        "        data = df[col].dropna()\n",
        "        \n",
        "        # 1. Descriptive Stats\n",
        "        mean_val = data.mean()\n",
        "        var_val = data.var()\n",
        "        skew_val = data.skew()\n",
        "        \n",
        "        # Overdispersion Ratio (phi)\n",
        "        dispersion_ratio = var_val / mean_val if mean_val > 0 else 0\n",
        "        # 2. Normality Test (D'Agostino's K^2 test)\n",
        "        k2, p_val = stats.normaltest(data)\n",
        "        \n",
        "        # Collect results\n",
        "        results_list.append({\n",
        "            'Variable': col,\n",
        "            'Mean': mean_val,\n",
        "            'Variance': var_val,\n",
        "            'Dispersion_Ratio': dispersion_ratio,\n",
        "            'Skew': skew_val,\n",
        "            'Normality_p_val': p_val\n",
        "        })\n",
        "        # 3. Visual Check (Histogram + KDE) - Display Only\n",
        "        sns.histplot(data, kde=True, ax=axes[i], bins=30, color='skyblue')\n",
        "        axes[i].set_title(f\"{col}\\n(Skew: {skew_val:.2f})\")\n",
        "        axes[i].set_xlabel(\"Count\")\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    if verbose:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "    \n",
        "    # --- Process Results Table ---\n",
        "    df_results = pd.DataFrame(results_list)\n",
        "    \n",
        "    # Create a formatted version for Display/Saving\n",
        "    df_formatted = df_results.copy()\n",
        "    \n",
        "    # Format Floats\n",
        "    df_formatted['Mean'] = df_formatted['Mean'].map('{:.2f}'.format)\n",
        "    df_formatted['Variance'] = df_formatted['Variance'].map('{:.2f}'.format)\n",
        "    df_formatted['Dispersion_Ratio'] = df_formatted['Dispersion_Ratio'].map('{:.2f}'.format)\n",
        "    df_formatted['Skew'] = df_formatted['Skew'].map('{:.2f}'.format)\n",
        "    \n",
        "    # Format P-values for publication (< 0.001)\n",
        "    def format_pval(p):\n",
        "        if p < 0.001:\n",
        "            return \"< 0.001\"\n",
        "        else:\n",
        "            return f\"{p:.4f}\"\n",
        "            \n",
        "    df_formatted['Normality_p_val'] = df_formatted['Normality_p_val'].apply(format_pval)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"\\n=== DISTRIBUTION ASSUMPTIONS CHECK ===\")\n",
        "        display(df_formatted)\n",
        "        \n",
        "        print(\"\\nINTERPRETATION:\")\n",
        "        print(\"1. If 'Normality p-val' < 0.05, data is NOT normal (justifies non-parametric/GLM).\")\n",
        "        print(\"2. If 'Dispersion_Ratio' > 1.0, data is OVERDISPERSED (justifies Negative Binomial over Poisson).\")\n",
        "    \n",
        "    # --- Save to CSV ---\n",
        "    if results_dir:\n",
        "        # --- Create output directory ---\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "            \n",
        "        save_path = os.path.join(results_dir, 'distribution_assumptions.csv')\n",
        "        df_formatted.to_csv(save_path, index=False)\n",
        "        if verbose:\n",
        "            print(f\"\\nSaved distribution table to: {save_path}\")\n",
        "\n",
        "# # Usage:\n",
        "# check_distribution_assumptions(\n",
        "#     final_df,\n",
        "#     ['outcome_emergencies', 'outcome_admissions', 'outcome_bed_days', 'outcome_mha_admissions'],\n",
        "#     results_dir=f\"{RESULTS_DIR}/distribution_checks\",\n",
        "#     verbose=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e66c9fb8-b142-4f34-be12-9abed316cc2f",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Missingness analysis\n",
        "File sub-directory: ```missingness/```\n",
        "- Saves table to: ```missingness_summary_counts.csv```\n",
        "- Saves table to: ```missingness_table_pct_remote.csv```\n",
        "- Saves table to: ```missingness_table_sex.csv```\n",
        "- Saves table to: ```missingness_table_imd_at_exposure.csv```\n",
        "- Saves table to: ```missingness_table_ethncity.csv```\n",
        "- Saves figure to: ```gp_missingness_grid.png```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ea71bc3-3822-48ae-8d82-6a2ad01b9b29",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. HELPER FUNCTIONS\n",
        "# ==========================================\n",
        "def format_p_value(p):\n",
        "    if pd.isna(p): return \"-\"\n",
        "    if p < 0.001: return \"<0.001 ***\"\n",
        "    if p < 0.01:  return f\"{p:.3f} **\"\n",
        "    if p < 0.05:  return f\"{p:.3f} *\"\n",
        "    return f\"{p:.3f}\"\n",
        "\n",
        "def calculate_smd(series1, series2, is_categorical=False):\n",
        "    s1 = series1.dropna()\n",
        "    s2 = series2.dropna()\n",
        "    if len(s1) == 0 or len(s2) == 0: return np.nan\n",
        "\n",
        "    if is_categorical:\n",
        "        p1 = s1.mean()\n",
        "        p2 = s2.mean()\n",
        "        var = (p1 * (1 - p1) + p2 * (1 - p2)) / 2\n",
        "        if var == 0: return 0.0\n",
        "        return (p1 - p2) / np.sqrt(var)\n",
        "    else:\n",
        "        m1, m2 = s1.mean(), s2.mean()\n",
        "        v1, v2 = s1.var(), s2.var()\n",
        "        if v1 == 0 and v2 == 0: return 0.0\n",
        "        pooled_sd = np.sqrt((v1 + v2) / 2)\n",
        "        return (m1 - m2) / pooled_sd\n",
        "\n",
        "def get_numeric_summary(series):\n",
        "    if len(series) == 0: return \"-\"\n",
        "    med = series.median()\n",
        "    q1 = series.quantile(0.25)\n",
        "    q3 = series.quantile(0.75)\n",
        "    return f\"{med:.2f} [{q1:.2f}-{q3:.2f}]\"\n",
        "\n",
        "def create_missing_summary(df):\n",
        "    missing_data = df.isnull().sum()\n",
        "    missing_percent = 100 * df.isnull().sum() / len(df)\n",
        "    summary_table = pd.concat([missing_data, missing_percent], axis=1, keys=['Missing (N)', 'Missing (%)'])\n",
        "    summary_table = summary_table[summary_table['Missing (N)'] > 0].sort_values('Missing (N)', ascending=False)\n",
        "    return summary_table.round(2)\n",
        "\n",
        "# ==========================================\n",
        "# 2. DETAILED PATTERN ANALYSIS\n",
        "# ==========================================\n",
        "def create_missingness_table(df, missing_col, columns_to_compare):\n",
        "    \"\"\"\n",
        "    Creates a comparison table for a specific missing column against a list of covariates.\n",
        "    columns_to_compare is now a required argument.\n",
        "    \"\"\"\n",
        "    missing_mask = df[missing_col].isna()\n",
        "    df_missing = df[missing_mask].copy()\n",
        "    df_present = df[~missing_mask].copy()\n",
        "    \n",
        "    if len(df_missing) == 0: return None\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    expand_cols = ['ethnicity', 'imd_at_exposure', 'period_id', 'sex']\n",
        "    \n",
        "    for col in columns_to_compare:\n",
        "        if col == missing_col or col not in df.columns: continue\n",
        "        \n",
        "        # --- LOGIC TYPE A: EXPANDED CATEGORICAL ---\n",
        "        if col in expand_cols:\n",
        "            try:\n",
        "                s_missing = df[missing_col].isna()\n",
        "                s_col = df[col].astype(str).replace('<NA>', 'nan')\n",
        "                contingency = pd.crosstab(s_missing, s_col)\n",
        "                if contingency.size > 0:\n",
        "                    stat, p, dof, ex = stats.chi2_contingency(contingency)\n",
        "                    p_str = format_p_value(p)\n",
        "                else:\n",
        "                    p_str = \"-\"\n",
        "            except:\n",
        "                p_str = \"Err\"\n",
        "\n",
        "            results.append({\n",
        "                'Characteristic': f\"{col} (Categorical)\",\n",
        "                f'Known (N={len(df_present)})': '',\n",
        "                f'Missing (N={len(df_missing)})': '',\n",
        "                'P-value': p_str,\n",
        "                'SMD': ''\n",
        "            })\n",
        "            \n",
        "            valid_vals = df[col].dropna().unique()\n",
        "            try: categories = sorted(valid_vals)\n",
        "            except: categories = valid_vals\n",
        "                \n",
        "            for cat in categories:\n",
        "                valid_pres = df_present[col].dropna()\n",
        "                valid_miss = df_missing[col].dropna()\n",
        "                \n",
        "                s_pres_bin = (valid_pres == cat).astype(int)\n",
        "                s_miss_bin = (valid_miss == cat).astype(int)\n",
        "                \n",
        "                count_pres = s_pres_bin.sum()\n",
        "                pct_pres = s_pres_bin.mean() * 100 if len(s_pres_bin) > 0 else 0\n",
        "                count_miss = s_miss_bin.sum()\n",
        "                pct_miss = s_miss_bin.mean() * 100 if len(s_miss_bin) > 0 else 0\n",
        "                \n",
        "                smd = calculate_smd(s_pres_bin, s_miss_bin, is_categorical=True)\n",
        "                \n",
        "                results.append({\n",
        "                    'Characteristic': f\"  {cat}\",\n",
        "                    f'Known (N={len(df_present)})': f\"{count_pres} ({pct_pres:.1f}%)\",\n",
        "                    f'Missing (N={len(df_missing)})': f\"{count_miss} ({pct_miss:.1f}%)\",\n",
        "                    'P-value': '',\n",
        "                    'SMD': f\"{abs(smd):.3f}\"\n",
        "                })\n",
        "\n",
        "        # --- LOGIC TYPE B: NUMERIC (Non-Parametric) ---\n",
        "        elif pd.api.types.is_numeric_dtype(df[col]) and not pd.api.types.is_bool_dtype(df[col]):\n",
        "            # Raw values (No scaling)\n",
        "            val_pres = get_numeric_summary(df_present[col])\n",
        "            val_miss = get_numeric_summary(df_missing[col])\n",
        "            \n",
        "            smd = calculate_smd(df_present[col], df_missing[col], is_categorical=False)\n",
        "            \n",
        "            try:\n",
        "                c_pres = df_present[col].dropna()\n",
        "                c_miss = df_missing[col].dropna()\n",
        "                if len(c_pres) > 0 and len(c_miss) > 0:\n",
        "                    if col == 'age_at_exposure':\n",
        "                        stat, p = stats.ttest_ind(c_pres, c_miss, equal_var=False)\n",
        "                    else:\n",
        "                        stat, p = stats.mannwhitneyu(c_pres, c_miss)\n",
        "                    p_str = format_p_value(p)\n",
        "                else:\n",
        "                    p_str = \"-\"\n",
        "            except:\n",
        "                p_str = \"Err\"\n",
        "                \n",
        "            results.append({\n",
        "                'Characteristic': f\"{col} (Median [IQR])\",\n",
        "                f'Known (N={len(df_present)})': val_pres,\n",
        "                f'Missing (N={len(df_missing)})': val_miss,\n",
        "                'P-value': p_str,\n",
        "                'SMD': f\"{abs(smd):.3f}\"\n",
        "            })\n",
        "\n",
        "        # --- LOGIC TYPE C: BOOLEAN ---\n",
        "        elif pd.api.types.is_bool_dtype(df[col]):\n",
        "            valid_pres = df_present[col].dropna()\n",
        "            valid_miss = df_missing[col].dropna()\n",
        "            \n",
        "            s_pres_bin = valid_pres.astype(int)\n",
        "            s_miss_bin = valid_miss.astype(int)\n",
        "\n",
        "            count_pres = s_pres_bin.sum()\n",
        "            pct_pres = s_pres_bin.mean() * 100 if len(s_pres_bin) > 0 else 0\n",
        "            count_miss = s_miss_bin.sum()\n",
        "            pct_miss = s_miss_bin.mean() * 100 if len(s_miss_bin) > 0 else 0\n",
        "\n",
        "            smd = calculate_smd(s_pres_bin, s_miss_bin, is_categorical=True)\n",
        "            \n",
        "            try:\n",
        "                contingency = pd.crosstab(df[missing_col].isna(), df[col].fillna(-1))\n",
        "                if contingency.size > 0:\n",
        "                    stat, p, dof, ex = stats.chi2_contingency(contingency)\n",
        "                    p_str = format_p_value(p)\n",
        "                else:\n",
        "                    p_str = \"-\"\n",
        "            except:\n",
        "                p_str = \"Err\"\n",
        "\n",
        "            results.append({\n",
        "                'Characteristic': f\"{col} (True)\",\n",
        "                f'Known (N={len(df_present)})': f\"{count_pres} ({pct_pres:.1f}%)\",\n",
        "                f'Missing (N={len(df_missing)})': f\"{count_miss} ({pct_miss:.1f}%)\",\n",
        "                'P-value': p_str,\n",
        "                'SMD': f\"{abs(smd):.3f}\"\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION & SAVING\n",
        "# ==========================================\n",
        "\n",
        "def run_missingness_analysis(df, target_vars, columns_to_compare, \n",
        "                             verbose=True, results_dir=None):\n",
        "    \"\"\"\n",
        "    Run missingness analysis.\n",
        "    \n",
        "    Required Arguments:\n",
        "    - df: The dataframe\n",
        "    - target_vars: List of columns containing missing values to analyze\n",
        "    - columns_to_compare: List of covariates to compare against\n",
        "    \n",
        "    Optional Arguments:\n",
        "    - verbose: Print output\n",
        "    - results_dir: Directory to save CSVs (default None)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Setup output directory\n",
        "    if results_dir:\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "    \n",
        "    # Summary table\n",
        "    if verbose:\n",
        "        print(\"=\"*60, \"\\n DATA COMPLETENESS SUMMARY\\n\", \"=\"*60, sep='')\n",
        "    summary_df = create_missing_summary(df)\n",
        "    if verbose:\n",
        "        display(summary_df)\n",
        "    if results_dir:\n",
        "        summary_df.to_csv(os.path.join(results_dir, \"missingness_summary_counts.csv\"))\n",
        "    \n",
        "    # Detailed patterns\n",
        "    results = {'summary': summary_df}\n",
        "    for target in target_vars:\n",
        "        if target in df.columns and df[target].isna().sum() > 0:\n",
        "            if verbose:\n",
        "                print(f\"\\n{'='*60}\\n PATTERN ANALYSIS: {target.upper()}\\n{'='*60}\")\n",
        "            \n",
        "            table = create_missingness_table(df, target, columns_to_compare)\n",
        "            \n",
        "            if table is not None:\n",
        "                results[target] = table\n",
        "                if verbose:\n",
        "                    display(table)\n",
        "                if results_dir:\n",
        "                    table.to_csv(os.path.join(results_dir, f\"missingness_table_{target}.csv\"), index=False)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# # Usage:\n",
        "# missingness_results = run_missingness_analysis(final_df, verbose=True, results_dir=f\"{RESULTS_DIR}/missingness\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f23c8509-bcc0-4ef3-a3da-8248975be4b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_gp_missingness_grid(df, targets, gp_col='gp_at_exposure', min_patients=50, verbose=True, results_dir=RESULTS_DIR):\n",
        "    \"\"\"\n",
        "    Analyzes missing data rates across GPs for multiple targets.\n",
        "    Produces a 2x2 grid of 'Caterpillar Plots'.\n",
        "    \"\"\"\n",
        "\n",
        "    VARIABLE_RENAME = {\n",
        "        'ethnicity':'Ethnicity', \n",
        "        'imd_at_exposure':'IMD', \n",
        "        'sex':'Sex', \n",
        "        'pct_remote':'Consultation Modality'\n",
        "    }\n",
        "    \n",
        "    # Filter valid targets that exist in DF\n",
        "    valid_targets = [t for t in targets if t in df.columns]\n",
        "    n_targets = len(valid_targets)\n",
        "    \n",
        "    if n_targets == 0:\n",
        "        print(\"No valid targets found in dataframe.\")\n",
        "        return\n",
        "\n",
        "    # Setup Grid (2x2 if 4 items, or adjust dynamically)\n",
        "    cols = 2\n",
        "    rows = (n_targets + 1) // 2\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 6 * rows))\n",
        "    axes = axes.flatten() # Flatten to 1D array for easy iteration\n",
        "    \n",
        "    if verbose: print(f\"Generating GP Missingness Grid for: {valid_targets}...\")\n",
        "\n",
        "    for i, target_col in enumerate(valid_targets):\n",
        "        ax = axes[i]\n",
        "        \n",
        "        # 1. Create Binary Missing Flag\n",
        "        # For pct_remote, missing is NaN. For others, it's also NaN.\n",
        "        is_missing = df[target_col].isna().astype(int)\n",
        "        \n",
        "        # 2. Group by GP\n",
        "        gp_stats = pd.DataFrame({\n",
        "            'gp': df[gp_col],\n",
        "            'is_missing': is_missing\n",
        "        }).groupby('gp').agg(\n",
        "            total_patients=('is_missing', 'count'),\n",
        "            missing_count=('is_missing', 'sum'),\n",
        "            missing_pct=('is_missing', 'mean')\n",
        "        ).reset_index()\n",
        "        \n",
        "        # Convert to %\n",
        "        gp_stats['missing_pct'] = gp_stats['missing_pct'] * 100\n",
        "        \n",
        "        # 3. Filter small GPs\n",
        "        gp_stats = gp_stats[gp_stats['total_patients'] >= min_patients].copy()\n",
        "        \n",
        "        # 4. Sort (Worst first)\n",
        "        gp_stats = gp_stats.sort_values('missing_pct', ascending=False)\n",
        "        \n",
        "        # 5. Stats\n",
        "        mean_miss = gp_stats['missing_pct'].mean()\n",
        "        std_miss = gp_stats['missing_pct'].std()\n",
        "        threshold = mean_miss + std_miss\n",
        "        \n",
        "        # 6. Plot on specific axis\n",
        "        colors = ['red' if x > threshold else 'steelblue' for x in gp_stats['missing_pct']]\n",
        "        \n",
        "        sns.barplot(\n",
        "            x='missing_pct', y='gp', data=gp_stats, \n",
        "            hue='gp', palette=colors, orient='h', legend=False,\n",
        "            ax=ax\n",
        "        )\n",
        "        \n",
        "        # Add Mean Line\n",
        "        ax.axvline(mean_miss, color='black', linestyle='--', alpha=0.7)\n",
        "        \n",
        "        # Formatting\n",
        "        ax.set_title(f'Missing {VARIABLE_RENAME[target_col]}', fontsize=14, fontweight='bold')\n",
        "        ax.set_xlabel('% Missing')\n",
        "        ax.set_ylabel('') # Remove label to save space\n",
        "        ax.set_yticks([]) # Anonymize GP codes\n",
        "        ax.grid(axis='x', alpha=0.3)\n",
        "        \n",
        "        # Add Comprehensive Info Box (Bottom Right)\n",
        "        stats_text = (\n",
        "            f\"Mean: {mean_miss:.1f}%\\n\"\n",
        "            f\"SD: {std_miss:.1f}%\\n\"\n",
        "            f\"Range: {gp_stats['missing_pct'].min():.1f}% - {gp_stats['missing_pct'].max():.1f}%\\n\"\n",
        "            f\"\\n\"\n",
        "            f\"-- Line: Average\\n\"\n",
        "            f\" Red: > Mean + 1 SD\"\n",
        "        )\n",
        "        \n",
        "        ax.text(0.95, 0.05, stats_text, transform=ax.transAxes, \n",
        "                fontsize=10, verticalalignment='bottom', horizontalalignment='right',\n",
        "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.95, edgecolor='gray'))\n",
        "\n",
        "    # Hide empty subplots if any\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Setup output directory and save\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    save_path = os.path.join(results_dir, \"gp_missingness_grid.png\")\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    if verbose:\n",
        "        print(f\"Grid saved to: {save_path}\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION\n",
        "# ==========================================\n",
        "# analyze_gp_missingness_grid(final_df, ['ethnicity', 'imd_at_exposure', 'sex', 'pct_remote'], verbose=True, results_dir=f\"{RESULTS_DIR}/missingness\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a41e95e-bda3-4f05-8ad1-d368801b0599",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Missing data and imputation\n",
        "File sub-directory: ```imputation/```\n",
        "- Saves text to: ```complete_case_summary.txt```\n",
        "- Saves text to: ```variables_used.txt```\n",
        "- Saves table to: ```imputation_stats_ethnicity.csv```\n",
        "- Saves table to: ```imputation_stats_sex.csv```\n",
        "- Saves table to: ```imputation_stats_imd_at_exposure.csv```\n",
        "- Saves table to: ```imputation_stats_pct_remote.csv```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29adb60f-38b7-4c3f-9d30-2ee766e150a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_missing_data(\n",
        "    df, \n",
        "    cols_to_impute, \n",
        "    targets_to_update,\n",
        "    complete_case=False,\n",
        "    keep_pct_remote_nan=False,\n",
        "    verbose=True,\n",
        "    n_datasets=5,        \n",
        "    n_iterations=100,    \n",
        "    mice_cycles=5,       \n",
        "    random_state=42,\n",
        "    results_dir=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Handles missing data via MICE imputation or Complete Case Analysis.\n",
        "    Saves metrics/summaries to results_dir if provided, but NOT the datasets.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create a working copy\n",
        "    df_in = df.copy()\n",
        "    \n",
        "    # Ensure results directory exists if provided\n",
        "    if results_dir and not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "        if verbose: print(f\"Created directory: {results_dir}\")\n",
        "\n",
        "    # --- NEW: Save Variables List to File ---\n",
        "    if results_dir:\n",
        "        vars_path = os.path.join(results_dir, 'variables_used.txt')\n",
        "        with open(vars_path, 'w') as f:\n",
        "            f.write(\"=== Variables Provided for Imputation ===\\n\")\n",
        "            for col in cols_to_impute:\n",
        "                # Check if the variable actually exists in the DF\n",
        "                status = \"OK\" if col in df_in.columns else \"MISSING IN DATAFRAME\"\n",
        "                f.write(f\"{col} : {status}\\n\")\n",
        "            \n",
        "            f.write(\"\\n=== Targets Being Updated ===\\n\")\n",
        "            for col in targets_to_update:\n",
        "                f.write(f\"{col}\\n\")\n",
        "        if verbose: print(f\"Saved variables list to: {vars_path}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # PATH A: COMPLETE CASE ANALYSIS\n",
        "    # ==========================================\n",
        "    if complete_case:\n",
        "        subset_cols = [c for c in targets_to_update if c in df_in.columns]\n",
        "        \n",
        "        if keep_pct_remote_nan and 'pct_remote' in subset_cols:\n",
        "            subset_cols.remove('pct_remote')\n",
        "            \n",
        "        original_len = len(df_in)\n",
        "        df_cc = df_in.dropna(subset=subset_cols).copy()\n",
        "        rows_dropped = original_len - len(df_cc)\n",
        "        \n",
        "        # --- Report String ---\n",
        "        report_str = (\n",
        "            \"=== COMPLETE CASE SUMMARY ===\\n\"\n",
        "            f\"Original rows: {original_len}\\n\"\n",
        "            f\"Rows dropped:  {rows_dropped} ({rows_dropped/original_len:.1%})\\n\"\n",
        "            f\"Remaining:     {len(df_cc)}\\n\"\n",
        "        )\n",
        "        if keep_pct_remote_nan:\n",
        "            report_str += \"Note: Rows with missing 'pct_remote' were retained.\\n\"\n",
        "        \n",
        "        if verbose:\n",
        "            print(\"\\n\" + report_str)\n",
        "            \n",
        "        # --- Saving Logic ---\n",
        "        if results_dir:\n",
        "            # Save the Summary Report ONLY (No dataset)\n",
        "            with open(os.path.join(results_dir, 'complete_case_summary.txt'), 'w') as f:\n",
        "                f.write(report_str)\n",
        "            if verbose: print(f\"Saved Complete Case summary to: {results_dir}\")\n",
        "        \n",
        "        return df_cc\n",
        "\n",
        "    # ==========================================\n",
        "    # PATH B: MICE IMPUTATION\n",
        "    # ==========================================\n",
        "    \n",
        "    # 1. Prepare Model Data\n",
        "    cols_to_use = [c for c in cols_to_impute if c in df_in.columns]\n",
        "    df_model = df_in[cols_to_use].copy()\n",
        "    \n",
        "    categorical_cols = ['ethnicity', 'sex']\n",
        "    for col in categorical_cols:\n",
        "        if col in df_model.columns:\n",
        "            df_model[col] = df_model[col].astype('category')\n",
        "\n",
        "    if 'imd_at_exposure' in df_model.columns:\n",
        "        df_model['imd_at_exposure'] = df_model['imd_at_exposure'].astype(float)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Running MICE (LightGBM) on {len(cols_to_use)} columns...\")\n",
        "        print(f\"Settings: {n_datasets} datasets, {mice_cycles} cycles, {n_iterations} trees per model.\")\n",
        "\n",
        "    # 2. Run MICE\n",
        "    kds = mf.ImputationKernel(\n",
        "        df_model,\n",
        "        datasets=n_datasets,\n",
        "        save_all_iterations=True,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    \n",
        "    kds.mice(iterations=mice_cycles, verbose=0, num_iterations=n_iterations) \n",
        "\n",
        "    # 3. Finalize Datasets\n",
        "    imputed_datasets = {}\n",
        "    \n",
        "    if verbose: print(f\"\\nProcessing {n_datasets} imputed datasets (Internal Only)...\")\n",
        "    for i in range(n_datasets):\n",
        "        df_completed = kds.complete_data(i)\n",
        "        \n",
        "        # Post-Processing\n",
        "        if 'imd_at_exposure' in df_completed.columns:\n",
        "            df_completed['imd_at_exposure'] = df_completed['imd_at_exposure'].round().clip(1, 10)\n",
        "        if 'pct_remote' in df_completed.columns:\n",
        "            df_completed['pct_remote'] = df_completed['pct_remote'].clip(0, 100)\n",
        "            \n",
        "        # Merge back\n",
        "        df_temp = df_in.copy()\n",
        "        for col in targets_to_update:\n",
        "            if col in df_completed.columns:\n",
        "                df_temp[col] = df_completed[col]\n",
        "        \n",
        "        imputed_datasets[i] = df_temp\n",
        "\n",
        "    # 4. Reporting (Using Dataset 0)\n",
        "    final_cohort_imputed = imputed_datasets[0]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n=== IMPUTATION SUMMARY (Dataset 0 of {n_datasets}) ===\")\n",
        "    \n",
        "    # --- Categorical Comparisons ---\n",
        "    check_cols = ['ethnicity', 'sex', 'imd_at_exposure']\n",
        "    for col in check_cols:\n",
        "        if col in df_in.columns:\n",
        "            try:\n",
        "                # A. CALCULATION (Run always)\n",
        "                before = df_in[col].value_counts(dropna=False).sort_index().rename(\"Before\")\n",
        "                after = final_cohort_imputed[col].value_counts(dropna=False).sort_index().rename(\"After\")\n",
        "                comp = pd.concat([before, after], axis=1).fillna(0).astype(int)\n",
        "                comp['Change'] = comp['After'] - comp['Before']\n",
        "                \n",
        "                # RENAME NaNs for CSV clarity\n",
        "                comp.index = comp.index.map(lambda x: 'Missing' if pd.isna(x) else x)\n",
        "\n",
        "                # B. SAVING (Run if directory exists)\n",
        "                if results_dir:\n",
        "                    comp.to_csv(os.path.join(results_dir, f'imputation_stats_{col}.csv'))\n",
        "                    \n",
        "                # C. PRINTING (Run only if verbose)\n",
        "                if verbose:\n",
        "                    print(f\"\\n--- {col} Distribution ---\")\n",
        "                    try:\n",
        "                        display(comp)\n",
        "                    except:\n",
        "                        print(comp)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                if verbose: print(f\"Could not display/save {col}: {e}\")\n",
        "\n",
        "    # --- Continuous Statistics (pct_remote) ---\n",
        "    if 'pct_remote' in df_in.columns:\n",
        "        # A. CALCULATION (Run always)\n",
        "        stats_before = df_in['pct_remote'].describe().rename(\"Before\")\n",
        "        stats_after = final_cohort_imputed['pct_remote'].describe().rename(\"After\")\n",
        "        \n",
        "        miss_before = pd.Series({'missing_count': df_in['pct_remote'].isna().sum()}, name=\"Before\")\n",
        "        miss_after = pd.Series({'missing_count': final_cohort_imputed['pct_remote'].isna().sum()}, name=\"After\")\n",
        "        \n",
        "        stats_compare = pd.concat([stats_before, stats_after], axis=1)\n",
        "        stats_compare = pd.concat([stats_compare, pd.concat([miss_before, miss_after], axis=1).T]).T\n",
        "        \n",
        "        # B. SAVING (Run if directory exists)\n",
        "        if results_dir:\n",
        "            stats_compare.to_csv(os.path.join(results_dir, 'imputation_stats_pct_remote.csv'))\n",
        "\n",
        "        # C. PRINTING (Run only if verbose)\n",
        "        if verbose:\n",
        "            print(\"\\n--- pct_remote Statistics ---\")\n",
        "            try:\n",
        "                display(stats_compare.round(2))\n",
        "            except:\n",
        "                print(stats_compare.round(2))\n",
        "\n",
        "    return imputed_datasets\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION BLOCKS\n",
        "# ==========================================\n",
        "\n",
        "# cols_to_impute_list = [\n",
        "#     # --- TARGETS ---\n",
        "#     'sex', 'ethnicity', 'imd_at_exposure', 'pct_remote',\n",
        "#     # --- PREDICTORS ---\n",
        "#     'age_at_exposure',\n",
        "#     'total_consults', 'total_consults_rate',\n",
        "#     'remote_consults', 'remote_consults_rate',\n",
        "#     'f2f_consults', 'f2f_consults_rate',\n",
        "#     'missing_modality_consults', 'missing_modality_consults_rate',\n",
        "#     'outcome_emergencies', 'outcome_emergencies_rate',\n",
        "#     'outcome_admissions', 'outcome_admissions_rate',\n",
        "#     'outcome_bed_days', 'outcome_bed_days_rate',\n",
        "#     'outcome_mha_admissions', 'outcome_mha_admissions_rate',\n",
        "#     'active_days_in_exposure',\n",
        "#     'outcome_days_at_risk',\n",
        "#     'died_in_outcome',\n",
        "#     'anx_ever', 'dep_ever', 'smi_ever', \n",
        "#     'af_ever', 'hf_ever', 'ihd_ever'\n",
        "# ]\n",
        "\n",
        "# # 1. RUN COMPLETE CASE (Dropping pct_remote NaNs)\n",
        "# # -----------------------------------------------\n",
        "# print(\"Running Complete Case Analysis...\")\n",
        "# df_complete_case = handle_missing_data(\n",
        "#     df=final_df,\n",
        "#     cols_to_impute=cols_to_impute_list, \n",
        "#     targets_to_update=['ethnicity', 'imd_at_exposure', 'sex', 'pct_remote'],\n",
        "#     complete_case=True,\n",
        "#     keep_pct_remote_nan=False, \n",
        "#     verbose=True,\n",
        "#     results_dir=f\"{RESULTS_DIR}/imputation\"\n",
        "# )\n",
        "\n",
        "# # 2. RUN IMPUTATION (Standard 5 datasets)\n",
        "# # -----------------------------------------------\n",
        "# print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "# print(\"Running MICE Imputation...\")\n",
        "# imputed_dict = handle_missing_data(\n",
        "#     df=final_df,\n",
        "#     cols_to_impute=cols_to_impute_list,\n",
        "#     targets_to_update=['ethnicity', 'imd_at_exposure', 'sex', 'pct_remote'],\n",
        "#     complete_case=False,\n",
        "#     keep_pct_remote_nan=False,\n",
        "#     verbose=True,\n",
        "#     n_datasets=5,\n",
        "#     n_iterations=25, # 100 for final run\n",
        "#     mice_cycles=5,    \n",
        "#     results_dir=f\"{RESULTS_DIR}/imputation\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98bf0421-e7f7-4b97-af92-efb4eac3cc99",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Create baseline characteristics tables\n",
        "File sub-directory: ```descriptive/```\n",
        "- Saves table to: ```baseline_characteristics_initial_cohort.csv```\n",
        "- Saves table to: ```baseline_characteristics_sample_imputed_cohort.csv```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18d70f91-670e-4601-ad61-d218492ae3ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_table_one(df, save_dir=None, filename='Table1_Baseline.csv', \n",
        "                       groupby_mode='remote', show_pval=True, \n",
        "                       remote_as_continuous=False, verbose=True,\n",
        "                       selected_columns=None):\n",
        "    \"\"\"\n",
        "    Generates a publication-ready Table 1 with custom filename support.\n",
        "    \"\"\"\n",
        "    # 1. SETUP & CLEANING\n",
        "    df_table = df.copy()\n",
        "\n",
        "    # Clean missing values\n",
        "    replace_vals = ['None', 'none', 'NaN', 'nan', 'Missing', 'missing']\n",
        "    df_table['sex'] = df_table['sex'].replace(replace_vals, np.nan)\n",
        "    df_table['ethnicity'] = df_table['ethnicity'].replace(replace_vals, np.nan)\n",
        "    df_table = df_table.dropna(subset=['pct_remote'])\n",
        "\n",
        "    # 2. CREATE BINS (Trimodal Split)\n",
        "    bins = [-0.1, 0.0001, 0.999, 1.1]\n",
        "    labels = ['Never Remote (0%)', 'Hybrid Usage', 'Fully Remote (100%)']\n",
        "    df_table['Remote_Group'] = pd.cut(df_table['pct_remote'], bins=bins, labels=labels).astype(str)\n",
        "\n",
        "    # 3. CONVERT BOOLEANS\n",
        "    bool_cols = ['anx_ever', 'dep_ever', 'smi_ever', 'af_ever', 'hf_ever', 'ihd_ever']\n",
        "    for col in bool_cols:\n",
        "        df_table[col] = df_table[col].replace({True: 'Yes', False: 'No'})\n",
        "\n",
        "    # 4. CONFIGURATION (mode-dependent)\n",
        "    if groupby_mode == 'remote':\n",
        "        groupby_col = 'Remote_Group'\n",
        "        columns = [\n",
        "            'age_at_exposure', 'sex', 'ethnicity', 'imd_at_exposure', \n",
        "            'total_consults', 'period_id',           \n",
        "            'anx_ever', 'dep_ever', 'smi_ever', \n",
        "            'af_ever', 'hf_ever', 'ihd_ever'\n",
        "        ]\n",
        "        categorical = [\n",
        "            'sex', 'ethnicity', 'period_id', \n",
        "            'anx_ever', 'dep_ever', 'smi_ever', \n",
        "            'af_ever', 'hf_ever', 'ihd_ever'\n",
        "        ]\n",
        "        nonnormal = ['imd_at_exposure', 'total_consults']\n",
        "        \n",
        "    elif groupby_mode == 'period':\n",
        "        groupby_col = 'period_id'\n",
        "        \n",
        "        if remote_as_continuous:\n",
        "            remote_var = 'pct_remote'\n",
        "            columns = [\n",
        "                'age_at_exposure', 'sex', 'ethnicity', 'imd_at_exposure', \n",
        "                'total_consults', 'pct_remote',           \n",
        "                'anx_ever', 'dep_ever', 'smi_ever', \n",
        "                'af_ever', 'hf_ever', 'ihd_ever'\n",
        "            ]\n",
        "            categorical = [\n",
        "                'sex', 'ethnicity', \n",
        "                'anx_ever', 'dep_ever', 'smi_ever', \n",
        "                'af_ever', 'hf_ever', 'ihd_ever'\n",
        "            ]\n",
        "            nonnormal = ['imd_at_exposure', 'total_consults', 'pct_remote']\n",
        "        else:\n",
        "            remote_var = 'Remote_Group'\n",
        "            columns = [\n",
        "                'age_at_exposure', 'sex', 'ethnicity', 'imd_at_exposure', \n",
        "                'total_consults', 'Remote_Group',           \n",
        "                'anx_ever', 'dep_ever', 'smi_ever', \n",
        "                'af_ever', 'hf_ever', 'ihd_ever'\n",
        "            ]\n",
        "            categorical = [\n",
        "                'sex', 'ethnicity', 'Remote_Group', \n",
        "                'anx_ever', 'dep_ever', 'smi_ever', \n",
        "                'af_ever', 'hf_ever', 'ihd_ever'\n",
        "            ]\n",
        "            nonnormal = ['imd_at_exposure', 'total_consults']\n",
        "    else:\n",
        "        raise ValueError(\"groupby_mode must be 'remote' or 'period'\")\n",
        "\n",
        "    rename_dict = {\n",
        "        'age_at_exposure': 'Age (years)',\n",
        "        'imd_at_exposure': 'IMD Decile (Deprivation)',\n",
        "        'period_id': 'Study Period',\n",
        "        'total_consults': 'Total Consultations',\n",
        "        'anx_ever': 'History of Anxiety',\n",
        "        'dep_ever': 'History of Depression',\n",
        "        'smi_ever': 'History of SMI',\n",
        "        'af_ever': 'History of Atrial Fib',\n",
        "        'hf_ever': 'History of Heart Failure',\n",
        "        'ihd_ever': 'History of IHD',\n",
        "        'sex': 'Sex',\n",
        "        'ethnicity': 'Ethnicity',\n",
        "        'Remote_Group': 'Remote Consult Usage',\n",
        "        'pct_remote': 'Remote Consult % (continuous)'\n",
        "    }\n",
        "\n",
        "    # Force 'Yes' to top and limit to 1 row\n",
        "    order = {k: ['Yes', 'No'] for k in bool_cols}\n",
        "    limit = {k: 1 for k in bool_cols}\n",
        "\n",
        "    # --- UPDATED FILTER LOGIC START ---\n",
        "    if selected_columns:\n",
        "        # 1. Filter the main columns list\n",
        "        columns = [c for c in columns if c in selected_columns]\n",
        "        \n",
        "        # 2. Filter config lists\n",
        "        categorical = [c for c in categorical if c in columns]\n",
        "        nonnormal = [c for c in nonnormal if c in columns]\n",
        "        \n",
        "        # 3. Filter order and limit to remove keys for columns we just deleted\n",
        "        order = {k: v for k, v in order.items() if k in columns}\n",
        "        limit = {k: v for k, v in limit.items() if k in columns}\n",
        "    # --- UPDATED FILTER LOGIC END ---\n",
        "\n",
        "    # 5. GENERATE TABLE\n",
        "    mytable = TableOne(\n",
        "        df_table, \n",
        "        columns=columns, \n",
        "        categorical=categorical, \n",
        "        groupby=groupby_col, \n",
        "        nonnormal=nonnormal, \n",
        "        rename=rename_dict, \n",
        "        limit=limit,        \n",
        "        order=order,        \n",
        "        pval=show_pval,\n",
        "        missing=False,      \n",
        "        include_null=False, \n",
        "        sort=False,         \n",
        "        overall=True,       \n",
        "        smd=False            \n",
        "    )\n",
        "\n",
        "    # 6. POST-PROCESSING\n",
        "    final_output = mytable.tableone.copy()\n",
        "\n",
        "    if isinstance(final_output.columns, pd.MultiIndex):\n",
        "        final_output.columns = final_output.columns.get_level_values(1)\n",
        "\n",
        "    if groupby_mode == 'remote':\n",
        "        priority_cols = ['Overall', 'Never Remote (0%)', 'Hybrid Usage', 'Fully Remote (100%)']\n",
        "    elif groupby_mode == 'period':\n",
        "        period_ids = sorted(df_table['period_id'].dropna().unique())\n",
        "        priority_cols = ['Overall'] + [str(pid) for pid in period_ids]\n",
        "    \n",
        "    all_cols = list(final_output.columns)\n",
        "    new_order = [c for c in priority_cols if c in all_cols] + \\\n",
        "                [c for c in all_cols if c not in priority_cols]\n",
        "    final_output = final_output[new_order]\n",
        "\n",
        "    if show_pval:\n",
        "        def add_stars(val):\n",
        "            try:\n",
        "                if isinstance(val, str) and '<' in val: return val + '***'\n",
        "                p = float(val)\n",
        "                if p < 0.001: return f\"{p:.3f}***\"\n",
        "                if p < 0.01:  return f\"{p:.3f}**\"\n",
        "                if p < 0.05:  return f\"{p:.3f}*\"\n",
        "                return f\"{p:.3f}\"\n",
        "            except:\n",
        "                return val\n",
        "\n",
        "        if 'P-Value' in final_output.columns:\n",
        "            final_output['P-Value'] = final_output['P-Value'].apply(add_stars)\n",
        "\n",
        "    # 7. DISPLAY & SAVE\n",
        "    if verbose: display(final_output)\n",
        "\n",
        "    if save_dir is None:\n",
        "        save_dir = '.' \n",
        "    \n",
        "    save_path = os.path.join(save_dir, filename)\n",
        "    final_output.to_csv(save_path)\n",
        "    if verbose: print(f\" Table saved successfully to: {save_path}\")\n",
        "    \n",
        "    return final_output\n",
        "\n",
        "# # generate table 1 for final (non-imputed dataset)\n",
        "# table1_non_impute = generate_table_one(\n",
        "#     final_df, \n",
        "#     f\"{RESULTS_DIR}/descriptive\", \n",
        "#     filename='baseline_characteristics_initial_cohort.csv', \n",
        "#     verbose=True,\n",
        "#     groupby_mode='period',\n",
        "#     show_pval=False,\n",
        "#     remote_as_continuous=True\n",
        "# )\n",
        "\n",
        "# # generate table 1 for (one sample) imputed dataset\n",
        "# sampled_imputed_dataset = imputed_dict[0].copy()\n",
        "# table1_impute = generate_table_one(\n",
        "#     sampled_imputed_dataset, \n",
        "#     f\"{RESULTS_DIR}/descriptive\", \n",
        "#     filename='baseline_characteristics_sample_imputed_cohort.csv', \n",
        "#     verbose=True,\n",
        "#     groupby_mode='period',\n",
        "#     show_pval=False,\n",
        "#     remote_as_continuous=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e69d521a-f208-483e-9662-c95d12a5f636",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Alpha parameter refinement\n",
        "File sub-directory: ```regression/```\n",
        "- Saves table to: ```gee_CC_alpha_optimisation_results.csv```\n",
        "- Saves table to: ```gee_MICE_alpha_optimisation_results_pooled.csv```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8774381d-2c1e-456f-ab18-0e13782c5f9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def optimize_gee_alpha(\n",
        "    df,\n",
        "    outcomes,\n",
        "    alphas_to_test,\n",
        "    formula_template,\n",
        "    offset_col='outcome_days_at_risk',\n",
        "    grouping_col='gp_at_exposure',\n",
        "    verbose=True,\n",
        "    save_dir=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Optimize alpha parameter for Negative Binomial GEE models using QIC.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame or dict\n",
        "        Single dataframe for complete case analysis, or dict of dataframes for \n",
        "        multiple imputation (will pool QIC values across imputations)\n",
        "    outcomes : list\n",
        "        List of outcome variable names to test\n",
        "    alphas_to_test : list\n",
        "        List of alpha values to test for each outcome (must be > 0)\n",
        "    formula_template : str\n",
        "        Formula template with {outcome} placeholder.\n",
        "        Example: \"{outcome} ~ pct_remote_10 + total_consults + ...\"\n",
        "    offset_col : str, default='outcome_days_at_risk'\n",
        "        Column name for offset variable (will be log-transformed)\n",
        "    grouping_col : str, default='gp_at_exposure'\n",
        "        Column name for grouping variable in GEE\n",
        "    verbose : bool, default=True\n",
        "        If True, displays graphs and results table. If False, suppresses all output.\n",
        "    save_dir : str, optional\n",
        "        Directory path to save QIC results table as CSV. If None, no file is saved.\n",
        "        Files will be labeled with \"_CC\" (complete case) or \"_MICE\" (multiple imputation).\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary mapping each outcome to its optimal alpha value\n",
        "        \n",
        "    Note:\n",
        "    -----\n",
        "    Alpha cannot be zero - it must be positive. Alpha = 0 would correspond to \n",
        "    a Poisson distribution (no overdispersion), while alpha > 0 allows for \n",
        "    overdispersion in the Negative Binomial model.\n",
        "    \n",
        "    When df is a dict (multiple imputation), QIC values are averaged across all\n",
        "    imputed datasets before selecting the optimal alpha. Only alphas with successful\n",
        "    convergence are considered - models that fail to converge are excluded.\n",
        "    \"\"\"\n",
        "    # Validate alpha values\n",
        "    if any(a <= 0 for a in alphas_to_test):\n",
        "        raise ValueError(\"All alpha values must be positive (> 0). Alpha = 0 would be Poisson, not Negative Binomial.\")\n",
        "    \n",
        "    # Determine if we're doing multiple imputation\n",
        "    is_imputed = isinstance(df, dict)\n",
        "    analysis_type = \"MICE\" if is_imputed else \"CC\"\n",
        "    \n",
        "    # Validate save directory\n",
        "    if save_dir is not None:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    # Initialize storage\n",
        "    optimal_alphas = {}\n",
        "    convergence_failures = []\n",
        "    other_failures = []\n",
        "    \n",
        "    if is_imputed:\n",
        "        # Multiple imputation: pool QICs across all imputed datasets\n",
        "        n_imputations = len(df)\n",
        "        \n",
        "        qic_by_imputation = {imp_idx: {outcome: {} for outcome in outcomes} \n",
        "                            for imp_idx in df.keys()}\n",
        "        \n",
        "        # Run optimization on each imputed dataset\n",
        "        for imp_idx, imp_df in df.items():\n",
        "            working_df = imp_df.copy()\n",
        "            \n",
        "            # Create log offset\n",
        "            working_df['log_offset'] = np.log(working_df[offset_col].replace(0, 1e-5))\n",
        "            \n",
        "            # Create pct_remote_10 if not present\n",
        "            if 'pct_remote_10' not in working_df.columns:\n",
        "                if 'pct_remote' in working_df.columns:\n",
        "                    working_df['pct_remote_10'] = working_df['pct_remote'] * 10\n",
        "                else:\n",
        "                    raise ValueError(\"'pct_remote' column not found in dataframe\")\n",
        "            \n",
        "            # Test each outcome and alpha for this imputation\n",
        "            for outcome in outcomes:\n",
        "                current_formula = formula_template.format(outcome=outcome)\n",
        "                \n",
        "                for alpha in alphas_to_test:\n",
        "                    try:\n",
        "                        model = smf.gee(\n",
        "                            formula=current_formula,\n",
        "                            data=working_df,\n",
        "                            groups=working_df[grouping_col],\n",
        "                            offset=working_df['log_offset'],\n",
        "                            family=sm.families.NegativeBinomial(alpha=alpha),\n",
        "                            cov_struct=sm.cov_struct.Exchangeable()\n",
        "                        )\n",
        "                        \n",
        "                        # Catch convergence warnings\n",
        "                        with warnings.catch_warnings(record=True) as w:\n",
        "                            warnings.simplefilter(\"always\")\n",
        "                            res = model.fit()\n",
        "                            \n",
        "                            # Check for convergence issues\n",
        "                            convergence_failed = any(\n",
        "                                \"convergence\" in str(warning.message).lower() or\n",
        "                                \"iteration limit\" in str(warning.message).lower()\n",
        "                                for warning in w\n",
        "                            )\n",
        "                            \n",
        "                            if convergence_failed:\n",
        "                                failure_key = (outcome, alpha)\n",
        "                                if failure_key not in convergence_failures:\n",
        "                                    convergence_failures.append(failure_key)\n",
        "                                qic_by_imputation[imp_idx][outcome][alpha] = np.nan\n",
        "                            else:\n",
        "                                qic, _ = res.qic(scale=1.0)\n",
        "                                qic_by_imputation[imp_idx][outcome][alpha] = qic\n",
        "                            \n",
        "                    except Exception as e:\n",
        "                        failure_key = (outcome, alpha, str(e))\n",
        "                        if failure_key not in other_failures:\n",
        "                            other_failures.append(failure_key)\n",
        "                        qic_by_imputation[imp_idx][outcome][alpha] = np.nan\n",
        "        \n",
        "        # Pool QICs by averaging across imputations\n",
        "        qic_matrix = {outcome: {} for outcome in outcomes}\n",
        "        for outcome in outcomes:\n",
        "            for alpha in alphas_to_test:\n",
        "                qic_values = [qic_by_imputation[i][outcome].get(alpha, np.nan) \n",
        "                             for i in df.keys()]\n",
        "                valid_qics = [q for q in qic_values if not np.isnan(q)]\n",
        "                \n",
        "                if valid_qics:\n",
        "                    qic_matrix[outcome][alpha] = np.mean(valid_qics)\n",
        "                else:\n",
        "                    qic_matrix[outcome][alpha] = np.nan\n",
        "        \n",
        "    else:\n",
        "        # Single dataset (complete case)\n",
        "        working_df = df.copy()\n",
        "        \n",
        "        # Create log offset\n",
        "        working_df['log_offset'] = np.log(working_df[offset_col].replace(0, 1e-5))\n",
        "        \n",
        "        # Create pct_remote_10 if not present\n",
        "        if 'pct_remote_10' not in working_df.columns:\n",
        "            if 'pct_remote' in working_df.columns:\n",
        "                working_df['pct_remote_10'] = working_df['pct_remote'] * 10\n",
        "            else:\n",
        "                raise ValueError(\"'pct_remote' column not found in dataframe\")\n",
        "        \n",
        "        qic_matrix = {outcome: {} for outcome in outcomes}\n",
        "        \n",
        "        # Main loop through outcomes\n",
        "        for outcome in outcomes:\n",
        "            current_formula = formula_template.format(outcome=outcome)\n",
        "            \n",
        "            # Test each alpha\n",
        "            for alpha in alphas_to_test:\n",
        "                try:\n",
        "                    model = smf.gee(\n",
        "                        formula=current_formula,\n",
        "                        data=working_df,\n",
        "                        groups=working_df[grouping_col],\n",
        "                        offset=working_df['log_offset'],\n",
        "                        family=sm.families.NegativeBinomial(alpha=alpha),\n",
        "                        cov_struct=sm.cov_struct.Exchangeable()\n",
        "                    )\n",
        "                    \n",
        "                    # Catch convergence warnings\n",
        "                    with warnings.catch_warnings(record=True) as w:\n",
        "                        warnings.simplefilter(\"always\")\n",
        "                        res = model.fit()\n",
        "                        \n",
        "                        # Check for convergence issues\n",
        "                        convergence_failed = any(\n",
        "                            \"convergence\" in str(warning.message).lower() or\n",
        "                            \"iteration limit\" in str(warning.message).lower()\n",
        "                            for warning in w\n",
        "                        )\n",
        "                        \n",
        "                        if convergence_failed:\n",
        "                            convergence_failures.append((outcome, alpha))\n",
        "                            qic_matrix[outcome][alpha] = np.nan\n",
        "                        else:\n",
        "                            qic, _ = res.qic(scale=1.0)\n",
        "                            qic_matrix[outcome][alpha] = qic\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    other_failures.append((outcome, alpha, str(e)))\n",
        "                    qic_matrix[outcome][alpha] = np.nan\n",
        "    \n",
        "    # Find optimal alphas (only from successfully converged models)\n",
        "    for outcome in outcomes:\n",
        "        valid_qics = {k: v for k, v in qic_matrix[outcome].items() if not np.isnan(v)}\n",
        "        \n",
        "        if valid_qics:\n",
        "            best_alpha = min(valid_qics, key=valid_qics.get)\n",
        "            optimal_alphas[outcome] = best_alpha\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\"  WARNING: Could not determine alpha for {outcome} (All models failed)\")\n",
        "            optimal_alphas[outcome] = None  # Don't use a fallback - make it explicit\n",
        "    \n",
        "    if verbose:\n",
        "        # Create combined plot with all outcomes\n",
        "        n_outcomes = len(outcomes)\n",
        "        n_cols = 2\n",
        "        n_rows = (n_outcomes + 1) // 2\n",
        "        \n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows))\n",
        "        if n_outcomes == 1:\n",
        "            axes = np.array([axes])\n",
        "        axes = axes.flatten()\n",
        "        \n",
        "        for idx, outcome in enumerate(outcomes):\n",
        "            valid_qics = {k: v for k, v in qic_matrix[outcome].items() if not np.isnan(v)}\n",
        "            \n",
        "            if valid_qics:\n",
        "                axes[idx].plot(list(valid_qics.keys()), list(valid_qics.values()), \n",
        "                              marker='o', linewidth=2, markersize=8)\n",
        "                best_alpha = optimal_alphas.get(outcome)\n",
        "                if best_alpha is not None:\n",
        "                    axes[idx].axvline(best_alpha, color='red', linestyle='--', alpha=0.5, \n",
        "                                     label=f'Best ={best_alpha}')\n",
        "                title_suffix = \" (pooled)\" if is_imputed else \"\"\n",
        "                axes[idx].set_title(f\"{outcome}\\nBest Alpha: {best_alpha}{title_suffix}\", \n",
        "                                   fontsize=11, fontweight='bold')\n",
        "                axes[idx].set_xlabel(\"Alpha\", fontsize=10)\n",
        "                axes[idx].set_ylabel(\"QIC\" + (\" (pooled)\" if is_imputed else \"\"), fontsize=10)\n",
        "                axes[idx].grid(True, alpha=0.3)\n",
        "                axes[idx].legend()\n",
        "            else:\n",
        "                axes[idx].text(0.5, 0.5, f'{outcome}\\nAll models failed', \n",
        "                              ha='center', va='center', transform=axes[idx].transAxes)\n",
        "                axes[idx].set_xlabel(\"Alpha\", fontsize=10)\n",
        "                axes[idx].set_ylabel(\"QIC\", fontsize=10)\n",
        "        \n",
        "        # Hide extra subplots if odd number of outcomes\n",
        "        for idx in range(n_outcomes, len(axes)):\n",
        "            axes[idx].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Display failure summary\n",
        "        if convergence_failures or other_failures:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"FAILURE SUMMARY:\")\n",
        "            print(\"=\"*50)\n",
        "            \n",
        "            if convergence_failures:\n",
        "                print(\"\\n  Convergence failures (excluded from recommendations):\")\n",
        "                # Group by outcome\n",
        "                conv_by_outcome = {}\n",
        "                for item in convergence_failures:\n",
        "                    outcome = item[0]\n",
        "                    alpha = item[1]\n",
        "                    if outcome not in conv_by_outcome:\n",
        "                        conv_by_outcome[outcome] = []\n",
        "                    if alpha not in conv_by_outcome[outcome]:\n",
        "                        conv_by_outcome[outcome].append(alpha)\n",
        "                \n",
        "                for outcome, alphas in conv_by_outcome.items():\n",
        "                    alphas_str = \", \".join([f\"={a}\" for a in sorted(alphas)])\n",
        "                    print(f\"  {outcome}: {alphas_str}\")\n",
        "            \n",
        "            if other_failures:\n",
        "                print(\"\\n Other failures:\")\n",
        "                for outcome, alpha, error in other_failures:\n",
        "                    print(f\"  {outcome}, ={alpha}: {error[:80]}\")\n",
        "        \n",
        "        # Display results table\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"QIC RESULTS TABLE ({analysis_type}):\")\n",
        "        print(\"=\"*50)\n",
        "        results_df = pd.DataFrame(qic_matrix)\n",
        "        results_df.index.name = 'alpha'\n",
        "        display(results_df.round(0))\n",
        "        \n",
        "        # Display final recommendations\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"FINAL RECOMMENDATION ({analysis_type}):\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"Use these alphas for your final models:\")\n",
        "        for outcome, alpha in optimal_alphas.items():\n",
        "            if alpha is not None:\n",
        "                print(f\"  {outcome}: {alpha}\")\n",
        "            else:\n",
        "                print(f\"  {outcome}: NO VALID ALPHA (all models failed)\")\n",
        "        print(\"=\"*50)\n",
        "    \n",
        "    # Save results table\n",
        "    if save_dir is not None:\n",
        "        results_df = pd.DataFrame(qic_matrix)\n",
        "        results_df.index.name = 'alpha'\n",
        "        results_path = os.path.join(save_dir, f'gee_{analysis_type}_alpha_optimization_results{'_pooled' if (analysis_type == 'MICE') else ''}.csv')\n",
        "        results_df.to_csv(results_path)\n",
        "    \n",
        "    return optimal_alphas\n",
        "\n",
        "\n",
        "# Example usage - Complete Case\n",
        "# optimal_alphas_cc = optimize_gee_alpha(\n",
        "#     df=complete_case_df,\n",
        "#     outcomes=OUTCOMES,\n",
        "#     alphas_to_test=alphas_to_test,\n",
        "#     formula_template=formula_template,\n",
        "#     save_dir=f\"{RESULTS_DIR}/regression\",\n",
        "#     verbose=True\n",
        "# )\n",
        "\n",
        "# Example usage - Multiple Imputation (pooled)\n",
        "# optimal_alphas_mice = optimize_gee_alpha(\n",
        "#     df=imputed_dict,  # Pass the entire dict\n",
        "#     outcomes=OUTCOMES,\n",
        "#     alphas_to_test=alphas_to_test,\n",
        "#     formula_template=formula_template,\n",
        "#     save_dir=f\"{RESULTS_DIR}/regression\",\n",
        "#     verbose=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9844fb3c-d81b-4b9a-80a0-e6e97e394bb7",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Perform regression\n",
        "File sub-directory: ```regression/```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99ef8a39-3bd4-4908-8a17-972e93b3c183",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. HELPER: DHARMa DIAGNOSTICS\n",
        "# ==============================================================================\n",
        "def _save_dharma_diagnostics(models_dict, df, alpha_dict, save_dir, mode_label, verbose=True, show_plots=False, n_sims=250):\n",
        "    \"\"\"\n",
        "    Generates DHARMa residual plots (QQ and Res-vs-Pred).\n",
        "    \"\"\"\n",
        "\n",
        "    OUTCOME_RENAME = {\n",
        "        'outcome_emergencies':'Emergency contacts', \n",
        "        'outcome_admissions':'Psychiatric hospital admissions', \n",
        "        'outcome_bed_days':'Inpatient bed-days', \n",
        "        'outcome_mha_admissions':'MHA admissions', \n",
        "        # 'outcome_long_stay_admissions':'Long-stay psychiatric hospital admissions' \n",
        "    }\n",
        "    \n",
        "    outcomes = list(models_dict.keys())\n",
        "    n_outcomes = len(outcomes)\n",
        "    \n",
        "    if n_outcomes == 0:\n",
        "        return\n",
        "\n",
        "    # Calculate grid dimensions\n",
        "    n_cols = math.ceil(math.sqrt(n_outcomes))\n",
        "    n_rows = math.ceil(n_outcomes / n_cols)\n",
        "    \n",
        "    # Create figures\n",
        "    fig_qq, axes_qq = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
        "    fig_rvp, axes_rvp = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
        "    \n",
        "    # Flatten axes\n",
        "    if n_outcomes > 1:\n",
        "        axes_qq = axes_qq.flatten()\n",
        "        axes_rvp = axes_rvp.flatten()\n",
        "    else:\n",
        "        axes_qq = [axes_qq]\n",
        "        axes_rvp = [axes_rvp]\n",
        "        \n",
        "    if verbose:\n",
        "        print(f\"   Generating DHARMa diagnostics ({mode_label}) for {n_outcomes} outcomes...\")\n",
        "    \n",
        "    for i, outcome in enumerate(outcomes):\n",
        "        model = models_dict[outcome]\n",
        "        alpha = alpha_dict.get(outcome, 1.0)\n",
        "        \n",
        "        # Simulate\n",
        "        mu = model.fittedvalues\n",
        "        y_observed = df[outcome].values\n",
        "        n_param = 1.0 / alpha\n",
        "        p_param = 1.0 / (1.0 + alpha * mu)\n",
        "        n_obs = len(y_observed)\n",
        "        \n",
        "        simulated_responses = np.zeros((n_obs, n_sims))\n",
        "        for s in range(n_sims):\n",
        "            simulated_responses[:, s] = stats.nbinom.rvs(n=n_param, p=p_param)\n",
        "            \n",
        "        # Dithering\n",
        "        obs_dithered = y_observed[:, None] + np.random.uniform(-0.5, 0.5, (n_obs, 1))\n",
        "        sim_dithered = simulated_responses + np.random.uniform(-0.5, 0.5, (n_obs, n_sims))\n",
        "        \n",
        "        rank = np.sum(sim_dithered <= obs_dithered, axis=1)\n",
        "        residuals = rank / n_sims\n",
        "        \n",
        "        # KS Test\n",
        "        ks_stat, ks_pval = stats.kstest(residuals, 'uniform')\n",
        "        title_text = f\"{OUTCOME_RENAME[outcome]}\\nKS p={ks_pval:.3f}\"\n",
        "        \n",
        "        # Plot QQ\n",
        "        ax_q = axes_qq[i]\n",
        "        sm.qqplot(residuals, dist=stats.uniform, line='45', ax=ax_q)\n",
        "        ax_q.set_title(title_text, fontsize=10, fontweight='bold')\n",
        "        ax_q.set_xlabel(\"Theoretical\", fontsize=9)\n",
        "        ax_q.set_ylabel(\"Observed\", fontsize=9)\n",
        "        \n",
        "        # Plot Res vs Pred\n",
        "        ax_r = axes_rvp[i]\n",
        "        ax_r.scatter(mu, residuals, alpha=0.2, s=10, color='black')\n",
        "        ax_r.axhline(0.5, color='red', linestyle='--', linewidth=1)\n",
        "        ax_r.axhline(0.25, color='gray', linestyle=':', alpha=0.5)\n",
        "        ax_r.axhline(0.75, color='gray', linestyle=':', alpha=0.5)\n",
        "        ax_r.set_xscale('log')\n",
        "        ax_r.set_title(f\"{OUTCOME_RENAME[outcome]}\", fontsize=10, fontweight='bold')\n",
        "        ax_r.set_xlabel(\"Predicted (Log)\", fontsize=9)\n",
        "        ax_r.set_ylabel(\"Scaled Residual\", fontsize=9)\n",
        "        ax_r.set_ylim(-0.05, 1.05)\n",
        "\n",
        "    # Cleanup\n",
        "    for j in range(i + 1, len(axes_qq)):\n",
        "        axes_qq[j].axis('off')\n",
        "        axes_rvp[j].axis('off')\n",
        "\n",
        "    # Save\n",
        "    fig_qq.suptitle(f\"Supplementary Figure A ({mode_label}): Uniformity Check (QQ Plots)\", fontsize=14, y=0.99)\n",
        "    fig_qq.tight_layout()\n",
        "    qq_path = os.path.join(save_dir, f\"gee_{mode_label}_dharma_qq_plots.png\")\n",
        "    fig_qq.savefig(qq_path, dpi=300)\n",
        "    \n",
        "    fig_rvp.suptitle(f\"Supplementary Figure B ({mode_label}): Dispersion Check (Res vs Pred)\", fontsize=14, y=0.99)\n",
        "    fig_rvp.tight_layout()\n",
        "    rvp_path = os.path.join(save_dir, f\"gee_{mode_label}_dharma_residuals_vs_predicted.png\")\n",
        "    fig_rvp.savefig(rvp_path, dpi=300)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"    Saved QQ plots to: {qq_path}\")\n",
        "        print(f\"    Saved Res/Pred plots to: {rvp_path}\")\n",
        "    \n",
        "    if not show_plots:\n",
        "        plt.close(fig_qq)\n",
        "        plt.close(fig_rvp)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. CORE: GEE ANALYSIS (Supports Pooling & Diagnostics)\n",
        "# ==============================================================================\n",
        "def run_gee_analysis(\n",
        "    data,\n",
        "    outcomes,\n",
        "    formula_template,\n",
        "    exposure_var='pct_remote_10',\n",
        "    alpha_dict=None,\n",
        "    fixed_alpha=2.0,\n",
        "    verbose=True,\n",
        "    full_output=True,\n",
        "    save_tables=False,\n",
        "    results_dir=None,\n",
        "    save_diagnostics=False,\n",
        "    show_plots=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Core function to run GEE. Handles both Complete Case (Single) and MICE (Pooled).\n",
        "    \"\"\"\n",
        "    if (save_tables or save_diagnostics) and results_dir is None:\n",
        "        raise ValueError(\"results_dir required for saving\")\n",
        "    if (save_tables or save_diagnostics) and not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "        \n",
        "    if alpha_dict is None: alpha_dict = {}\n",
        "\n",
        "    # Detect Mode\n",
        "    if isinstance(data, dict):\n",
        "        mode = 'MICE'\n",
        "        datasets = data\n",
        "        n_datasets = len(datasets)\n",
        "        mode_label = 'MICE'\n",
        "    elif isinstance(data, pd.DataFrame):\n",
        "        mode = 'Complete Case'\n",
        "        datasets = {0: data}\n",
        "        n_datasets = 1\n",
        "        mode_label = 'CC'\n",
        "    else:\n",
        "        raise TypeError(\"data must be DataFrame or dict\")\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*60}\\nGEE CORE ANALYSIS | Mode: {mode}\\n{'='*60}\")\n",
        "    \n",
        "    # Storage\n",
        "    summary_results = []\n",
        "    all_coefficients = {}\n",
        "    all_fit_stats = []\n",
        "    all_mice_diagnostics = [] # For MICE mode\n",
        "    all_models = {} \n",
        "\n",
        "    # --- MAIN LOOP ---\n",
        "    for outcome in outcomes:\n",
        "        outcome_alpha = alpha_dict.get(outcome, fixed_alpha)\n",
        "        \n",
        "        # Pooled Storage\n",
        "        all_params, all_vcovs, fit_stats_list = [], [], []\n",
        "        param_names = None\n",
        "        \n",
        "        for i, (key, df) in enumerate(datasets.items()):\n",
        "            try:\n",
        "                working_df = df.copy()\n",
        "                working_df['log_offset'] = np.log(working_df['outcome_days_at_risk'].replace(0, 1e-5))\n",
        "                if 'pct_remote_10' not in working_df.columns:\n",
        "                    working_df['pct_remote_10'] = working_df['pct_remote'] * 10\n",
        "                \n",
        "                # Fit\n",
        "                formula = formula_template.format(outcome=outcome)\n",
        "                # with warnings.catch_warnings():\n",
        "                #     warnings.simplefilter(\"ignore\")\n",
        "                #     model = smf.gee(\n",
        "                #         formula=formula, data=working_df, groups=working_df['gp_at_exposure'],\n",
        "                #         offset=working_df['log_offset'],\n",
        "                #         family=sm.families.NegativeBinomial(alpha=outcome_alpha),\n",
        "                #         cov_struct=sm.cov_struct.Exchangeable()\n",
        "                #     )\n",
        "                #     fit = model.fit()\n",
        "                model = smf.gee(\n",
        "                    formula=formula, data=working_df, groups=working_df['gp_at_exposure'],\n",
        "                    offset=working_df['log_offset'],\n",
        "                    family=sm.families.NegativeBinomial(alpha=outcome_alpha),\n",
        "                    cov_struct=sm.cov_struct.Exchangeable()\n",
        "                )\n",
        "                fit = model.fit()\n",
        "\n",
        "                # Save Model for Diagnostics (Dataset 0 only)\n",
        "                if save_diagnostics and i == 0:\n",
        "                    all_models[outcome] = fit\n",
        "                \n",
        "                if param_names is None: param_names = fit.params.index.tolist()\n",
        "                \n",
        "                all_params.append(fit.params.values)\n",
        "                all_vcovs.append(fit.cov_params().values)\n",
        "                \n",
        "                # Fit Stats\n",
        "                try:\n",
        "                    qic_vals = fit.qic(scale=fit.scale)\n",
        "                    qic, qicu = qic_vals[0], qic_vals[1]\n",
        "                except:\n",
        "                    qic, qicu = np.nan, np.nan\n",
        "                    \n",
        "                fit_stats_list.append({\n",
        "                    'n_obs': int(fit.nobs), 'n_groups': len(working_df['gp_at_exposure'].unique()),\n",
        "                    'scale': fit.scale, 'qic': qic, 'qicu': qicu\n",
        "                })\n",
        "                \n",
        "            except Exception as e:\n",
        "                if verbose: print(f\"   Dataset {i} failed: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # --- POOLING (Rubin's Rules) ---\n",
        "        if not all_params: continue\n",
        "        \n",
        "        all_params = np.array(all_params)\n",
        "        all_vcovs = np.array(all_vcovs)\n",
        "        m = len(all_params)\n",
        "        \n",
        "        if mode == 'Complete Case' or m == 1:\n",
        "            pooled_params = all_params[0]\n",
        "            pooled_se = np.sqrt(np.diag(all_vcovs[0]))\n",
        "            df_dof = 1000 \n",
        "            fmi, riv = np.nan, np.nan\n",
        "        else:\n",
        "            Q_bar = np.mean(all_params, axis=0)\n",
        "            U_bar = np.mean(all_vcovs, axis=0)\n",
        "            B = np.cov(all_params.T, ddof=1)\n",
        "            if B.ndim == 0: B = np.array([[B]])\n",
        "            T = U_bar + (1 + 1/m) * B\n",
        "            pooled_params = Q_bar\n",
        "            pooled_se = np.sqrt(np.diag(T))\n",
        "            \n",
        "            # MICE Stats\n",
        "            r = (1 + 1/m) * np.diag(B) / np.maximum(np.diag(U_bar), 1e-10)\n",
        "            fmi = (r + 2/(np.maximum(r, 1e-10) + 3)) / (r + 1)\n",
        "            riv = r\n",
        "            df_dof = (m - 1) * (1 + 1/np.maximum(r, 1e-10))**2\n",
        "\n",
        "        # Inference\n",
        "        t_stats = pooled_params / pooled_se\n",
        "        p_vals = 2 * (1 - stats.t.cdf(np.abs(t_stats), df=np.minimum(df_dof, 1000)))\n",
        "        ci_low = pooled_params - 1.96 * pooled_se\n",
        "        ci_high = pooled_params + 1.96 * pooled_se\n",
        "\n",
        "        # Build Results Tables\n",
        "        coef_df = pd.DataFrame({\n",
        "            'Coefficient': param_names, 'Beta': pooled_params, 'SE': pooled_se,\n",
        "            'IRR': np.exp(pooled_params), 'IRR_CI_Low': np.exp(ci_low), 'IRR_CI_High': np.exp(ci_high),\n",
        "            'P_Value': p_vals, 'Significant': ['*' if p < 0.05 else '' for p in p_vals]\n",
        "        })\n",
        "        \n",
        "        # MICE Diagnostics (FMI/RIV)\n",
        "        if mode == 'MICE': \n",
        "            coef_df['FMI'] = fmi\n",
        "            coef_df['RIV'] = riv\n",
        "            \n",
        "            # Store for MICE diagnostic file\n",
        "            mice_diag = pd.DataFrame({\n",
        "                'Coefficient': param_names,\n",
        "                'FMI': fmi,\n",
        "                'RIV': riv,\n",
        "                'Outcome': outcome\n",
        "            })\n",
        "            all_mice_diagnostics.append(mice_diag)\n",
        "        \n",
        "        all_coefficients[outcome] = coef_df\n",
        "        \n",
        "        # Summary Row (Primary Exposure)\n",
        "        try:\n",
        "            exp_idx = param_names.index(exposure_var)\n",
        "            summary_results.append({\n",
        "                'Outcome': outcome, 'IRR': np.exp(pooled_params[exp_idx]),\n",
        "                'CI_Low': np.exp(ci_low[exp_idx]), 'CI_High': np.exp(ci_high[exp_idx]),\n",
        "                'P_Value': p_vals[exp_idx], 'Significant': '*' if p_vals[exp_idx] < 0.05 else '',\n",
        "                'Method': mode\n",
        "            })\n",
        "        except: pass\n",
        "        \n",
        "        # Fit Stats Row\n",
        "        avg_fit = {\n",
        "            'Outcome': outcome, 'Alpha': outcome_alpha, 'Method': mode,\n",
        "            'N_Obs': int(np.mean([f['n_obs'] for f in fit_stats_list])),\n",
        "            'QIC': np.mean([f['qic'] for f in fit_stats_list])\n",
        "        }\n",
        "        all_fit_stats.append(avg_fit)\n",
        "\n",
        "    # --- COMPILING ---\n",
        "    summary_df = pd.DataFrame(summary_results)\n",
        "    fit_stats_df = pd.DataFrame(all_fit_stats)\n",
        "    \n",
        "    # Create Formatted Summary (The Pretty One)\n",
        "    display_summary = summary_df.copy()\n",
        "    display_summary['IRR'] = display_summary['IRR'].apply(lambda x: f\"{x:.3f}\")\n",
        "    display_summary['95% CI'] = display_summary.apply(\n",
        "        lambda r: f\"{r['CI_Low']:.3f}-{r['CI_High']:.3f}\", axis=1\n",
        "    )\n",
        "    display_summary['P-value'] = display_summary.apply(\n",
        "        lambda r: f\"{r['P_Value']:.4f}{r['Significant']}\", axis=1\n",
        "    )\n",
        "    display_summary = display_summary[['Outcome', 'IRR', '95% CI', 'P-value', 'Method']]\n",
        "    \n",
        "    # Display\n",
        "    if verbose:\n",
        "        print(f\"\\nSUMMARY RESULTS ({mode}):\")\n",
        "        display(display_summary)\n",
        "        \n",
        "    # Saving\n",
        "    if save_tables:\n",
        "        # 1. Summary (Raw)\n",
        "        summary_df.to_csv(os.path.join(results_dir, f'gee_{mode_label}_summary.csv'), index=False)\n",
        "        # 2. Summary (Formatted) - RESTORED\n",
        "        display_summary.to_csv(os.path.join(results_dir, f'gee_{mode_label}_summary_formatted.csv'), index=False)\n",
        "        # 3. Fit Stats\n",
        "        fit_stats_df.to_csv(os.path.join(results_dir, f'gee_{mode_label}_fit_statistics.csv'), index=False)\n",
        "        # 4. Combined Coefficients\n",
        "        pd.concat(all_coefficients.values()).to_csv(os.path.join(results_dir, f'gee_{mode_label}_all_coefficients.csv'))\n",
        "        \n",
        "        # 5. Individual Coefficients - RESTORED\n",
        "        for outcome, coef_df in all_coefficients.items():\n",
        "            safe_outcome = outcome.replace(' ', '_')\n",
        "            coef_df.to_csv(os.path.join(results_dir, f'gee_{mode_label}_coefficients_{safe_outcome}.csv'), index=False)\n",
        "\n",
        "        # 6. MICE Diagnostics - RESTORED\n",
        "        if mode == 'MICE' and all_mice_diagnostics:\n",
        "            pd.concat(all_mice_diagnostics, ignore_index=True).to_csv(\n",
        "                os.path.join(results_dir, f'gee_{mode_label}_mice_diagnostics.csv'), index=False\n",
        "            )\n",
        "\n",
        "        if verbose: print(f\"    Saved all tables to {results_dir}\")\n",
        "\n",
        "    # Diagnostics Trigger\n",
        "    if save_diagnostics and all_models:\n",
        "        df_diag = datasets[0].copy() # Use Dataset 0\n",
        "        if 'pct_remote_10' not in df_diag.columns:\n",
        "            df_diag['pct_remote_10'] = df_diag['pct_remote'] * 10\n",
        "            \n",
        "        _save_dharma_diagnostics(\n",
        "            models_dict=all_models, df=df_diag, alpha_dict=alpha_dict,\n",
        "            save_dir=results_dir, mode_label=mode_label, \n",
        "            verbose=verbose, show_plots=show_plots\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        'summary': summary_df,\n",
        "        'coefficients': all_coefficients,\n",
        "        'fit_stats': fit_stats_df,\n",
        "        'models': all_models\n",
        "    }\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. INTERACTION TESTS (Pooled for MICE)\n",
        "# ==============================================================================\n",
        "def run_interaction_tests(\n",
        "    data, outcomes, formula_template,\n",
        "    interaction_vars, exposure_var='pct_remote_10',\n",
        "    alpha_dict=None, results_dir=None, verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs pooled interaction tests by calling run_gee_analysis iteratively.\n",
        "    \"\"\"\n",
        "    if isinstance(data, dict): \n",
        "        mode_label = 'MICE'\n",
        "    else: \n",
        "        mode_label = 'CC'\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*60}\\nINTERACTION TESTS ({mode_label}) | Pooling: {'Yes' if mode_label=='MICE' else 'No'}\\n{'='*60}\")\n",
        "    \n",
        "    pooled_results = []\n",
        "    \n",
        "    for outcome in outcomes:\n",
        "        if verbose: print(f\"Testing interactions for: {outcome}...\")\n",
        "        for int_var in interaction_vars:\n",
        "            # Construct Formula\n",
        "            if int_var in ['ethnicity', 'sex'] and 'C(' not in int_var:\n",
        "                term = f\"C({int_var})\"\n",
        "            else:\n",
        "                term = int_var\n",
        "            \n",
        "            # Add interaction term\n",
        "            int_formula = f\"{formula_template} + {exposure_var}:{term}\"\n",
        "            \n",
        "            # Run GEE (Quietly)\n",
        "            try:\n",
        "                res = run_gee_analysis(\n",
        "                    data=data, outcomes=[outcome], formula_template=int_formula,\n",
        "                    exposure_var=exposure_var, alpha_dict=alpha_dict,\n",
        "                    verbose=False, full_output=True, save_tables=False, \n",
        "                    save_diagnostics=False \n",
        "                )\n",
        "                \n",
        "                # Extract Interaction Row\n",
        "                coefs = res['coefficients'][outcome]\n",
        "                # Find row with colon and exposure\n",
        "                int_row = coefs[\n",
        "                    coefs['Coefficient'].str.contains(':') & \n",
        "                    coefs['Coefficient'].str.contains(exposure_var)\n",
        "                ]\n",
        "                \n",
        "                for _, row in int_row.iterrows():\n",
        "                    pooled_results.append({\n",
        "                        'Outcome': outcome, 'Interaction': int_var, 'Term': row['Coefficient'],\n",
        "                        'IRR': row['IRR'], 'CI_Low': row['IRR_CI_Low'], 'CI_High': row['IRR_CI_High'],\n",
        "                        'P_Value': row['P_Value'], 'Significant': row['Significant']\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                if verbose: print(f\"   Failed {int_var}: {e}\")\n",
        "\n",
        "    # Save\n",
        "    df_int = pd.DataFrame(pooled_results)\n",
        "    if results_dir:\n",
        "        # Conditional Filename\n",
        "        if mode_label == 'MICE':\n",
        "            fname = f'gee_{mode_label}_interactions_pooled.csv'\n",
        "        else:\n",
        "            fname = f'gee_{mode_label}_interactions.csv'\n",
        "            \n",
        "        save_path = os.path.join(results_dir, fname)\n",
        "        df_int.to_csv(save_path, index=False)\n",
        "        if verbose: print(f\"\\n Saved interaction results to: {save_path}\")\n",
        "        \n",
        "    if verbose: display(df_int)\n",
        "    return df_int\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. MASTER PIPELINE\n",
        "# ==============================================================================\n",
        "def run_full_analysis_pipeline(\n",
        "    data, outcomes, formula_template, alpha_dict, results_dir, interaction_vars,\n",
        "    run_main=True, run_diagnostics=True, run_interactions=True, verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Master wrapper that runs regression, diagnostics, and interactions in one go.\n",
        "    \"\"\"\n",
        "    # 1. Main Regression & Diagnostics\n",
        "    if run_main:\n",
        "        results = run_gee_analysis(\n",
        "            data=data,\n",
        "            outcomes=outcomes,\n",
        "            formula_template=formula_template,\n",
        "            alpha_dict=alpha_dict,\n",
        "            verbose=verbose,\n",
        "            save_tables=True,\n",
        "            results_dir=results_dir,\n",
        "            save_diagnostics=run_diagnostics, \n",
        "            show_plots=False # Always suppress screen plots for pipeline\n",
        "        )\n",
        "    \n",
        "    # 2. Interactions\n",
        "    if run_interactions:\n",
        "        run_interaction_tests(\n",
        "            data=data,\n",
        "            outcomes=outcomes,\n",
        "            formula_template=formula_template,\n",
        "            interaction_vars=interaction_vars,\n",
        "            alpha_dict=alpha_dict,\n",
        "            results_dir=results_dir,\n",
        "            verbose=verbose\n",
        "        )\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"PIPELINE COMPLETE\")\n",
        "        print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a035a797-0d46-4280-9278-bd97e2e2abbc",
      "metadata": {},
      "source": [
        "### Full run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f45d3c61-f256-4576-8a13-2614be600079",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CONFIGURATION & CONSTANTS\n",
        "# ==============================================================================\n",
        "\n",
        "# Outcomes\n",
        "OUTCOMES = [\n",
        "    'outcome_emergencies', \n",
        "    'outcome_admissions', \n",
        "    'outcome_bed_days', \n",
        "    'outcome_mha_admissions', \n",
        "    # 'outcome_long_stay_admissions' \n",
        "]\n",
        "\n",
        "# Missingness targets\n",
        "MISSINGNESS_TARGET_VARS = [\n",
        "    'ethnicity',\n",
        "    'imd_at_exposure',\n",
        "    'sex',\n",
        "    'pct_remote'\n",
        "]\n",
        "\n",
        "# Missingness variables to compare\n",
        "MISSINGNESS_COMPARE_VARS = [\n",
        "    'age_at_exposure', 'sex', 'ethnicity', 'imd_at_exposure',\n",
        "    'period_id', 'total_consults_rate', 'pct_remote',\n",
        "    'missing_modality_consults_rate', 'died_in_outcome',\n",
        "    'anx_ever', 'dep_ever', 'smi_ever',\n",
        "    # 'af_ever', 'hf_ever', 'ihd_ever'\n",
        "]\n",
        "\n",
        "# Imputation columns\n",
        "IMPUTATION_PREDICTORS = [\n",
        "    # --- TARGETS ---\n",
        "    'sex', 'ethnicity', 'imd_at_exposure', 'pct_remote',\n",
        "    # --- PREDICTORS ---\n",
        "    'age_at_exposure',\n",
        "    'total_consults', 'total_consults_rate',\n",
        "    'remote_consults', 'remote_consults_rate',\n",
        "    'f2f_consults', 'f2f_consults_rate',\n",
        "    'missing_modality_consults', 'missing_modality_consults_rate',\n",
        "    'active_days_in_exposure',\n",
        "    'outcome_days_at_risk',\n",
        "    'died_in_outcome',\n",
        "    'anx_ever', 'dep_ever', 'smi_ever', \n",
        "    # 'af_ever', 'hf_ever', 'ihd_ever'\n",
        "]\n",
        "\n",
        "# Dynamically build the full list of columns to impute\n",
        "# This ensures new outcomes and their rates are added automatically\n",
        "for outcome in OUTCOMES:\n",
        "    # missingness outcomes\n",
        "    # MISSINGNESS_COMPARE_VARS.append(outcome) ### NOT USING ABSOLUTE NUMBERS FOR MISSINGNESS\n",
        "    MISSINGNESS_COMPARE_VARS.append(f\"{outcome}_rate\")\n",
        "    # imputation outcomes\n",
        "    IMPUTATION_PREDICTORS.append(outcome)\n",
        "    IMPUTATION_PREDICTORS.append(f\"{outcome}_rate\")\n",
        "\n",
        "BASELINE_CHARACTERISTICS_VARS = [\n",
        "    'age_at_exposure', 'sex', 'ethnicity', 'imd_at_exposure',\n",
        "    'total_consults', 'pct_remote',\n",
        "    'anx_ever', 'dep_ever', 'smi_ever', \n",
        "    # 'af_ever', 'hf_ever', 'ihd_ever'\n",
        "]\n",
        "\n",
        "# Covariates and Formula\n",
        "BASE_COVARIATES_STR = (\n",
        "    \"pct_remote_10 + total_consults + age_at_exposure + \"\n",
        "    \"imd_at_exposure + C(period_id, Treatment(1)) + C(sex, Treatment('M')) + \"\n",
        "    \"C(ethnicity, Treatment('White')) + \"\n",
        "    \"smi_ever + anx_ever + dep_ever\"\n",
        "    # \" + ihd_ever + hf_ever + af_ever\"\n",
        ")\n",
        "\n",
        "# The full formula template for GEE\n",
        "FORMULA_TEMPLATE = f\"{{outcome}} ~ {BASE_COVARIATES_STR}\"\n",
        "\n",
        "# Variables to test for interactions with exposure\n",
        "INTERACTION_VARS = [\n",
        "    \"C(ethnicity, Treatment('White'))\",\n",
        "    'imd_at_exposure',\n",
        "    'age_at_exposure',\n",
        "    \"C(sex, Treatment('M'))\"\n",
        "]\n",
        "\n",
        "# ==============================================================================\n",
        "# ANALYSIS PIPELINE\n",
        "# ==============================================================================\n",
        "\n",
        "# Define periods\n",
        "periods_df = generate_and_visualize_periods(\n",
        "    START_DATE, \n",
        "    END_DATE_LDN, \n",
        "    END_DATE_SLAM, \n",
        "    EXPOSURE_LENGTH, \n",
        "    OUTCOME_LENGTH, \n",
        "    WINDOW_STEP,\n",
        "    PERIODS_START,\n",
        "    visualize=False,\n",
        "    save_png=True,  # New parameter\n",
        "    results_dir=f\"{RESULTS_DIR}/period_definition/\"\n",
        ")\n",
        "\n",
        "# Summary of initial cohort\n",
        "save_summary_statistics(\n",
        "    verbose=False,\n",
        "    results_dir=f\"{RESULTS_DIR}/descriptive\",\n",
        "    ldn_dates=(periods_df.exposure_start.min(), periods_df.exposure_end.max()),\n",
        "    slam_dates=(periods_df.exposure_start.min(), periods_df.outcome_end.max()) ### MAYBE USE OUTCOME_START INSTEAD\n",
        ")\n",
        "\n",
        "# Populate periods with data\n",
        "final_df = generate_cohort(\n",
        "    demos,\n",
        "    consults,\n",
        "    emergencies,\n",
        "    admissions,\n",
        "    demos_dynamic,\n",
        "    periods_df,\n",
        "    extraction_date=EXTRACTION_DATE,\n",
        "    use_single_random_period=True,\n",
        "    sample_after_eligibility=True,\n",
        "    random_seed=2025,\n",
        "    verbose=False,\n",
        "    long_stay_threshold_days=7 ### OPTIONAL: ONLY IF USING LONG STAY OUTCOME\n",
        ")\n",
        "\n",
        "# Normality and parametric testing\n",
        "print('Checking distributions...')\n",
        "distribution_checks = check_distribution_assumptions(\n",
        "    final_df,\n",
        "    OUTCOMES,\n",
        "    results_dir=f\"{RESULTS_DIR}/distribution_checks\",\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Missingness analysis\n",
        "print('Checking overall missingness...')\n",
        "missingness_results = run_missingness_analysis(\n",
        "    final_df,\n",
        "    target_vars=MISSINGNESS_TARGET_VARS,\n",
        "    columns_to_compare=MISSINGNESS_COMPARE_VARS,\n",
        "    verbose=False,\n",
        "    results_dir=f\"{RESULTS_DIR}/missingness\",    \n",
        ")\n",
        "print('Checking GP missingness...')\n",
        "gp_missingness_results = analyze_gp_missingness_grid(\n",
        "    final_df,\n",
        "    MISSINGNESS_TARGET_VARS,\n",
        "    verbose=False,\n",
        "    results_dir=f\"{RESULTS_DIR}/missingness\",\n",
        ")\n",
        "\n",
        "# Missing data and imputation\n",
        "print('Creating complete case dataset...')\n",
        "df_complete_case = handle_missing_data(\n",
        "    df=final_df,\n",
        "    cols_to_impute=IMPUTATION_PREDICTORS, \n",
        "    targets_to_update=MISSINGNESS_TARGET_VARS,\n",
        "    complete_case=True,\n",
        "    keep_pct_remote_nan=False, \n",
        "    verbose=False,\n",
        "    results_dir=f\"{RESULTS_DIR}/imputation\"\n",
        ")\n",
        "\n",
        "print('Creating imputed datasets...')\n",
        "imputed_dict = handle_missing_data(\n",
        "    df=final_df,\n",
        "    cols_to_impute=IMPUTATION_PREDICTORS,\n",
        "    targets_to_update=MISSINGNESS_TARGET_VARS,\n",
        "    complete_case=False,\n",
        "    keep_pct_remote_nan=False,\n",
        "    verbose=False,\n",
        "    n_datasets=5,\n",
        "    n_iterations=25,\n",
        "    mice_cycles=5,      \n",
        "    results_dir=f\"{RESULTS_DIR}/imputation\"\n",
        ")\n",
        "\n",
        "# Create baseline characteristics tables\n",
        "print('Generating table one: initial')\n",
        "table_one_initial = generate_table_one(\n",
        "    final_df, \n",
        "    f\"{RESULTS_DIR}/descriptive\", \n",
        "    filename='baseline_characteristics_initial_cohort.csv', \n",
        "    verbose=False,\n",
        "    groupby_mode='period',\n",
        "    show_pval=False,\n",
        "    remote_as_continuous=True,\n",
        "    selected_columns=BASELINE_CHARACTERISTICS_VARS\n",
        ")\n",
        "print('Generating table one: imputed (sample)')\n",
        "table_one_imputed = generate_table_one(\n",
        "    imputed_dict[0], \n",
        "    f\"{RESULTS_DIR}/descriptive\", \n",
        "    filename='baseline_characteristics_imputed_cohort.csv', \n",
        "    verbose=False,\n",
        "    groupby_mode='period',\n",
        "    show_pval=False,\n",
        "    remote_as_continuous=True,\n",
        "    selected_columns=BASELINE_CHARACTERISTICS_VARS\n",
        ")\n",
        "\n",
        "# Alpha optimisation\n",
        "print('Optimising alphas: complete case...')\n",
        "optimal_alphas_cc = optimize_gee_alpha(\n",
        "    df=df_complete_case,\n",
        "    outcomes=OUTCOMES,\n",
        "    alphas_to_test=[0.05, 0.1, 0.2, 0.5, 1.0, 1.5, 2.0, 3.0],\n",
        "    formula_template=FORMULA_TEMPLATE,\n",
        "    save_dir=f\"{RESULTS_DIR}/regression\",\n",
        "    verbose=True\n",
        ")\n",
        "print('Optimising alphas: imputed...')\n",
        "optimal_alphas_mice = optimize_gee_alpha(\n",
        "    df=imputed_dict,\n",
        "    outcomes=OUTCOMES,\n",
        "    alphas_to_test=[0.05, 0.1, 0.2, 0.5, 1.0, 1.5, 2.0, 3.0],\n",
        "    formula_template=FORMULA_TEMPLATE,\n",
        "    save_dir=f\"{RESULTS_DIR}/regression\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Regression\n",
        "print(\"Performing regression: complete case...\")\n",
        "run_full_analysis_pipeline(\n",
        "    data=df_complete_case,\n",
        "    outcomes=OUTCOMES,\n",
        "    formula_template=FORMULA_TEMPLATE,\n",
        "    alpha_dict=optimal_alphas_cc,\n",
        "    results_dir=f\"{RESULTS_DIR}/regression\",\n",
        "    interaction_vars=INTERACTION_VARS,\n",
        "    run_main=True,\n",
        "    run_diagnostics=True,     \n",
        "    run_interactions=True,   \n",
        "    verbose=True    \n",
        ")\n",
        "print(\"Performing regression: imputed...\")\n",
        "run_full_analysis_pipeline(\n",
        "    data=imputed_dict,\n",
        "    outcomes=OUTCOMES,\n",
        "    formula_template=FORMULA_TEMPLATE,\n",
        "    alpha_dict=optimal_alphas_mice,\n",
        "    results_dir=f\"{RESULTS_DIR}/regression\",\n",
        "    interaction_vars=INTERACTION_VARS,\n",
        "    run_main=True,\n",
        "    run_diagnostics=True,     \n",
        "    run_interactions=True,\n",
        "    verbose=True         \n",
        ")\n",
        "\n",
        "print('Finished!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a343f823-f1a2-451d-af21-66b21c8bb6c9",
      "metadata": {},
      "source": [
        "### Save all outputs and code to combined files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88fffa29-d2ea-4037-bc17-58aca6b5861c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supplementary Materials Compiler for Jupyter Notebook\n",
        "# Run this cell after defining RESULTS_DIR\n",
        "# Install python-docx if needed (uncomment if not installed)\n",
        "# !pip install python-docx pandas\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "from docx.shared import Inches, Pt\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "from docx.oxml.ns import qn\n",
        "from docx.oxml import OxmlElement\n",
        "# ============================================================================\n",
        "# OUTPUT CONFIGURATION - Choose which files to create\n",
        "# ============================================================================\n",
        "CREATE_SUPPLEMENTARY_MATERIALS = True   # Create Word doc with all results\n",
        "EXPORT_NOTEBOOKS_AS_IPYNB = True        # Export cleaned .ipynb files (outputs removed)\n",
        "EXPORT_NOTEBOOKS_AS_DOCX = True         # Export notebook code as Word docs\n",
        "# ============================================================================\n",
        "# ============================================================================\n",
        "# FILES TO INCLUDE - Customize which files appear in supplementary materials\n",
        "# Format: ['filename', 'Display Name for Appendix', Include (True/False)]\n",
        "# Files will appear in the order listed below, grouped by folder\n",
        "# ============================================================================\n",
        "FILES_TO_INCLUDE = [\n",
        "    # period_definition\n",
        "    ['periods.csv', 'Study Period Details', True],\n",
        "    ['timeline_visualization.txt', 'Timeline Visualization (Text)', False],\n",
        "    ['timeline_visualization.png', 'Timeline Visualization', True],\n",
        "    \n",
        "    # descriptive\n",
        "    ['summary_statistics.txt', 'Summary Statistics', True],\n",
        "    ['baseline_characteristics_initial_cohort.csv', 'Baseline Characteristics by Period (Initial Cohort)', True],\n",
        "    ['baseline_characteristics_imputed_cohort.csv', 'Baseline Characteristics by Period (Sample Imputed Cohort)', True],\n",
        "    \n",
        "    # distribution_checks\n",
        "    ['distribution_assumptions.csv', 'Distribution Assumptions', True],\n",
        "    \n",
        "    # missingness\n",
        "    ['missingness_summary_counts.csv', 'Missingness Summary Counts', True],\n",
        "    ['missingness_table_ethnicity.csv', 'Missingness by Ethnicity', True],\n",
        "    ['missingness_table_imd_at_exposure.csv', 'Missingness by IMD at Exposure', True],\n",
        "    ['missingness_table_sex.csv', 'Missingness by Sex', True],\n",
        "    ['missingness_table_pct_remote.csv', 'Missingness by Percent Remote', True],\n",
        "    ['gp_missingness_grid.png', 'GP Missingness Grid', True],\n",
        "    \n",
        "    # imputation\n",
        "    ['complete_case_summary.txt', 'Complete Case Summary', True],\n",
        "    ['variables_used.txt', 'Variables Used in Imputation', True],\n",
        "    ['imputation_stats_ethnicity.csv', 'Imputation Statistics by Ethnicity', True],\n",
        "    ['imputation_stats_imd_at_exposure.csv', 'Imputation Statistics by IMD at Exposure', True],\n",
        "    ['imputation_stats_pct_remote.csv', 'Imputation Statistics by Percent Remote', True],\n",
        "    ['imputation_stats_sex.csv', 'Imputation Statistics by Sex', True],\n",
        "    \n",
        "    # regression\n",
        "    ['gee_CC_summary.csv', 'GEE Complete Case Summary', False],\n",
        "    ['gee_CC_summary_formatted.csv', 'GEE Complete Case Summary (Formatted)', True],\n",
        "    ['gee_CC_coefficients_outcome_emergencies.csv', 'GEE Complete Case Coefficients: Emergencies', True],\n",
        "    ['gee_CC_coefficients_outcome_admissions.csv', 'GEE Complete Case Coefficients: Admissions', True],\n",
        "    ['gee_CC_coefficients_outcome_bed_days.csv', 'GEE Complete Case Coefficients: Bed Days', True],\n",
        "    ['gee_CC_coefficients_outcome_mha_admissions.csv', 'GEE Complete Case Coefficients: MHA Admissions', True],\n",
        "    ['gee_CC_fit_statistics.csv', 'GEE Complete Case Fit Statistics', True],\n",
        "    ['gee_CC_alpha_optimization_results.csv', 'GEE Complete Case Alpha Optimization Results', True],\n",
        "    ['gee_CC_all_coefficients.csv', 'GEE Complete Case All Coefficients', False],\n",
        "    ['gee_CC_dharma_qq_plots.png', 'GEE Complete Case DHARMa QQ Plots', True],\n",
        "    ['gee_CC_dharma_residuals_vs_predicted.png', 'GEE Complete Case DHARMa Residuals vs Predicted', True],\n",
        "    ['gee_CC_interactions.csv', 'GEE Complete Case Interactions', True],\n",
        "    ['gee_MICE_summary.csv', 'GEE MICE Summary', False],\n",
        "    ['gee_MICE_summary_formatted.csv', 'GEE MICE Summary (Formatted)', True],\n",
        "    ['gee_MICE_coefficients_outcome_emergencies.csv', 'GEE MICE Coefficients: Emergencies', True],\n",
        "    ['gee_MICE_coefficients_outcome_admissions.csv', 'GEE MICE Coefficients: Admissions', True],\n",
        "    ['gee_MICE_coefficients_outcome_bed_days.csv', 'GEE MICE Coefficients: Bed Days', True],\n",
        "    ['gee_MICE_coefficients_outcome_mha_admissions.csv', 'GEE MICE Coefficients: MHA Admissions', True],\n",
        "    ['gee_MICE_fit_statistics.csv', 'GEE MICE Fit Statistics', True],\n",
        "    ['gee_MICE_alpha_optimization_results_pooled.csv', 'GEE MICE Alpha Optimization Results (Pooled)', True],\n",
        "    ['gee_MICE_all_coefficients.csv', 'GEE MICE All Coefficients', False],\n",
        "    ['gee_MICE_dharma_qq_plots.png', 'GEE MICE DHARMa QQ Plots', True],\n",
        "    ['gee_MICE_dharma_residuals_vs_predicted.png', 'GEE MICE DHARMa Residuals vs Predicted', True],\n",
        "    ['gee_MICE_mice_diagnostics.csv', 'GEE MICE Diagnostics', True],\n",
        "    ['gee_MICE_interactions_pooled.csv', 'GEE MICE Interactions (Pooled)', True],\n",
        "]\n",
        "# ============================================================================\n",
        "# ============================================================================\n",
        "# FORMATTING PARAMETERS - Edit these to customize the document appearance\n",
        "# ============================================================================\n",
        "FONT_NAME = 'Arial'          # Font family for all text\n",
        "FONT_SIZE = 10               # Font size in points\n",
        "IMAGE_WIDTH = 6.0            # Image width in inches\n",
        "TABLE_STYLE = 'Light Grid Accent 1'  # Word table style\n",
        "# ============================================================================\n",
        "def add_table_border(table):\n",
        "    \"\"\"Add borders to a table\"\"\"\n",
        "    tbl = table._element\n",
        "    tblPr = tbl.tblPr\n",
        "    if tblPr is None:\n",
        "        tblPr = OxmlElement('w:tblPr')\n",
        "        tbl.insert(0, tblPr)\n",
        "    tblBorders = OxmlElement('w:tblBorders')\n",
        "    for border_name in ['top', 'left', 'bottom', 'right', 'insideH', 'insideV']:\n",
        "        border = OxmlElement(f'w:{border_name}')\n",
        "        border.set(qn('w:val'), 'single')\n",
        "        border.set(qn('w:sz'), '4')\n",
        "        border.set(qn('w:space'), '0')\n",
        "        border.set(qn('w:color'), '000000')\n",
        "        tblBorders.append(border)\n",
        "    tblPr.append(tblBorders)\n",
        "def format_value(val):\n",
        "    \"\"\"Format values to 3 decimal places if float, remove 'outcome_' prefix from strings\"\"\"\n",
        "    if pd.isna(val):\n",
        "        return ''\n",
        "    if isinstance(val, float):\n",
        "        return f'{val:.3f}'\n",
        "    # Remove 'outcome_' prefix from string values\n",
        "    val_str = str(val)\n",
        "    if val_str.startswith('outcome_'):\n",
        "        val_str = val_str[8:]  # len('outcome_') = 8\n",
        "    return val_str\n",
        "def clean_column_name(col_name):\n",
        "    \"\"\"Clean column names - return empty string for blank or 'Unnamed' columns, remove 'outcome_' prefix\"\"\"\n",
        "    col_str = str(col_name).strip()\n",
        "    if not col_str or 'Unnamed' in col_str or col_str == 'nan':\n",
        "        return ''\n",
        "    # Remove 'outcome_' prefix from column names\n",
        "    if col_str.startswith('outcome_'):\n",
        "        col_str = col_str[8:]  # len('outcome_') = 8\n",
        "    return col_str\n",
        "# Create document\n",
        "if CREATE_SUPPLEMENTARY_MATERIALS:\n",
        "    doc = Document()\n",
        "    title = doc.add_heading('Supplementary Materials', level=0)\n",
        "    title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "    doc.add_paragraph()\n",
        "# Process each subfolder as an appendix\n",
        "if CREATE_SUPPLEMENTARY_MATERIALS:\n",
        "    results_path = Path(RESULTS_DIR)\n",
        "    subdirs = sorted([d for d in results_path.iterdir() if d.is_dir() and d.name != '.ipynb_checkpoints'])\n",
        "    \n",
        "    print(f\"Processing supplementary materials from {len(subdirs)} folders...\")\n",
        "    \n",
        "    # Build lookup dictionary from FILES_TO_INCLUDE\n",
        "    file_config = {}\n",
        "    for idx, (filename, display_name, include) in enumerate(FILES_TO_INCLUDE):\n",
        "        file_config[filename] = {\n",
        "            'display_name': display_name,\n",
        "            'include': include,\n",
        "            'order': idx,\n",
        "            'found': False,\n",
        "            'folder': None\n",
        "        }\n",
        "    \n",
        "    # Find all files across all subdirectories\n",
        "    all_files_in_dirs = {}\n",
        "    for subdir in subdirs:\n",
        "        for ext in ['*.csv', '*.txt', '*.png', '*.jpg', '*.jpeg']:\n",
        "            for file_path in subdir.glob(ext):\n",
        "                if file_path.name not in all_files_in_dirs:\n",
        "                    all_files_in_dirs[file_path.name] = []\n",
        "                all_files_in_dirs[file_path.name].append(file_path)\n",
        "    \n",
        "    # Match configured files to actual files\n",
        "    for filename in file_config.keys():\n",
        "        if filename in all_files_in_dirs:\n",
        "            if len(all_files_in_dirs[filename]) > 1:\n",
        "                print(f\" Warning: '{filename}' found in multiple folders:\")\n",
        "                for fp in all_files_in_dirs[filename]:\n",
        "                    print(f\"    - {fp.parent.name}\")\n",
        "                print(f\"    Using first occurrence: {all_files_in_dirs[filename][0].parent.name}\")\n",
        "            \n",
        "            file_path = all_files_in_dirs[filename][0]\n",
        "            file_config[filename]['found'] = True\n",
        "            file_config[filename]['folder'] = file_path.parent.name\n",
        "            file_config[filename]['path'] = file_path\n",
        "    \n",
        "    # Check for missing files\n",
        "    missing_files = [f for f, cfg in file_config.items() if not cfg['found'] and cfg['include']]\n",
        "    if missing_files:\n",
        "        print(f\"\\n Warning: {len(missing_files)} file(s) in FILES_TO_INCLUDE not found:\")\n",
        "        for f in missing_files:\n",
        "            print(f\"    - {f}\")\n",
        "    \n",
        "    # Check for unlisted files\n",
        "    configured_files = set(file_config.keys())\n",
        "    found_files = set(all_files_in_dirs.keys())\n",
        "    unlisted_files = found_files - configured_files\n",
        "    if unlisted_files:\n",
        "        print(f\"\\n Warning: {len(unlisted_files)} file(s) found but not in FILES_TO_INCLUDE:\")\n",
        "        for f in sorted(unlisted_files):\n",
        "            folder = all_files_in_dirs[f][0].parent.name\n",
        "            print(f\"    - {f} (in {folder})\")\n",
        "    \n",
        "    # Group files by folder and order by FILES_TO_INCLUDE position\n",
        "    files_by_folder = {}\n",
        "    folder_first_appearance = {}  # Track when each folder first appears in FILES_TO_INCLUDE\n",
        "    \n",
        "    for filename, cfg in file_config.items():\n",
        "        if cfg['found'] and cfg['include']:\n",
        "            folder = cfg['folder']\n",
        "            if folder not in files_by_folder:\n",
        "                files_by_folder[folder] = []\n",
        "                folder_first_appearance[folder] = cfg['order']  # Track order from FILES_TO_INCLUDE\n",
        "            files_by_folder[folder].append({\n",
        "                'filename': filename,\n",
        "                'display_name': cfg['display_name'],\n",
        "                'path': cfg['path'],\n",
        "                'order': cfg['order']\n",
        "            })\n",
        "    \n",
        "    # Sort files within each folder by their order in FILES_TO_INCLUDE\n",
        "    for folder in files_by_folder:\n",
        "        files_by_folder[folder].sort(key=lambda x: x['order'])\n",
        "    \n",
        "    # Sort folders by their first appearance in FILES_TO_INCLUDE (not alphabetically)\n",
        "    folder_names = sorted(files_by_folder.keys(), key=lambda f: folder_first_appearance[f])\n",
        "    \n",
        "    print(f\"\\nIncluding {sum(len(files) for files in files_by_folder.values())} file(s) across {len(folder_names)} folder(s)\")\n",
        "    \n",
        "    # Process each folder as an appendix\n",
        "    for appendix_num, folder_name in enumerate(folder_names, start=1):\n",
        "        print(f\"\\nAppendix {appendix_num}: {folder_name}\")\n",
        "        \n",
        "        # Add page break and appendix heading\n",
        "        doc.add_page_break()\n",
        "        appendix_title = f\"Appendix {appendix_num}: {folder_name.replace('_', ' ').title()}\"\n",
        "        doc.add_heading(appendix_title, level=1)\n",
        "        \n",
        "        files_in_folder = files_by_folder[folder_name]\n",
        "        print(f\"  Including {len(files_in_folder)} file(s) (ordered by FILES_TO_INCLUDE)\")\n",
        "        \n",
        "        # Process each file with sub-numbering\n",
        "        for file_idx, file_info in enumerate(files_in_folder):\n",
        "            # Create sub-number (1, 2, 3, etc.)\n",
        "            sub_number = f\"{appendix_num}.{file_idx + 1}\"\n",
        "            file_path = file_info['path']\n",
        "            full_title = f\"Appendix {sub_number}: {file_info['display_name']}\"\n",
        "            \n",
        "            # Process based on file type\n",
        "            if file_path.suffix.lower() == '.csv':\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path)\n",
        "                    doc.add_heading(full_title, level=2)\n",
        "                    \n",
        "                    # Truncate if too large\n",
        "                    if len(df) > 100 or len(df.columns) > 15:\n",
        "                        note_para = doc.add_paragraph(f\"Note: Table truncated. Original: {len(df)} rows  {len(df.columns)} cols\")\n",
        "                        for run in note_para.runs:\n",
        "                            run.font.name = FONT_NAME\n",
        "                            run.font.size = Pt(FONT_SIZE)\n",
        "                            run.font.italic = True\n",
        "                        df = df.iloc[:100, :15]\n",
        "                    \n",
        "                    # Create table\n",
        "                    table = doc.add_table(rows=len(df) + 1, cols=len(df.columns))\n",
        "                    table.style = TABLE_STYLE\n",
        "                    \n",
        "                    # Header\n",
        "                    for i, col in enumerate(df.columns):\n",
        "                        cell = table.rows[0].cells[i]\n",
        "                        cell.text = clean_column_name(col)\n",
        "                        for paragraph in cell.paragraphs:\n",
        "                            for run in paragraph.runs:\n",
        "                                run.font.bold = True\n",
        "                                run.font.name = FONT_NAME\n",
        "                                run.font.size = Pt(FONT_SIZE)\n",
        "                    \n",
        "                    # Data with 3 decimal place formatting\n",
        "                    for i, row in enumerate(df.itertuples(index=False), start=1):\n",
        "                        for j, val in enumerate(row):\n",
        "                            cell = table.rows[i].cells[j]\n",
        "                            cell.text = format_value(val)\n",
        "                            for paragraph in cell.paragraphs:\n",
        "                                for run in paragraph.runs:\n",
        "                                    run.font.name = FONT_NAME\n",
        "                                    run.font.size = Pt(FONT_SIZE)\n",
        "                    \n",
        "                    add_table_border(table)\n",
        "                    doc.add_paragraph()\n",
        "                    print(f\"   {sub_number} Table: {file_path.name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"   {sub_number} Error with {file_path.name}: {e}\")\n",
        "            \n",
        "            elif file_path.suffix.lower() == '.txt':\n",
        "                try:\n",
        "                    doc.add_heading(full_title, level=2)\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        content = f.read()\n",
        "                    p = doc.add_paragraph()\n",
        "                    run = p.add_run(content)\n",
        "                    run.font.name = FONT_NAME\n",
        "                    run.font.size = Pt(FONT_SIZE)\n",
        "                    doc.add_paragraph()\n",
        "                    print(f\"   {sub_number} Text: {file_path.name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"   {sub_number} Error with {file_path.name}: {e}\")\n",
        "            \n",
        "            elif file_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
        "                try:\n",
        "                    doc.add_heading(full_title, level=2)\n",
        "                    doc.add_picture(str(file_path), width=Inches(IMAGE_WIDTH))\n",
        "                    doc.paragraphs[-1].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "                    doc.add_paragraph()\n",
        "                    print(f\"   {sub_number} Image: {file_path.name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"   {sub_number} Error with {file_path.name}: {e}\")\n",
        "# Save supplementary materials document\n",
        "if CREATE_SUPPLEMENTARY_MATERIALS:\n",
        "    output_path = Path(RESULTS_DIR) / 'Supplementary_Materials.docx'\n",
        "    doc.save(str(output_path))\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\" COMPLETE! Saved to: {output_path}\")\n",
        "    print(f\"  {len(folder_names)} appendices processed\")\n",
        "    print(f\"  {sum(len(files) for files in files_by_folder.values())} files included\")\n",
        "    print(f\"{'='*50}\")\n",
        "# ============================================================================\n",
        "# NOTEBOOK EXPORT (with outputs removed for patient data privacy)\n",
        "# ============================================================================\n",
        "if EXPORT_NOTEBOOKS_AS_IPYNB or EXPORT_NOTEBOOKS_AS_DOCX:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"Exporting notebook code...\")\n",
        "    print(f\"{'='*50}\")\n",
        "    try:\n",
        "        import json\n",
        "        import glob\n",
        "        \n",
        "        # Find ALL .ipynb files in current directory (excluding checkpoints)\n",
        "        ipynb_files = glob.glob('*.ipynb')\n",
        "        ipynb_files = [f for f in ipynb_files if '.ipynb_checkpoints' not in f]\n",
        "        \n",
        "        if not ipynb_files:\n",
        "            print(\"\\n No notebook files found in current directory\")\n",
        "        else:\n",
        "            print(f\"\\nFound {len(ipynb_files)} notebook(s) to export\")\n",
        "            \n",
        "            for nb_filename in ipynb_files:\n",
        "                nb_path = Path(nb_filename)\n",
        "                nb_name = nb_path.stem\n",
        "                print(f\"\\n  Processing: {nb_name}\")\n",
        "                \n",
        "                try:\n",
        "                    # Read notebook\n",
        "                    with open(nb_path, 'r', encoding='utf-8') as f:\n",
        "                        nb_data = json.load(f)\n",
        "                    \n",
        "                    # Remove all outputs\n",
        "                    for cell in nb_data.get('cells', []):\n",
        "                        if cell.get('cell_type') == 'code':\n",
        "                            cell['outputs'] = []\n",
        "                            cell['execution_count'] = None\n",
        "                    \n",
        "                    # Save cleaned .ipynb file (if enabled)\n",
        "                    if EXPORT_NOTEBOOKS_AS_IPYNB:\n",
        "                        clean_ipynb_path = Path(RESULTS_DIR) / f'{nb_name}_cleaned.ipynb'\n",
        "                        with open(clean_ipynb_path, 'w', encoding='utf-8') as f:\n",
        "                            json.dump(nb_data, f, indent=2)\n",
        "                        print(f\"     Saved cleaned notebook: {clean_ipynb_path}\")\n",
        "                    \n",
        "                    # Create Word document with code (if enabled)\n",
        "                    if EXPORT_NOTEBOOKS_AS_DOCX:\n",
        "                        code_doc = Document()\n",
        "                        code_title = code_doc.add_heading('Notebook Code', level=0)\n",
        "                        code_title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "                        code_doc.add_paragraph()\n",
        "                        \n",
        "                        # Add notebook name subtitle\n",
        "                        subtitle = code_doc.add_heading(nb_name, level=1)\n",
        "                        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "                        code_doc.add_paragraph()\n",
        "                        \n",
        "                        for cell_idx, cell in enumerate(nb_data.get('cells', []), start=1):\n",
        "                            cell_type = cell.get('cell_type')\n",
        "                            source = ''.join(cell.get('source', []))\n",
        "                            \n",
        "                            if not source.strip():\n",
        "                                continue\n",
        "                            \n",
        "                            if cell_type == 'markdown':\n",
        "                                # Add markdown cells as regular text\n",
        "                                code_doc.add_heading(f'Markdown Cell {cell_idx}', level=2)\n",
        "                                para = code_doc.add_paragraph(source)\n",
        "                                for run in para.runs:\n",
        "                                    run.font.name = FONT_NAME\n",
        "                                    run.font.size = Pt(FONT_SIZE)\n",
        "                            \n",
        "                            elif cell_type == 'code':\n",
        "                                # Add code cells in monospace\n",
        "                                code_doc.add_heading(f'Code Cell {cell_idx}', level=2)\n",
        "                                para = code_doc.add_paragraph()\n",
        "                                run = para.add_run(source)\n",
        "                                run.font.name = 'Courier New'\n",
        "                                run.font.size = Pt(9)\n",
        "                                # Light gray background for code\n",
        "                                from docx.oxml import parse_xml\n",
        "                                shading_elm = parse_xml(r'<w:shd {} w:fill=\"F0F0F0\"/>'.format('xmlns:w=\"http://schemas.openxmlformats.org/wordprocessingml/2006/main\"'))\n",
        "                                para._element.get_or_add_pPr().append(shading_elm)\n",
        "                            \n",
        "                            code_doc.add_paragraph()  # Spacing between cells\n",
        "                        \n",
        "                        # Save code document\n",
        "                        code_doc_path = Path(RESULTS_DIR) / f'{nb_name}_code.docx'\n",
        "                        code_doc.save(str(code_doc_path))\n",
        "                        print(f\"     Saved code document: {code_doc_path}\")\n",
        "                \n",
        "                except Exception as e:\n",
        "                    print(f\"     Error processing {nb_name}: {e}\")\n",
        "        \n",
        "    except ImportError as e:\n",
        "        print(f\"\\n Import error: {e}\")\n",
        "        print(\"  Make sure required packages are installed\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Error during notebook export: {e}\")\n",
        "        print(\"  Skipping notebook export\")\n",
        "else:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"Notebook export disabled in configuration\")\n",
        "    print(f\"{'='*50}\")\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\" ALL TASKS COMPLETE!\")\n",
        "print(f\"{'='*50}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
